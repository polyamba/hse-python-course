id,date,text
1,2023-06-10,первый пост будет посвящен работе относительно свежей по меркам deep learning где как известно статьи отличаются небольшим сроком годности в последние несколько лет мы стали свидетелями бешеного прогресса глубокого обучения во всех его ипостасях проявлениях и приложениях и флагманом прогресса выступает область nlp natural language processing разрастающиеся в ширину и глубину модели обученные на колосалльных обьемах данных дополненные современными плюшками и прибамбасами в виде reinforcement learning on human feedback и прочими другими демонстируют способности которые еще не так давно казались бы чемто из разряда научной фантастики однако выдающиеся возможности языковых моделей не даются за бесплатно всему есть цена характерный размер современных языковых моделей используемых в новомодных чатботах по типу и прочих представителей животного мира исчисляется порядка десятков миллиардов параметров что делает традиционный инференс на пользовательских gpu и любимом колабе затруднительным отсюда возникает естественный вопрос на сжатие таких моделей и квантизация представление чисел в более низкой точности на текущий момент по всей видимости является наиболее ходовым решением но если хочется пользоваться не просто готовой моделькой а еще дообучить на какуюто специфичную задачу как бы много данных не видела сеть в процессе своего обучения для достижения хорошего качества в конкретном приложении обыкновенно требуется дообучать модель и существует множество способов и вариантов эффективного по памяти и вычислительным ресурсам техник по дообучению большихх сетей различные адаптеры среди них наиболее популярна заслуживающая отдельного разбора prompt tuning in context learning и иже с ними и в статье которой посвящен сегодняшний выпуск предложили эффективный алгоритм квантования весов и последующую процедуру дообучения модели под чатбота в результате которой получается весьма годный ассистент который как утверждается бьет всех opensource конкурентов если перечислить списком основные результаты статьи выходит чтото такое 1 новый тип для хранения весов 4bit normalfloat nf4 2 двойное квантование double quantization 3 paged отпимизаторы 4 экстенстивная валидация модели на разных бенчмарках и исследование самих бенчмарков
2,2023-06-10,теперь пройдемся по каждому из пунктов подробнее 1 при квантовании весов каждый параметр принимает одно из небольшого количества возможных значений скажем если квантование в 4 бита то у нас 24 16 вариантов но как выбрать эти значения самая очевидная и по существу используемая на практике стратегия задать максимальное и минимальное значение которое может принимать вес и разбить интервал на одинаковые промежутки и если точки распределены равномерно то это и правда оптимальный выбор однако на практике веса скорее распределены в соотвествии с нормальным распределением имеющим колоколообразную форму то есть веса обычно кучкуются ближе к центру интервала и вероятность их встретить убывает к краям интервала отсюда возникает идея выбрать интервалы таким образом чтобы в каждый интервал попадала примерно одна и та же доля весов на языке высокой науки значения которые случайная величина не превышает с заданной вероятностью называется квантилями распределения например 10 квантиль такой значение что слева от него лежит 10 массы распределения для некоторых случайных величин квантили ищутся легко как для равномерного распределения но для нормального распределения это табличные значения которые приходится получать численными методами
3,2023-06-10,авторы наглядно демонстрируют что normalfloat4 дает заметно большее качество по сравнению с обычным float4
4,2023-06-10,2 следующее ноухау данной работы двойное квантование из квантовой механики здесь не причем при квантовании группы значений каждое число представляется в виде целого числа домноженного на некоторое вещественное называемое масштабом scale со сдвигом от некоторого значения zero point в случае ассиметричной квантизации в данной статье рассматривается симметричная квантизация
5,2023-06-10,вот так мы квантуем получаем целые числа
6,2023-06-10,а вот так получаем значения с плавающей точкой из квантованных
7,2023-06-10,группы для квантования могут быть различного размера можно зафиксировать один масштаб на весь тензор но так как разные измерения тензора могут сильно отличаться масштабом то шаг квантования разность между ближайшими возможными значениями может быть слишком велик для точного представления весов посему выгодно брать меньшие группы фиксируя масштаб для одной размености скажем либо даже для меньшей группы соседних весов однако если брать слишком маленькие группы то память выделяемая на хранение scale cтановится довольно существенной скажем если хранить scale в fp32 то для при квантовании в группы из 64 весов скейлы уже будут давать дополнительные 32 64 05 бита в среднем возникает идея а почему бы и не заквантовать сами скейлы и так до бесконечности но авторы решили ограничиться двумя квантованиями в данной работе scale квантуется группами по 256 весов в 8 бит что снижает накладные расходы на их использование до 8 64 32 64 256 0127 бит в среднем на параметр данная процедура не приводит к просадке качества но дает существенную экономию по памяти
8,2023-06-10,3 и еще одно нововведение так называемые paged optimizers у нвидиевских gpu есть опция автоматической перекачки памяти с gpu на cpu ram если память видеокарты забивается причем данная перекачка не приводит к заметному замедлению процесса обучения весьма полезная фича учитывая сколько нейронов в голове программистов и исследователей убивает ежесекудно сообщение низкоранговый адаптер lora заслуживающая отдельного выпуска не квантуется и хранится в bf16 но учитывая его мизерный вес по сравнению со всей моделью это и не требуется итого в конечном виде обученный на целевой задаче имеет следующий вид
9,2023-06-10,где первый член это дважды квантованный вес а второй низкоранговая добавка double dequant расквантование скейлов с последующим расквантованием весов
10,2023-06-10,4 отдельного упоминания и уважения заслуживает валидация их модели и конкуретных на ряде бенчмарков сначала авторы демонстрируют что при finetuning qlora имеет примерно такое же качество как и дообучение всей модели и lora для модели с весами с плавающей точкой без квантования
11,2023-06-10,далее авторы дообучают квантованную сеть на различных датасетах и замеряют качество работы на бенчмарках тестирующих language understanding и качество ответов модели на чатботбенчмарках замеренное с помощью gpt4 и человеками среди датасетов берутся доступные публично данные selfinstruct longform chip2 hhrlhf unnatural instruct oasst1 и конечная модель guanaco обучается именно на нем alpaca flan v2
12,2023-06-10,для замеров способности модели понимать языки используется стандартный mmlu бенчмарк при обучении на датасете flan v2 здоровенная смесь из множества разнообразных задач на инструкции конечная модель выдает наилучшее качество вероятно изза количества данных и их структуры
13,2023-06-10,потом авторы проводят замеры на vicuna benchmark наборе инструкций против chatgpt используя в качестве арбитра gpt4 в этом случае небольшой oasst1 оказывается наилучшим для instruction finetuning самая большая модель практически сравнивается по качеству с chatgpt
14,2023-06-10,однако делается примечание что разброс довольно существеннен от запуска к запуски и ранжирование у gpt4 и людей не всегда совпадает хоть корреляция и существенна далее качество работы оценивается на запросах из vicuna и openassistant с агреграцией оценок через elo score методики придуманной для шамхатных турниров уже непосредственно кожаными мешками и версии guanaco33b65b опережают конкуретные opensource модели и chatgpt
15,2023-06-10,таким образом с одной стороны авторы получают довольно качественного чатбота и заодно делают вывод про то что в разные датасеты заточены под разные свойства модели скажем flanv2 лучший для прокачки модели под понимание языка не столь хорош для создания чатбота и наоборот для openassistant и самое приятное для обучения самой большой версии guanaco не требуется значительных ресурсов всего день на одной rtx a6000 хоть и много больше бесплатного колаба есть модели на huggingface и несколько нотбуков на странице проекта примечателен с файнтьюном gptneox20b на колабовской t4 c 16gib которая такто весит 40 гигов или 20 в 8битной квантизации введеный в работе формат квантизации имплементирован в библиотеке и некоторые модели не только лишь все можно подгружать в данном формате
16,2023-06-10,ах да маленький нюанс формат используется для хранения весов а в момент вычислений веса материализуется в обычный тип с плавающей точкой fp16 bf16 потому ускорения пока нет и даже некоторые накладные расходы на квантизацию и деквантизацию но в будущем вероятно с развитием карточек и разработкой ядер возможно и добавится еще и ускорение в общем весьма сильный и практически полезный результат
17,2023-06-10,
18,2023-06-12,tldr в данной работе авторы предложили простой и дешевый эффективный по памяти способ дообучения квантованной модели для восстановления качества и под instruction finetuning
19,2023-06-17,tldr в данной статье авторы исследуют влияние факторов и гиперпараметров обучения на изменение качества квантованной модели по сравнению с ее исходной версией в числах с плавающей точкой
20,2023-06-21,данный пост будет в необычном формате обзор на обзор он же обзор 2го порядка
21,2023-06-24,история про диффузионку генерирующую картинки на уровне stable diffusion менее чем за 2с на мобилке
22,2023-07-02,еще одна статья про квантование llm на основе фишерского приближения гессиана и разбиения весов на выбросы и невыбросы
23,2023-07-05,есть просто причем даже не есть дядюшка сэм есть и на днях вышел еше быстрый сэм напомню что в оригинальной работе segment anything model была предложена методология сегментации обьектов из широкого набора категорий на основе различных типов запросов 1 пользователь делает тык и нейросеть выделяет самый примечательный обьект содержащий данную точку 2 пользователь выделяет прямоугольник с предполагаемым обьектом и нейросеть ищет маску уточняет контуры 3 t пользователь пишет текстом название желаемого обьекта и он выделяется маской помимо прочего авторы выпустили самый большой датасет по сегментации собранный частично вручную а частично размеченный автоматически sa1b работа вызвала большой ажиотаж появилось много однако у всей этой красоты есть один большой недостаток основная моделька используемая в работе увесистый vithuge с 630m который довольно нелегко инферить даже на карточках что уж говорить про мобильный инференс и авторы задались вопросом можно ли достичь сопоставимого качества используя значительно меньше вычислений и предложенное решение работает в 50 раз быстрее при сопоставимом качестве на rtx3090 суть идеи в том чтобы использовать легковесную модель и не утруждая себя долгими раздумьями авторы взяли известное рабочее решение c для сегментации модель обучалась на малой части данных sa1b всего 2 миллионах картинках из миллиарда обладая значительно большими inductive biases и специализированными модулями yolo сходится значительно быстрее и требует куда меньше данных энкодеры промтов те же что и в исходном sam модельку валидируют на разных бенчмарках по сегментации в zeroshot fastsam выделяет границы примерно так же хорошо как и sam на coco и lvisv1 примерно на одном уровне с sam не сильно уступая finetuned vitdeth и еще в ряде приложений вышло неплохо salient object detection выделении самого примечательного обьекта и anomaly segmentation метод все же слегка уступает sam основные проблемы возникают с маленькими обьектами утверждается что box confidence score предсказываемый yolo не всегда соотвествует качеству маски маски для маленьких обьектов выходят чрезмерно квадратными
24,2023-07-05,результаты
25,2023-07-05,сегментация с текстовыми промптами
26,2023-07-05,anomaly detection какуюто бяку нашли на таблетке
27,2023-07-05,схематичный рисунок фреймворка feature pyramid с box и mask head nms модули
28,2023-07-05,работает реально быстро
29,2023-07-05,failure cases
30,2023-07-09,на днях выкатили новую диффузионку как можно догадаться из названия моделька заметно набрала в мышечной массе по сравнению с исходной моделью расшумляющий unet вырос в 3 раза в размере стал длиньше и толще остальные изменения носят скорее инкрементальный характер но тем не менее любопытны выше было сказано сеть увеличилась в размерах при этом убрали selfattention на максимальном разрешении изза большого количества вычислений и всего 2 стадии уменьшения разрешения в 2 раза вместо 3 в исходной sd те в середине unet пространственное разрешение в 4 раза меньше чем на входе и выходе в середину напихали аж 10 блоков attention картинки в датасетах бывают разных размеров изза двухстадийной структуры диффузии в латентном пространстве и sr каскада stable diffusion имеет жесткие ограничения на размер данных sd v14 обучалась на картинках где меньшая из сторон имеет размер 512 а существенная доля данных 39 имеет размер менее 256 минимальный принимаемый sdxl можно было бы интерполировать до минимального приемлемого размера но полученные картинки будут размытыми и сеть будет считать что так и надо потому авторы предложили при обучении и генерации обуславливать на размер пару значений высоташирина если надо мыльную картинку получай мыльную надо резкую получай резкую модификация улучшает метрики fid и is stable diffusion при генерации по промпту нередко выдает кропы картинок которые смотрятся неэстетично по всей видимости причина в том что подобная аугментация использовалась на обучении модели решение простое снова condition на положение кропа нормализованные от 01 координаты верхнего левого угла в итоге при condition на 00 выдаются более центрированные картинки захватывающие объект целиком оба типа conditioning добавляют фурьеэмбеддинги в процесс генерации на практике интересна генерация не только квадратных картинок но и прямоугольных потому сеть дообучают на генерацию с разными соотношениями ширины и высоты чтобы обучение было эффективным в один батч собирают картинки с похожим aspect ratio bucketing потюнили параметры обучения и улучшили реконструкцию из латентного пространства после базового unet для диффузии в латентном пространстве добавили еще один unet для улучшения полученных представлений подобная модификация как утверждается помогает генерировать более мелкие детали stable diffusion xl по мнению большинства пользователей всегда или почти всегда лучше стандартной stable diffusion причем версия refinement стадией выглядит более предпочтительной чем без нее что интересно метрики fid и clip score на coco стандартном бенчмарке для оценки качества генерации по промптам даже просели по сравнению с sd v15 v21 но на них как известно следует ориентировать с некоторой опаской sdxl сравнили с midjourney v51 на partiprompts p2 и на ряде категорий пользователи чаще отдавали предпочтение sdxl что выглядит солидно учитывая что midjourney считается флагманом по генерации есть однако подозрения у знающих людей что саму sdxl файнтьюнили на генерациям от midjourney сама модель настолько же опенсорсная как и llama веса выдаются только с одобрения авторов ждем пираток короче
31,2023-07-09,пользовательские предпочтения
32,2023-07-09,архитектура новый модуль предложенный refiner
33,2023-07-09,распределение обучающей выборки по ширине и высоте
34,2023-07-09,эффект от condition на размер
35,2023-07-09,эффект от сondition на crop
36,2023-07-09,refinement позволяет лучше прорисовывать мелкие детали
37,2023-07-09,stable diffusionxl против midjourney v51
38,2023-07-09,fidclip кривые для stable diffusion xl хуже чем для sd v15v21 напомню что согласно этим метрикам генерация тем лучше чем ниже и левее находится кривая формы эмблемы компании nike
39,2023-07-19,на днях tri dao это имя и фамилия а не три выпустил сиквел знаменитого блокбастера flash attention 2 обновленная версия flash attention примерно в быстрее своей предшественницы и местами до раз бывает быстрее наивной реализации pytorch потребность в работе с длинным контекстом возникает в ряде приложений задачах связанных с написанием или пониманием книг обработкой видео аудио наивная реализация attention требует квадратичного по длине последовательности объема памяти и вычислений изза чего на практике редко используют контекст более 1k токенов в свое время много работ и усилий исследователей было потрачено на разработку альтернатив алгоритму внимания в его исходной формулировке но нельзя сказать чтобы один из множества подходов оказался конкурентоспособен на широком наборе задач в работе по flash attention добились значительного ускорения attention и снижения пикового расхода памяти за счет оптимизации входящих в него операции при этом сам алгоритм математически эквивалентен up to numerical precision исходному attention напомню что в основе оригинальной работы по flash attention лежат следующие наблюдения 1 большую часть времени занимают не вычисления а работа с памятью 2 для подсчета attention не обязательно материализовывать всю матрицу attention квадратичную по длине последовательности целиком можно считать ее поблочно а затем агрегировать результат flash attention уменьшает количество операций по перекачке данных с памяти в кэши gpu за счет выполнения нескольких математических операций сразу в пределах одного блока kernel fusion при обратном проходе и подсчете градиентов по параметрам матрица attention пересчитывается снова поблочно flash attention делает больше вычислений чем исходный алгоритм но так как вычисления в разы быстрее перекачки памяти тудасюда получается значительный выигрыш в производительности однако даже оптимизированный flash attention сильно недоиспользует возможности современных ускорителей вычислений достигая всеголишь от теоретической максимальной производительности в то время как матричные умножения могут достигать
40,2023-07-19,в работе flash attention 2 по существу еще слегка подкрутили процедуру вычисления и повысили степень параллелизма самой операций автор заметил что операции не являющиеся матричным умножением выполняются куда медленнее в 16 раз чем матричные умножения потому переписал алгоритм так чтобы уменьшить их количество казалось бы их количество невелико но тем не менее они занимают существенную часть общего времени работы кроме того при авторегрессионной генерации нужна лишь верхнетреугольная часть матрицы attention и вместо того чтобы считать ее а затем занулять ее просто не считают вот так вот благодаря перечисленным выше нововведениям удается добиться ускорения 23x flash attention1 параллелизует вычисления по размеру батча и числу голов в трансформере но если батч не слишком большой или трансформер не очень огромный то многие streaming multiprocessors sm простаивают и чтобы не оставлять их без дела предлагается паралеллизовывать вычисления и по длине последовательности на прямом проходе ряды матрицы attention можно считать независимо а на обратном проходе колонки и каждый поток обрабатывает свой токен кроме того для уменьшения коммуникации между варпами группами потоков оказывается целесообразным держать куски матриц ключей key и значений values общими для групп поток а query свою на варп в flash attention1 было наоборот уменьшение количество операций чтениязаписи приводит к дополнительному ускорению flashattention2 сравнивается с flashattention из оригинального репозитория реализации на triton и xformers для замеров рассматривают последовательности длиной от 512 до 16k токенов и слой attention со скрытой размерностью 2048 64 или 128 голов flashattention2 в 1315x быстрее на прямом проходе и до 2x быстрее на обратном проходе по сравнению с flashattention 1 особенно велик выигрыш при использовании causal mask flashattention 2 использует до 72 теоретической производительности a100 на h100 разница еще заметнее данная история поучительна тем что одна и та же математическая операция в зависимости от реализации может выполняться принципиально разное время замечательный пример того что насколько учет особенностей железа время работы различных компонент сильных и слабых сторон ускорителя вычислений важен при проектировании алгоритмов
41,2023-07-19,алгоритм прямого прохода
42,2023-07-19,алгоритм обратного прохода
43,2023-07-19,параллелизм по длине последовательности на прямом и обратном проходе
44,2023-07-19,распределение матриц q k v по варпам
45,2023-07-19,сравнение скорости forward pass для различных реализаций attention
46,2023-07-19,сравнение скорости forwardbackward pass для различных реализаций attention
47,2023-07-19,attention forwardbackward на h100
48,2023-07-21,обучение всех параметров больших языков моделей весьма прожорливо по памяти изза необходимости хранить кроме самой тяжеловесной модели еще и состояния оптимизатора 8 байт на параметр один из самых ходовых методов заключающийся в обучении низкоранговых добавок к весам позволяет сильно сэкономить по памяти демонстрируя при этом хорошее качество при обучении предобученной модели на downstream задачах но низкоранговые представления имеют место при дообучении в то время как для эффективного предобучения на разнообразных данных желательно использовать все имеющуюся в распоряжении емкость сети то есть обучение должно быть высокоранговым в данной статье авторы предлагают метод последовательного обучения низкоранговых добавок к весам линейных слоев нейронной сети с последующим их слиянием с основными весами и как утверждается подобная процедура для достаточно больших сетей самая большая обученная сеть имеет 350m параметров сущий пустяк по современным меркам работает ненамного хуже стандартной полноранговой процедуры обучения ранг суммы двух и более матриц ограничен сверху суммой рангов матриц если низкоранговые матрицы в достаточной мере взаимно независимы то их сумма может иметь значительно больший ранг чем каждое слагаемое по отдельности последовательно обучая низкоранговые добавки возможно в итоге добиться высокорангового изменения весов матрицы в этом и суть метода однако чтобы метод заработал авторам пришлось учесть ряд нюансов и применить пару трюков вопервых используемый при обучении трансформеров хранит скользящие статистики градиентов и при переходе к обучению новой низкоранговой добавки если не предпринимать никаких действий оптимизация будет проводиться в том же подпространстве что и у предыдущей lora добавки нивелируя всякий смысл в итеративной процедуре для предотвращения такого сценария авторы зануляют 99 состояний оптимизатора с меньшей абсолютной величиной почему не все почему не любую другую долю при инициализации новой добавки кроме того learning rate в момент начала обучения новой добавки зануляется и потом быстро разогревается до примерно того же значения с которым закончила обучение прошлая добавка используется cosine annealing learning rate без короткой warmup фазы обучение расходится предложенная cтратегия именуется
49,2023-07-21,авторы обучают семейство декодерных моделей моделей от 60 до 350m типичный размер языковых моделей в 1819 году на данных из архитектура модели повторяет процедура обучения состоит из первоначальной фазы полнорангового обучения те обучения всех параметров модели в течение 5k шагов и 3 циклов обучения низкоранговых добавок на протяжении тех же 5k шагов с warmup фазой в 100 шагов при переходе к новой lora пиковый расход памяти такой же как и в стандартной процедуре обучения в качестве бейзлайнов используются стандартное обучение обучение меньшей модели с таким же количеством обучаемых параметров как с lora control lora метод ожидаемо бьет lora обладая большей выразительностью и меньшую сеть с тем же числом обучаемых параметров за исключением самой маленькой модели при этом несколько уступая стандартной процедуре обучения авторы анализируют спектральное разложение обученных матриц и у relora оно больше напоминает изменение весов при обучении всех параметров по сравнению с lora хоть все еще заметно отличается показывает что все компоненты метода важны для приемлемого результата первичная процедура стандартного обучения зануление состояний отпимизатора и warmup довольно интересный и разумный подход применимость его в качестве претрейна по моему мнению ограничена изза необходимости фазы высорангового обучения в начале изза чего большие llmки какоето время придется обучать на множестве хостов основной выигрыш может быть при файнтьюнинге на достаточно больших и разнообразных задачах где выразительности низкоранговых добавок недостаточно
50,2023-07-21,кривые обучения стандартного обучения синяя relora оранжевая и меньшей модели control
51,2023-07-21,learning rate schedule из статьи на графике 4 цикла relora вместо 3х
52,2023-07-21,ключевое тождество из статьи
53,2023-07-21,алгоритм
54,2023-07-21,сингулярное разложение разницы весов на 20k шагов конец обучения и 5 k шагов конец warmup фазы
55,2023-07-21,relora против бейзлайнов
56,2023-07-24,не прошло и года и даже половины года как запрещенная в россии экстремистская организация meta выпустила новую версию всем полюбившейся llmки llama2 первая версия модели стала настоящим хитом среди исследователей практиков да и простых обывателей будучи наиболее качественной языковой моделью среди находящихся в публичном доступе стала основой для множества чатботов получила множество интеграций для запуска на чем угодно начиная от продвинутых gpu и заканчивая в плане архитектуры и процедуры предобучения llama2 не претерпела значительных изменений вместо стандартного attention блока где количество голов в query key value проекциях одинаково и каждому query соотвествует отдельный key и value используется c 8 проекциями вместо те каждая key value активация спаривается с головами query делать полный multi query с 1 проекций на все головы не стали по двум соображениям 1 на инференсе они параллелизует вычисления между 8 gpu и пришлось бы все равно копировать key value между всеми устройствами 2 multi query просаживается по качеству по сравнению с исходным attention а grouped query имеет примерно то же качество данное изменение полезно при авторегрессионой генерации с использованием key value кэшей так как приводит к заметной экономии в памяти при той же длине последовательности экономия в рвз длину контекста увеличили до токенов позиционные энкодинги могут работать и с данные отфильтровали более тщательно увеличили в размере на и обучили все модели на токенов вместо итоговая модель на common sense reasoning question answering world knowledge и тд оказывается лучше прошлой версии и всех других моделей в открытом доступе но уступает флагманским закрытым gpt4 palm2l куда более занимательна ей же и уделено основное внимание процедура instructionfinetuning и получения чатбота из языковой модели instructionfinetuning процедура состоит из 2 стадий 1 sft supervised finetuning 2 rlhf reinforcement learning авторы собирают свой собственный датасет из инструкций в котором акцент был сделан не на количество инструкций а на их качество и разнообразие актуальные работы утверждают что для instruction finetuning данных много и не требуется в полученном датасете 27540 инструкций на первом стадии sft модель обучают на causal lm как на этапе преобучения на датасете инструкций промты и ответы контатенируют с одну последовательность разделяя специальным токеном данные для обучения reward модели собирали с помощью человекоподобных разметчиков каждый респодент выбирает между двумя вариантами с градацией разницы significantly better better slightly better or negligibly betterunsure для максимизации разнообразия варианты ответов генерируются случайно выбранными моделями из семейства llama2 с разной температурой
57,2023-07-24,обучают две reward модели 1 helpfullness полезность 2 safety безопасность для моделирования reward используются предобученные чекпоинты с 1го этапа в качестве функции потерь используется бинарная ранжировочная функция потерь из с добавкой зависящей от степени увереннности в ответе чтобы разница в оценках для ответа с большей уверенностью была больше чем для менее уверенного ответа полученные reward модели сравнивают с теми что получаются при обучении на других instruction датасетах и gpt4 и по отдельности reward модели оказываются лучше безйлайнов на своих и прочих датасетах но для gpt4 нет данных на других instruction датасетах затем исследуется scaling поведение от количества данных и размеров модели ожидаемо большие модели и большее количество данных улучшает качество reward модели с ростом количества полученных данных от аннотаторов авторы итеративно дообучают reward модель 5версий с использованием ppo и нередко перед чатботом ставится задача следовать некоторой инструкции или парадигме поведения на протяжении нескольких раундов вопросответ или всего диалога чтобы поддерживать в модели подобный сценарий поведения авторы статьи используют метод gatt ghost attention ко всем запросам пользователя добавляется целевая инструкция но чтобы не нарушать распределение данных диалог где пользователь повторяет одну и ту же инструкцию много раз смотрится неестественно лосс от прошлых сообщений в диалоге не учитывается данная модификация действительно способствует следованию ассистентом целевой инструкции
58,2023-07-24,llama2chat уверенно побеждает чатботов основанных на моделях в открытом доступе сопоставимых размеров и с небольшим отрывом оказывается лучше с точки зрения человеческих предпочтений чем chatgpt при оценке helpfulness на собранных meta 4k инструкциях при обучении на safety данных с ростом количества safety данных стабильно уменьшается доля небезопасных ответов без просадки по метрике полезности по safety доле небезопасных ответов и общему рейтингу полезности и безопасности llama2 чатботы опережают конкуретных открытых чатботов и chatgptpalm при оценке на собственном бенчмарке из 2k промптов из дополнительных экспериментов авторы показывают что модель можно научить действовать корректно подав инструкцию относящуюся к заданному времени например модель не будет знать ответ на то кто побелил во второй мировой войне если бы запрос был адресован в 1940 году и хорошо взаимодействует с llama2 новая sota среди моделей в открытом доступе и с учетом бешеного прогресса в области большого интереса в dlсообществе за несколько дней с выпуска народ уже успел изрядно поиграться с моделью покрутить и повертеть ее данная работа труд скорее инженерный чем научный но безусловно полезный и важный приятное отличие от первой версии где месяцами можно было ждать одобрения на скачивание весов хотя все кому надо воспользовались пиратками в том что запрос на llama2 удовлетворяется оперативно обычно в течение пары часов
59,2023-07-24,сравнение llama2chat против других чатботов по метрике helpfulness
60,2023-07-24,сравнение llama2chat против других чатботов по метрике safety
61,2023-07-24,схема процедуры обучения llama2chat
62,2023-07-24,кривые обучения моделей из семейства llama2
63,2023-07-24,сравнение на стандартных бенчмарках для lm
64,2023-07-24,сравнение на academic бенчмарках
65,2023-07-24,сравнение reward моделей
66,2023-07-24,скейлингкривые для reward моделей
67,2023-07-24,демонстрация пользы от gatt в multiturn диалоге
68,2023-07-24,скейлинг по safety от количества safety данных
69,2023-07-24,time awareness у llama2 при подаче инструкции где чатбот должен ответить использую актуальные на то время знания человечества о мире
70,2023-07-25,с течением времени запрос на обучение языковых моделях основанных на трансформерах растет быстрее чем имеющиеся в распоряжении вычислительные ресурсы потому и возникает запрос на коллективный разум dlсообщества предложил великое множество стратегий ускорения обучения модификации процедуры обучения с использованием только части слоев использованием части данных и алгоритмами отпимизации каждая статья заявляет в той или иной форме что предложенный метод эффективнее базового решения но что означает эффективнее ответ на этот вопрос не столь тривиален количество шагов алгоритмов может быть плохим показателем так как время одного шага может существенно различаться между алгоритмами алгоритм делающий в два раза меньше шагов но с пятикратной стоимостью шага не слишком полезен время работы зависит от используемой конфигурации вычислительных ресурсов число операций с плавающей точкой зачастую не отражает специфику реализации математических операций на железе время доступа к памяти накладные расходы на коммуникацию потому авторы предлагают использовать относительное время работы привязанное к конкретному железу для некоторой конфигурации видеокарты cpu озу фиксируется вычислительный бюджет и при запуске на другом сервере надо замерить отношение работы алгоритма к исходной конфигурации и с поправкой на этот фактор проводить все замеры те если новый сервер в 2 раза быстрее то эксперимент должен быть в два раза короче
71,2023-07-25,обучаем меньшую модель некоторое число шагов а затем дублируем слои и обучаем далее уже вдвое большую модель предполагаемый выигрыш за счет того что меньшая модель делает большее число шагов при фиксированном размере в архитектурах с residual connections в целях регуляризации иногда пропускают вычисления части слоев например mha или feedforward block кроме того сия процедура дает некоторую экономию в количестве вычислений обыкновенно в начале обучения вероятность пропуска слоя равна нулю и с течением времени растет до некоторого максимального значения кроме того вероятность растет от входа модели к ее выходу согласно народной мудрости в начале обучаются универсальные и общие представления а ближе к выходу более специфичные на обратном проходе градиенты считаются только по примерам с наибольшим значением функции потерь на прямом проходе считается лосс а на обратном сэмплируются примеры отранжированные по недавней истории функции потерь предложенная в работе метод уменьшает вес примеров с большими значениями train лосса так как они скорее всего соотвествуют шумным или некорректно размеченным данным в качестве альтернативы общеупотребимому adamу в начале этого года был предложен отпимизатор lion найденный с помощью reinforcement learning теоретических гарантий на его превосходство нет но в ряде работ утверждается что он немного накидывает по сравнению с adam при этом сам алгоритм достаточно прост все новое это хорошо забытое старое как известно sophia по существу тот же adam но использующий другой способ оценки кривизны для preconditioning в исходной работе использовались две формулировки через и gaussnewtonbartlett в экспериментах здесь используется вторая так как ее реализация была опубликована и работала немного лучше в cтатье по sophia
72,2023-07-25,для сравнения различных методик эффективного обучения авторы берут bert и t5 модели обучают на задачи mlm и spancorrupting mlm соответственно для предобучения используется c4 для каждого метода параметры подстраиваются на основе некоторой сетки алгоритмы сравниваются на вычислительных бюджетах в 61224 часов на одной rtx 3090 a100 для т5 для оценки качества модели bert валидируют результат файтьюнинга bert на задачах из gluesuperglue бенчмарков кроме того оценивается качество работы mlm на валидации для bert и t5 и для t5 модель проверяют на super natural instructions как оказалось ни один из перечисленных методов оптимизации процедуры обучения стабильного выигрыша по сравнению со стандартной процедуро обучения adam без наворотов на коротких отрезках 612 часов некоторый профит имеет место от практически исчезающий при более длительном обучении остальные алгоритмы из рассмотренных не дают выигрыша против бейзлайна ни при каких бюджетах обучения методы отбора данных при заданном ограничении на время работают хуже и на mlm и на glue новомодные отпимизаторы lion и t5 сходятся хуже по времени со сравнению с baseline результат данной статьи в очередной раз подтверждает важность аккуратного и честного сравнения методов друг с другом статьи по deep learning выходят все чаще и чаще потому спрос на санитаров леса будет только расти с течением времени наука на то и наука что фальсифицируема
73,2023-08-01,каждый исследователь или практик алкает следующих качеств от архитектуры нейронной сети 1 эффективное обучение 2 хорошее качество 3 дешевый инференс архитектура трансформер доминирующая во многих областях и приложениях deep learning удовлетворяет первым двух из требований но к сожалению довольно тяжеловесна и прожорлива до вычислений множество работ предлагали различные альтернативы и приближения attention но ни один из них не стал общеупотребимым на практике аналогично варианты с внедрением рекуррентных механизмов и statespace моделей хоть и получили признание но не составили полноценной конкуренции attention и в этой статье авторы делают довольно громкое заявление о том что они смогли воплотить невозможный треугольник в реальность создав архитектуру обладающую всеми желаемыми аттрибутами retnet все прошлые архитектуры можно теперь со спокойной душой отправить на свалку истории
74,2023-08-01,предложенный механизм рассматривается в качестве альтернативы стандартному attention в трансформер блоках то есть чередуется с feedforward слоями и применяется на последовательностях токенов retention является по существу версией rnn c обновляемым вектором состояния где каждое последующее состояние получается как взвешенная матрично сумма прошлого состояния и текущего элемента последовательности и с выходной проекцией превращающей скрытое состояние в выход слоя все проекции взвешивающая скрытое состояние текущий элемент последовательности и выход получаются линейным преобразованием входа полный аналог query key value проекций в attention в частном случае необучаемых проекций архит далее авторы диагонализуют матрицу преобразующую скрытое состояние перепараметризуют веса и приходят в итоге к форме удобной для вычисления полученная операция включает в себе attention без softmax causal masking и экспоненциальное затухание по длине последовательности замечательным свойством retention является возможность представить его в 3 ипостасях 1 параллельная версия оптимальная для обучения на gpu 2 последовательная версия бюджетная на инференсе 3 чанковая реализация tradeoff между двумя первыми в первой версии одновременно обрабатывается вся последовательность во второй один элемент в третьей блок некоторого размера далее как и в оригинальном attention предлагается многоголовая версия retention однако если обучаемыми разными головами паттерны в multiheadattention имеют случайное поведение то здесь разные головы отвечают разным масштабам коэффициент затухания γ определяет характерный масштаб длину контекста с которой работает данная голова на выход multiheadretention довешивается groupnorm для повышения выразительности операции выход multiheadretention умножается на gating механизм и пропускается через выходную проекцию
75,2023-08-01,предложенную архитектуру retnet обучают на задаче на смеси из в качестве бейзлайна берут стандартный трансформер для оценки качества модели используют 0shot и fewshot бенчмарки из retention network достигает меньшей перплексии на валидации по сравнению с transformer на размерах 27b 67b и по форме кривой авторы утверждают что retention network масштабируется лучше трансформера сильноватое заявление на основе графика из 3х точек аналогично retention network превосходит transformer на 0shot и fewshot далее следуют замеры расхода памяти и скорости инференса и retnet оказывается быстрее и экономичнее по расходу gpu ram даже оптимизированной версии flash attention на тритоне расход памяти при работе в рекуррентном режиме не растет с размером последовательности как и скорость генерации одного токена не зависит от длины последовательности при сравнении на ряде бенчмарков по language modelling retnet превосходит другие альтернативы трансформера statespace модели линейный трансформер удивительно что нет сравнения на которое является признанным мерилом способности сети работать с длинным контекстом каким бы данный бенчмарк ограниченным не несовершенным ни был авторы проводят ablation компонент архитектуры и все компоненты экспоненциальный спад γ с разным коэффициентом в разных головах оказывается важен хоть разница не сказать чтобы кардинальная для каждой из компонент
76,2023-08-01,интересная идея с понятным value proposition однако валидация выглядит на текущий момент слишком ограниченной и специфичной чтобы утверждать про рождение преемника трансформера наличие экспоненциального затухания пусть и на разных масштабах ограничивает длину обрабатываемого контекста устремление же γ1 по всей видимости приводит к нестабильности в обучении код модели недавно появился в открытом доступе так что ждем независимых проверок а можем и сами убедиться в чудесных свойствах предложенной архитектуры но за подачу материала тем не менее следует отдать комплимент авторам красивая реклама важная составляющая не только в коммерции но и в науке
77,2023-08-15,очередной гипермегаультра эффективный гибрид трансформера и сверточной архитектуры от nvidia согласно кривой парето throughput vs top1 imagenet accuracy представленной в работе fastervit заметно опережает всех предшественников при замерах на a100 в чем же секрет такого чудесного быстродействия работа по ускорению и оптимизации vision и не только трансформеров активно ведется примерно со времени их младенчества исходный трансформер очень гибок и универсален и в то же обладая меньшим набором inductive biases менее заточен под задачи компьютерного зрения поэтому не столь эффективен с 2021 года появилось множество работ совмещающих архитектурные элементы из трансформера и более привычных сверточных архитектур и многие другие серьёзным недостатком трансформера с точки зрения вычислительной сложности является квадратичная сложность по длине последовательности потому приходится либо ограничиваться attention на низких разрешениях либо нарезать на крупные патчи в литературе был предложен ряд способов удешевить attention за счет его локализации в окне swin факторизации на локальный и глобальный attention в этой работе предложили по сути новую версию факторизации attention заводят на каждое окно аналогичное таковому в swin некоторое количество carrier токенов которых гораздо меньше чем патчей в окне и в каждом hierarchical attention происходит обмен информацией между carrier токенами из разных окон а затем carrier токены обмениваются информацией с патчами в окнах после одного или несколько раундов hat carrier токены сливаются с исходными патчами для передачи глобальной информации кроме того популярные в мобильных архитектурах depthwise свертки не эффективны по скорости изза memorybound природы верхних слоев поэтому используются обычные 3x3 свертки без факторизации дизайн архитектуры довольно стандартный для эффективных гибридных архитектур на первых двух стадиях высоком разрешении сверточные блоки на меньшем разрешении работают hat блоки
78,2023-08-15,fastervit оказывается значительно эффективнее альтернатив по throughput хоть и не по количеству параметров кроме того архитектура неплохо себя показывает в качестве бэкбоуна на задачах сегментации и детекции далее в статье есть ablation компонент подтверждающий необходимость того или иного решения эффект от каждой из них по отдельности честного говоря небольшой если верить представленным результатам то вышло довольно эффективное решение с относительно простой структурой вопрос в том насколько ускорение на gpu будет переноситься на мобильные архитектуры кои скорее всего и будут являться целевой аудиторией комбинация локальной и глобальной агрегации признаков выглядит универсальным подходом в разработке современных архитектур и дальнейший прогресс носит скорее инкрементальный характер кроме того процедура обучения если присмотреться включает некоторый тюнинг гиперпараметров learning rate dropout drop path по сравнению со стандарнтыми рецептами и неизвестно насколько выигрыш обусловлен самой архитектурой а насколько удачным оптимизационным рецептом отдельный вопрос насколько наработки в области эффективных архитектур переносятся на clipмодели и selfsupervised претрейны есть ли какаято польза от специфичных архитектур или базовый трансформер всех победит
79,2023-08-26,главным недостатком любимых всеми диффузионных моделей для генерации чеголибо является их скорость каждый процесс генерации представляет собой итеративное применение довольно увесистой модели и потому есть два направления для ускорения 1 уменьшение количества шагов сэмплирования 2 удешевление одного шага сэмплирования сегодняшний сюжет будет относиться ко направлению за основу берется как известно данная модель состоит из модели генерирующей в пространстве текстового энкодера и вариационного кодировщика погружающего в скрытое пространство и из него но так как почти все вычисления приходятся на unet целесообразно сжимать в первую очередь его unet stable diffusion состоит из сверточных блоков и блоков которое делают авторы оказывается что из unet stable diffusion можно вытащить среднюю часть с наименьшим разрешением карт активаций с довольно небольшой просадкой по качеству и метрикам без какого либо дообучения что любопытно выходит что данный участок сети не выполняет серьезной работы либо сеть как человеческая печень продолжает функционировать в штатном режиме при удалении части от нее далее авторы предлагают три варианта уменьшенной stable diffusion 1 base меньше блоков в верхних слоях 2 small без блоков в середине модели 3 tiny укорачивание на промежуточных разрешениях обучать с нуля это хозяйство оригинальная stable diffusion обучалась 150к gpu часов на a100 удел богатых и авторы прибегают к лосс состоит из трех компонент 1 исходного denoising loss 2 mse с учителем 3 mse между активациями на выходах каждой стадии перед понижениемповышением разрешения существенная деталь от которого зависит итоговое качество всей процедуры на каких данных следует дистиллировать в условиях ограниченного бюджета авторы используют подвыборку из примеров из с наибольшими оценками по эстетике
80,2023-08-26,процедура дистилляции выходит относительно бюджетной по нынешним меркам обучение самой большой модели bksdm base занимает на одной a100 для оценки качества генераций модели используется стандартный бенчмарк на реалистичность изображений и соответствие описанию картинки дистиллированые bksdm несколько просаживаются по метрикам по сравнению с материнской моделью тем не менее обладают все еще неплохим качеством генераций что интересно оптимальное качество по fid достигается еще до последней итерации авторы демонстрируют что bksdm генерирует лучше ganов хотя осмысленность подобного сравнения без сопоставления времени генераций выглядит сомнительной полученные модели позволяют генерировать картинку на в зависимости от размера быстрее чем исходная sd v14 далее авторы проводят ablation инициализация моделиученика весами stable diffusion работает на порядок лучше чем с нуля что ожидаемо учитывая короткое время обучения оба лосса в дистилляции на выходе модели и на уровне промежуточных активаций полезны и улучшают качество больший размер батча немного лучше чем меньший в среднем по метрикам на специализированной генерации с dreambooth дистиллированные модели почти не уступают базовой stable diffusion данная статья достигает довольно неплохих результатов по сжатию моделей в условиях ограниченных ресурсов используя стандартные методы из дистилляции трансформеров совсем просадки по качеству избежать не удалось и сжатые модели по всей видимости более специализированы под генерацию в стиле laion aestethics и скорее всего проседают более заметно на промптах из другого распределения однако сама возможность восстановить качество близкое к исходному за счет отбрасывания некоторых блоков говорит о том что есть некоторая свобода и простор в направлении оптимизации архитектур для диффузионных моделей классификаторы imagenet1k и бэкбоуны для детекциисегментации на mscoco несколько приелись
81,2023-08-27,вопреки обыкновению данная статья не про квантование нейронных сетей и даже не про квантование векторов а про как известно чем больше данных тем лучше итоговая модель однако обучение на большом датасете требует значительных затрат по времени да и данные надо гдето хранить потому возникает естественный вопрос существующие методы отбирают примеры либо на по примерам для фиксированной архитектуры либо способами на основе некоторых эвристик проблема первого семейства подходов что они не обобщаются на другие модели и архитектуры а качество работы второго класса методов обычно оставляет желать лучшего кроме того первый класс методов требует весьма значительных вычислительных затрат и в данной работе предлагают метод который с одной стороны не привязан к конкретной модели и с хорошим качеством хочется чтобы полученный набор данных был как можно более разнообразным за основу берут метод который стартует с произвольно выбранного примера и каждый следующий пример подбирают так чтобы он был как можно дальше от выбранных ранее и ближе к еще не выбранным однако проблема исходной постановки в том что пока примеров выбрано мало по сравнению с размером всех данных будут браться примеры наиболее близкие к центроиде еще не выбранных и разнообразие примеров выйдет довольно ограниченным авторы предлагают пройтись по примерам в порядке определенном graphcut и добавлять разбить примеры на несколько корзин и затем равномерно выбирать примеры из каждой полученной корзины утверждается что образованный таким образом датасет будет обладать достаточным разнообразием и репрезентативностью для дальнейшего сжатия авторы оценивают информативность патчей с помощью модифицированной версии и заменяют на черные квадраты наименее информативные
82,2023-08-27,качество работы метода проверяется на cifar10 imagenet1k ade20k ms coco и инструкциях из alpaca предложенный метод заметно превосходит альтернативы и при этом оказывается обобщаемым на другие архитектуры оптимальная доля маскируемых патчей изображения 25 отбор с помощью gradcam лучше случайного выбора здесь однако возникает вопрос насколько полезно такое сжатие так как количество операций при прогонке что исходной что маскированной картинки для большинства архитектур одна и та же кроме того за счет уменьшения количества цветов прочих алгоритмов сжатия можно добиться более сильного сжатия оптимальное число корзин в районе 10 для задач из компьютерного зрения без просадки в качестве удается сжать датасет на 20 40 it aint much but its honest work датасет с инструкциями же удается сжать куда лучше 8098 вероятно потому что рассмотренные задачи для компьютерного зрения требуют существенной перестройки модели а instruction tuning небольшая корректировка весов модели под желаемый паттерн поведения кроме того было показано что удачно выбранное подмножество из alpacaset лучше всего alpaca кроме того метод работает за разумное время от силы час но непонятно для какого датасета задача полезная и востребованная однако сам метод не имеет какихто строгих теоретических гарантий и имеет ограниченные возможности по сжатию датасетов хоть и превосходит альтернативы было бы интересно прогнать на там как раз больше миллиона инструкций и для генеративных моделей
83,2023-09-08,статьи нет но обещают свершилось тысячелетний сокол расправил крылья и явился широкой публике во всей красе самая большая модель в публичном доступе побеждающая все открытые модели на бенчмарках и коегде даже проприентарные охладим бурю эмоций и перейдем к сути дела модель и правда самая большая параметров чуть больше и обучалась эта здоровенная птица на токенов против у различных версий из и других источников диалогов статей кода датасет настолько велик что даже это немалое число меньше чем одна эпоха длину контекста 2k токенов не меняли возникает вопрос о computeоптимальности модели ибо масштабирование по размеру модели больше чем по количеству данных а chincilla law предписывает масштабировать модель и данные примерно одинаково на стандартных бенчмарках falcon180b опережает все иные модели из находящихся в публичном доступе без instructionfinetuning хотя памятуя о том что ранее была выявлена лажа с валидацией меньшей версии модели 1 требуется независимая экспертиза научного сообщества для проверки справедливости заявлений архитетурно большой сокол не отличается существенно от меньших соколиков за исключением multiquery attention по аналогии с llama2 учитывая колосалльный размер модели ее инференс и файнтьюнинг представляет определенные сложности если самую большую llamallama2 можно без проблем засунуть на одну a100 80gb а в 4бита через bitsnandbytes gptq интеграцию и на a100 40gb rtx a6000 то falcon 80gb не влезает целиком ни на одну gpu стандартными методами без квантизации в менее чем в 4 бита блог содержит информацию о требуемых ресурсах для запуска модели однако есть вопросы к приведенным в таблице цифрам ибо qlora вряд ли требует меньше памяти чем инференс с gptq с той же битностью с 4 битной квантизацией на батче размера 1 и последовательностью длины порядка 1k должно выходить 90 gb без информации о длине последовательности непонятен расход память на kvкэши некоторые утверждают 2 что falcon180b лучше справляется со сложными промптами чем gpt35 и llama2 возможно статья приоткроет интересные подробности об обучении модели хотя скорее всего размер модели и данных играют определяющую роль статья про llama2 примечательна в первую очередь дообучением модели под чатбота и заточкой ее под полезность и безопасность предполагаю что нечто подобное будет предьявлено и для falcon180b 1 2
84,2023-09-10,кода нет с ростом размера моделей используемых в разных приложениях все острее встает проблема о том как обучать нейросети в условиях ограниченной памяти gpu на gpu надо както разместить модель состояния оптимизатора и промежуточные активации необходимые для большинства операций на обратном проходе для экономии памяти на состояниях оптимизатора существуют различные методы но память расходуемая на хранение активаций тоже бывает весьма существенной особенно при обучении на больших батчах и длинных последовательностей есть давно известный способ позволяющий экономить расход по памяти за счет дополнильных вычислений но в идеале хотелось бы обойтись без этого в рассматриваемой статье предлагается делать backprogation только через часть выбранных случайным образом токенов одинаковых для всех блоков трансформера а для остальных можно высвобождать активации после выполнения операции при длине последовательности и количестве выбранных токенов экономия по памяти раз предложенный подход применяется к задачам классификации и в качестве конечного состояния трансформера подающегося в классификатор используют усредненный эмбеддинг выбранных токенов авторы проверяют свой метод на бенчмарках из используя маленькую по современным меркам bertlarge модель при одинаковых гиперпараметрах процедуры обучения за исключением подбираемого индивидуально под каждый метод learning rate selective finetuning c k16 не сильно уступает fullfinetuning и слегка опережает lora далее авторы показывают что их метод гораздо лучше масштабируется с размером батча по сравнению с альтернативами кроме того selective finetuning можно применять в связке с lora для экономии на активациях и состояниях оптимизатора одновременно потом приводится ablation качества с количеством выбранных токенов первоначально качество модели сильно улучшается с ростом k но затем выходит почти на плато идея проста в реализации и довольно интуитивна интересно насколько хорошо полученные в статье результаты будут обобщаться на другие задачи например пресловутый instructing finetuning да и glue содержит преимущественно последовальности длиной в несколько десятков токенов идея об экономии памяти за счет использования части токенов не нова например работы по token pruning и merging и др предлагают оставлять только часть или объединять в один несколько токенов по ходу прогонки последовательности через трансформер здесь же длина последовательности фиксирована и экономия достигается за счет backward pass для полноты картины нехватает сравнения с gradient checkpointing по скорости шага обучения и расходу памяти
85,2023-09-14,на текущий момент диффузионные модели безусловный лидер среди различных семейств генеративных моделей однако их существенным недостатком является итеративная природа генерации вынуждающая несколько раз прогонять сеть чтобы получить выход приемлемого качества а в идеале бы хотелось сразу из шума и промпта получать конфетку для уменьшения количества шагов генерации были разработаны продвинутые солверы способы дистилляции учителя в ученика с меньшим количеством шагов сэмплирования но при стремлении количества шагов к единице у всех методов неизбежно проседает качество генераций в режиме одношаговой генерации новые архитектуры ganов которые как известно не обучаются без жертвоприношений богам хаоса и разрушения все еще остаются лучшими по качеству была еще работа по но ее не проверяли в практически интересном сценарии возникает резонный вопрос в рассматриваемой предлагается способ одношагового сэмплирования который как утверждается дает одновременно хорошее качество генераций и работает всего за шаг
86,2023-09-14,процесс генерации диффузионной моделью можно рассматривать как перемещение в пространстве состояний из шума обусловленного на чтото в изображение или любую иную сущность перемещение в пространстве состояний задает некоторую траекторию вообще говоря довольно произвольной формы описываемую некоторым дифференциальным уравнением уравнение не имеет аналитического решения поэтому приходится решать его численно в пределе бесконечно мало шага траектория полученная численными методами совпадает с истинным решением но на практике количество шагов ограничено и чем больше шаг тем сильнее полученная ломаная отклоняется от кривой отсюда возникает мысль а что если вместо того чтобы улучшать солвер выпрямить траектории в пределе идеально прямой траектории от шума до картинки самый простой солвер будет попадать идеально в яблочко а чтобы выпрямить траектории авторы предлагают метод итеративного выпрямления траекторий суть метода заключается в следующем у нас есть изначальный метод генерации сеть солвер выдающий некоторые траектории xt а новая сеть должна за один шаг стартуя из начальной точки попасть в но уже по прямому пути а затем процесс повторяется с использованием сети с последней итерации для генерации траекторий таким образом постепенно траектории становятся все более и более прямыми фиксированная точка итеративного процесса когда прошлый генератор траекторий совпадает с текущим и есть генератор идеально прямых траекторий но процесс сходится к идеально прямым траекториям только в пределе бесконечного числа итераций что недостижимо на практике но оказывается что уже пары итераций выпрямления достаточно для получения достаточно прямых траекторий далее последнюю модель с rectifiedflow дистиллируют в модель предсказывающую картинку из шума за раз в качестве функций потерь на данной стадии используют mse и the lpips более коррелирующий с человеческим представлением о качестве изображения
87,2023-09-14,за основу авторы берут v14 и ее же пробуют наивно дистиллировать по шагам на исходных траекториях и с использованием наивная дистилляция приводит к сильной просадке качества и некрасивым нереалистичным картинкам в то время как полученные при дистилляции с rectifiedflow картинки выглядят значительно лучше хоть все еще не дотягивают до уровня базовой модели сам rectifiedflow при том же числе шагов что и базовая модель имеет то же качество далее авторы экспериментируют с разными параметрами и модификациями сети 2 этапов выпрямления достаточно и большее количество шагов выпрямления уже не отражается существенно на качестве для повышения выразительности сети предлагается соединить как два вагона железнодорожного состава исходную сеть с ее копией тоже инициализированной весами stable diffusion полученная сущность называется t при значительном приросте качества снижении fid с 20 до 137 время инференса растет не так сильно c 009с до 012c на сэмпл архитектурный stablediffusion которая учится предсказывать за один шаг называется а stacked версия конечный пайплайн обучения выглядит следующим образом 1 2 шага rectifiedflow второй шаг с большим размером батча 2 дистилляция в instaflow с mse лоссом 3 дистилляция в instaflow с lpips лоссом почему дистилляция проводится с разными лоссами а не их взвешенной комбинацией не понятно
88,2023-09-14,instaflow09b17b в 30 24 раза быстрее базовой stablediffusion хоть и с некоторой потерей в качестве 13101183 против 962 на замерах fid30k на ms coco 14 при том же бюджете на время модель выдает сопоставимые метрики с ganами превосходя но проигрывая потом авторы сравнивают генерации stablediffusion и instaflow на разном количестве шагов генерации и stablediffusion ожидаемо дает низкокачественные и мыльные картинки на малом количестве шагов а instaflow вполне приемлемого качества если смотреть на конкретные пиксели при отдельности то в stablediffusion их значение меняется по сложным траекториям а у recitifiedflow почти по прямой кроме того мало чувствительна к guidance scale что безусловный плюс и позволяет сэкономить по памяти так как не требуется дополнительно прогонять unconditioned расшумление в classifierfree guidance авторы статьи утверждают что не имеют обьяснения данному эффекту вероятно classifierfree guidance нужен чтобы не сбиться с траектории генерации а так как у нас теперть прямые траектории то и слететь с них не так просто вдобавок полученная модель дает красивые интерполяции в латентном пространстве по всей видимости опять изза выпрямленных траекторий генерации instaflow из коробки комбинируется с refiner и superresolution в и полученные картинки выглядят вполне себе сносно вся процедура обучения занимает 108 для instaflow09b и 130 для instaflow17b a100 gpu дней соответственно что не так много по нынешним меркам stablediffusion v14 обучалась 6250 a100 gpu дней обучение rectifiedflow и дистиллированной модели не успело сойтись до конца так что при более длительном обучении качество может еще возрасти
89,2023-09-14,метод довольно прост и интуитивен хоть и поди догадайся так сам придумать просадка по качеству генераций довольно заметна по сравнению с исходными моделями но при таком ускорении результат все равно довольно впечатляющий наверняка ктото из бигтехов исследовательских лабораторий с большими вычислительными ресурсами попробует воспроизвести идею генерация картинок за прогон это движение в сторону к realtime генерации снижении расходов на генерацию контента улучшение useer experience будем посмотреть
90,2023-09-20,диффузионные модели в процессе обучения нелегкому искусству расшумления картинок и прочих данных попутно выучивают представления которые могут быть применены в других приложениях ранее было что на unetа можно обучить который дает неплохое качество на задаче сегментации кроме того хоть обучающая выборка не содержит явно информацию о расположении объектов в трехмерном пространстве диффузионные модели обладают понятием о 3dгеометрии в ряде работ было что сеть для генерации изображений можно без файнтьюнинга превратить в генератор объёмных моделей в данной работе авторы находят еще 2 применения внутренним представлениям и в данной статье используются промежуточные представления важно что именно первой версии ибо вторая обучалась с depth prior и явно получала информацию о картах глубины тем интереснее что сеть сама по себе имеет понятие о глубине и расстояниях предполагая что внутренние представления сети уже достаточно информативны себя можно предположить что даже простой классификатор линейный слой поверх признаков будет давать неплохое качество в данной работе берут признаки с разных слоев так как ранее данные feature maps были наиболее полезны в прошлых работах рассматривают две постановки задачи где предполагается разделять примечательные объекты и фон и с вещественными метками
91,2023-09-20,для обучения линейных классификаторов авторы генерируют датасет из 1000 картинок с помощью stable diffusion и размечают на salient object distinction и depth estimation при помощи моделей и промпты для картинок берут из затем из датасета фильтруется непотребный контент и изображения без понятия о глубине то что остается содержит 617 примеров и разбивается на обучающую и тестовую выборку в отношении 246371 пример метки имеют разрешение 512x512 а предсказания классификатора куда меньшее разрешение скрытых представлений stable diffusion потом последние интерполируют до размера конечного изображения для оценки качества сегментации salient objects используется и для предсказания глубины для saliency distinction depth estimation пробуют признаки с разных блоков unetа и разные шаги диффузии существенной разницы между выбором блока нет а вот что примечательно понятие о глубине возникает на ранних шагах расшумления еще задолго до того как изображение напоминает чтото осмысленное то есть расположение обьектов на сцене формируется еще до того как из шума начинают выделяться обьекты интересно что даже карты активаций низкого разрешения позволяют выделить мелкие обьекты stable diffusion со случайно инициализированными весами ожидаемо дает низкое качество линейного классификатора авторы проверяют что глубина заложена в unetе а не преобразующем из латентного пространства в пространство изображений попытки обучить классификатор на признаках из vqvae приводят к фиаско вдобавок ко всему прочему авторы демонстрируют что с предложенный подход позволяет двигать сцену во время генерации а именно обученный классификатор выделяет некоторую исходную маску есть некоторая желаемая позиция левее правее ниже выше и с помощью градиентного спуска по предсказанному шуму его модифицируют таким образом чтобы целевой объект находился в желаемой позиции фон при этом может сильно измениться
92,2023-09-20,диффузионные модели хоть и обучаются явно только на генеративное моделирование в качестве приятного бонуса содержат в себе кладезь различной полезной информации и в некотором роде могут рассматриваться как foundation models в области компьютерного зрения задача предсказания шума является denseprediction task потому можно предположить что признаки из диффузионок более подходят для сегментациидетекции и прочих задачах с множественными предсказаниями по сравнению с imagelevel обучением классификацией imagenet clip разными ssl методами
93,2023-09-23,в ряде приложений суммаризации и написанных длинных текстов ответов на сложные вопросы возникает необходимость работы с длинным контекстом но обучать на длинных последовательностях как и проводить инференс очень затратно изза квадратичной сложности потому языковые модели обычно обучаются на контексте порядка нескольких тысяч токенов от силы но на более длинных последовательностях качество резко просаживается в литературе были предложены разные стратегии дообучения на длинные последовательности но стандартное обучение вычислительно затратно требует нескольких машин для современных llm а стратегии модифицирующие attention retrievalbased подходы обычно несколько хуже по качеству да и не все могут использовать преимущества эффективных cudaкернелов аля flash attention метод прост как пробка и относится к разряду гдето я уже это видел тем не менее любопытен токены группируются по окнам некоторого размера порядка длины контекста на обучении и attention делается только в пределах этих окон чтобы информация протекала между группами в половине групп маска attention сдвинута на половину размера группы технически это реализовано следующим образом в половине голов attention применяются обычные окна в другой половине сдвинутые метод явно вдохновлен работой по группы перегоняются в размерность батча перед операцией attention поэтому реализация не требует существенных усилий другой источник дороговизны обучения состояния оптимизатора и потому авторы предлагают использовать lora для дообучения наивное применение без предложенной выше стратегии работы с attention экономит по памяти на градиентах и моментах adam но не самих операциях и активациях и кроме того не обладает достаточной выразительностью чтобы выучивать длинный контекст предложенный же метод по сути работает с исходным attention поэтому не требует существенного дообучения для наилучшего качества оказывается полезным дообучать нормализационные слои и эмбеддинги кои не сильно прибавляют к вычислительной сложности
94,2023-09-23,предложенный метод валидируется на ряде бенчмарков по языковому моделированию с большим контекстом pg19 proofpile topic retrieval на longchat для обучения используется longlora работает значительно лучше обычного файнтьюна с lora и не сильно уступает полному дообучению там где это было посильно сдвиг окон важен для качества причем если его делать в головах attention а не в чередующихся последовательных блоках качество немного выше dilated sparse attention на данных задачах показывают себя плохо упомянутый ранее тюнинг эмбеддингов и нормализаций называемый lora неплохо накидывает дообученная таким образом llama2 13b выступает на одном уровне или даже бьет специализированные модели под длинный контекст такие как простая идея которую можно быстро применить не хватает однако сравнения с парой бейзлайнов например не требующей вообще никакого дообучения да и непонятно насколько подход универсален
95,2023-09-29,кода нет прунинг и квантизация две широко известные стратегии сжатия и ускорения нейронных сетей с тем или иным успехом применяемые в различных задачах и приложениях но если стоит выбор между этими двумя то какой выбор следует сделать принять красную или синюю таблетку ранее в литературе не было полноценного сравнения прунинга и квантизации и в статье авторы пытаются дать ответ на обозначенный выше вопрос в данной работе авторы используют симметричное квантование к ближайшему значению на сетке roundtonearest и magnitude неструктурированный pruning как простые для анализа и самые распространенные на практике размер и качество модели сравнивается относительно fp16 модели
96,2023-09-29,в первой части исследования авторы смотрят на между исходными и сжатыми весами или если быть точнее разницы сначала рассматривают аналитические распределения и обрезанное чтобы моделировать тяжелые хвосты для квадратичная ошибка нормированная на плотность распределения для квантизации возрастает между узлами решетки но при этом существенно ниже чем максимальная ошибка у прунинга при той же степени сжатия при уменьшении модели в фиксированное число раз ошибка квантизации всегда ниже таковой у прунинга для при сильном сжатии и меры тяжести хвостов распределения прунинг может быть лучше квантизации однако такое распределение редко встречается на практике затем авторы берут из torchvision и вновь смотрят на ошибку приближения весов и почти все оказывается что при заданном сжатии у квантизации ошибка меньше далее авторы смотрят на ошибку уже на выходе слоя и рассматривают более продвинутые алгоритмы прунинга и квантизации в сценарии posttraining compression то есть без дообучения оптимизирующие ошибку на уровне слоя выводы тем не менее те же что и раньше оказывается что да авторы берут несколько архитектур и прогоняют sparse training quantization aware training соотвественно сжатых моделей на ряде задач компьютерного зрения классификации сегментации детекции и почти всегда квантованная модель оказывается лучше запруненной гиперпараметры процедуры обучения одинаковы в обоих случаях для честного сравнения здесь стоить важную ремарку что модели сжимают за один раз в то время как для прунинга куда оптимальнее сжимать итеративно и тот же mobilenetv3 вполне реально сжать до 875 с умеренной просадкой в качестве довольно интересное хоть и сравнительно короткое исследование наверное основной вывод естественен что небольшие пертурбации всех весов влияют на качество модели меньше чем большие у части вероятно результат зависит еще от деталей процедуры обучения в особенности weight decay и было бы интересно посмотреть на аналогичное исследования для языковых моделей кроме того прунинг можно комбинировать с квантованием и можно поставить задачу поиска оптимального соотношения между прунингом и квантованием тем для будущих исследований предостаточно
97,2023-10-02,давеча коллеги из хуавей подогнали статью про квантование и дообучение больших языковых моделей как известно квантование позволяет значительно уменьшать размер модели и ускорять большие языковые модели а низкоранговые адаптеры упоминаемая чуть не в каждом посте дообучать в условиях ограниченных ресурсов однако квантование применяется к исходной модели перед ее дообучением поэтому при вливании низкоранговых адаптеров в модель придется переквантовывать модель что может привести к заметной просадке качества и в этой статье авторы исследую причины приводящие к просадке качества при переквантовании модели и предлагают способ бесшовного слияния lora c весами базовой модели сразу скажу что в статье есть несколько некорректных утверждений и ослабления бейзлайнов по невнимательности или по злому умыслу наиболее близкая по теме статья разобранная ранее напомню что там модель квантуется в 4 бит и поверх квантованной модели обучается низкоранговый адаптер на авторы утверждают что выгода от этого подхода только во время обучения так как на инференсе все равно придется сливать веса с адаптерами но данное утверждение более чем спорно ибо можно параллельно прогонять вход через квантованные веса и floatingpoint адаптер и накладные расходы на последний довольно маленькие так как типичный ранг добавки в сотни и тысячи раз меньше размерности в сети
98,2023-10-02,наивное квантование работает не очень хорошо изза несовпадения степеней свободы у квантования и низкоранговой добавки на каждый входной канал приходится один скейл и один zero вообщето нет и при этом чисел в адаптере но чтобы можно было просто взять и поменять параметры квантизации нужно чтобы все r чисел в адаптере соотвествующие конкретному ряду матрицы были одинаковы что по сути ограничивает lora одноранговой добавкой чтобы както повысить выразительность предлагается о боже квантовать входную размерность и тогда ранг добавки следует делать равным числу групп и после этого можно сливать добавку без проблем тут стоит напомнить что размера 64 которые дополнительно квантуют одна из ключевых идей в qlora чтобы провалидировать предложенный подход авторы квантуют модель с помощью gptq с размером группы 32 и дообучают lora на alpaca и для валидации используются и ряд других стандартных бенчмарков языковых моделей arc hellaswag piqa замечу что используемый размер группы дает более чем 05 бит на параметр что не пренебрежимо мало qalora на 4 битах несколько уступает qlora без вливания но уверенно бьет варианты с вливанием весов и повторным квантованием как и peqa с дообучением скейлов в квантизации метод неплохо себя показывает при низких битностях давая качество статистически выше случайного даже при двух битном квантовании качество случайного классификатора на mmlu 25 далее авторы смотрят на эффект от размера группы и ожидаемо меньшие группы дают лучшее качество так как с одной стороны и приближение исходных весов лучше и больше обучаемых параметров в lora размер подвыборки flanv2 заметно влияет на качество особенно при квантовании в низкую битность интересная постановка задачи и подход однако мотивация метода строится сразу на нескольких неверных утверждениях дороговизне инференса qlora отсутствия квантования малыми группами в qlora используемые группы даже меньше чем в qlora потому расходы на хранение статистик квантования как было выше сказано довольно существенны
99,2023-10-08,кода нет и хрен с ним как известно модели обученные на колоссальных объёмах данных демонстрируют сравнительно простые зависимости качества работы с изменением количества данных и размера модели выражающиеся обычно степенными законами и на основе этих закономерностей подбирают модели по размеру и количеству данных при заданном ограничении на бюджет обучения одна из стандартных методик по уменьшению и ускорению моделей приравнивающая нулю некоторую долю весов тем самым при заданной размерности активаций нейронной сети суммарное количество ненулевых параметров меньше чем у плотной модели возникает вопрос и в приведенной работе авторы впервые проводят систематическое исследование по масштабированию спарсных сетей авторы рассматривают 2 задачи 1 обучение на корпусе 2 обучение на проприетарный гугловский датасет для рассматривают 7 моделей размера от 066m до 424m параметров и 4 конфигурации количества шагов обучения а для 4 модели от 13m до 85m параметров и 3 конфигурации длительности обучения рассматривают 4 уровня прореживания 0 50 75 875 менее 50 не целесообразно рассматривать обычно на практике а выше 875 оптимизация становится затруднительной рассматриваемые датасеты настолько велики что ни в одном из сценариев модель не успевает проделать более одной эпохи тем самым постановка эксперимента удовлетворяет предположению о бесконечности выборки из которой сэмплируются данные первые 25 времени обучение обучается плотная модель следующие 50 времени обучения уровень прореживания постепенно поднимается до целевого значения и последние 25 модель обучается с постоянным прореживанием
100,2023-10-08,авторы отталкиваются от стандартной формулы по количеству данных и размеру модели в ней три аддитивных члена 1 спадающий степенным образом с размером выборки 2 спадающий степенным образом с размером модели 3 неустранимая ошибка некоторая константа однако не очевидно каким образом sparsity будет входить в конечный закон чтобы угадать форму закона авторы прогоняют эксперименты с перечисленными выше конфигурациями и обнаруживают что 1 графики лосса против количества параметров образуют почти параллельные линии 2 чем выше степень прореживания тем меньше лосс но выигрыш от прореживания быстро спадает с ростом степени сжатия 3 форма кривых лосса против количества параметров почти не зависит от количества данных из наблюдений выше возникает анзац для scaling law c прореживанием вместо константы помноженной на степень от размера модели возникает степень доли ненулевых параметров некоторая константа полученный анзац весьма неплохо согласуется с экспериментальными данными и кроме того экстраполируется на большие модели например вдобавок к конфигурациям t5моделей рассмотренных в работе впридачу берут на порядок большую самой большой модели из списка которая тем не менее хорошо ложится на выведенную зависимость
101,2023-10-08,исходя из полученных scaling law можно вывести оптимальную степень прореживания при заданном количестве ненулевых параметров и flops потраченных на обучение чем больше перебор по длительности обучения по сравнению с тем выгоднее иметь более сильное прореживание авторы рассматривают два способа оценки количества flops 1 в предположении что железо умеет идеально пропускать операции с нулевыми весами 2 и в типичном сценарии где операции со спарсными матрицами требуют столько же вычислений как и с произвольными оба метода будут отличаться на некоторый фактор контуры против для разных значений sparsity при которых данная степень прореживания оптимальна удивительным образом оказываются параллельны scaling law chincilla однако даже для chincilla law оптимальным является ненулевое значение sparsity в предположении идеальной утилизации спарсности при обучении 50 модель требует в примерно раза для vit раза для t5 больше flops чем предсказание chincilla law для dense модели а без этого предположения и соотвественно кажется что это слишком много но на самом деле современные языковые модели обучаются нередко значительно больше чем computeoptimal значение например обучалась в раз больше дальше можно задаться вопросом для 50 sparse модели соответствующая ей модель должна быть в раз больше и для 75 те преимущество от sparsity уменьшается с увеличением степени прореживания примечательно что закономерность примерно одна и та же для vit и t5 хотя модели и задачи совершенно разные затем авторы исследуют scaling laws для полуструктурированной sparsity n ненулевых значений данный паттерн более ограничителен потому закономерно ожидать что он менее эффективен чем произвольное расположение нулевыхненулевых весов 24 паттерн поддерживаемый nvidia gpu начиная с ampere не уступает 50 неструктурированной sparsity а вот 14 28 уже заметно хуже с точки зрения масштабирования чем 75 sparsity
102,2023-10-08,на практике чаще берут обученную модель и сжимают ее авторы берут три модели s m b small medium base а 16 размер патча и прореживают их тем же самым способом что sparse модели в экспериментах выше только прореживая сразу а не через 25 времени обучения используя бюджета на обучение плотной модели для 50 75 сжатия такой способ в раз эффективнее чем обучение sparse модели from scratch но при большем сжатии выигрыш уменьшается по всей видимости причина этого в том что модель сильно просаживается по сравнению с исходной плотной если учитывать бюджет обучения плотной модели в суммарных затратах на создание sparse модели заданного качества то генерация sparse модели с нуля значительно эффективнее весьма интересное и нужное исследование мотивирующее дальнейшую разработку железа и алгоритмов способных работать с прореженными матрицами при фиксированной производительности и памяти железа по всей видимости оптимальнее всего будет брать большую насколько возможно модель с некоторой долей нулевых весов и квантованную в низкую точность дальнейшее повышение эффективности могут дать conditional sparsity архитектуры использующие часть параметров на прямом и обратном проходе как пресловутые и модели
103,2023-10-09,подумал я что длинные полотна подобные shine on you crazy diamond с детальным описанием экспериментов выходят чрезмерно многословными и содержат слишком многа букв и думаю сократить среднюю длину постов акцентируя внимания только на ключевых моментах и результатах тем самым заодно можно будет повысить и throughput постов
104,2023-10-09,так как голоса по всей видимости разделяются поровну сделаем гибридный формат
105,2023-10-11,в завезли sparsity напомню что произвольный паттерн неструтурированного прунинга не поддерживается на gpu но начиная с поколения ampere реализована аппаратная и программная поддержка 24 паттерна где на 2 позициях из 4 стоят ненулевые веса в общем случае nm sparsity n ненулевых весов из m до недавних пор чтобы воспользоваться 24 sparsity нужно было напрямую использовать ядра из или или пользоваться скомпилированной с моделью в tensorrt но теперь semistructured sparsity доступна почти всем желающим теоретическое ускорение и сжатие которое дает такой паттерн 169 для half precision и 1610 для int8 но на практике увы не все так радужно реальное ускорение инференса в районе 1030 в хорошем сценарии сжатие в 24 без дообучения обычно заметно просаживает качество за исключением совсем больших моделей но его можно быстро восстановить дообучая на целевой задаче в приведенном примере c bert качество сжатой модели такое же как у и исходной после дообучения при маленьких батчах инференс bert c 24 sparsity работает медленее чем dense матричные операции но с ростом батча выигрыш от разреженности становится заметным и стабилизируется в районе 2030 заметим что в bert сравнительно небольшие матрицы и на условной llama выигрыш будет наблюдаться уже на батче с 12 последовательностями
106,2023-10-13,большинство методов прунинга сжимают все слои нейронной сети с одинаковой силой естественно предположить что слой слою рознь и какието можно сильно проредить не потеряв заметно в качестве а спарсификация других же напротив будет очень болезненно восприниматься сеткой потому при некотором целевом уровне сжатия отпимальным будет скорее всего неоднородное распределение процента нулевых весов по разным слоям но как найти это распределение современные sotaметоды сжатия llmок опираются обычно на гессиана которое ничего не знает о влиянии данного слоя на конечный выход как вариант можно оценить важность слоя с помощью градиентов модели в частности используя диагональ матрицы фишера но для многомиллиардных моделей подсчет градиента и хранение промежуточных активаций удовольствие не из дешевых потому в данной статье предлагается заменить честный градиент его приближением нулевого порядка а именно сэмплируется гауссов шум считается разница значений лосс функций при сдвиге в положительном и отрицательном направлении от данной точки и делится на величину шума производная по случайному направлению по существу похорошему требуется много сэмплов чтобы оценить аккуратно градиент но в этой работе утверждают что одного достаточно вооружившись определенным выше критерием важности слоев можно для каждого слоя задать свою степень сжатия и применить известный метод прунинга авторы валидируют метод на задача vqa на mmlu на imagenet1k не то чтобы подсчет градиентов был непосильной задачей для моделей этого размера но зато можно оценить качество приближения нулевого порядка по сравнению с честным фишером в качестве метода прунинга используется где заменили послойный гессиан его диагональю и модели не дообучаются после прунинга предложенный метод неплохо накидывает по сравнению с uniform sparsity при сжатии 5060 однако просадка все же довольно существенна по сравнению с плотной моделью во всех трех сценариях приближение градиента разностью почти не просаживает качество по сравнению с использованием истинного градиента найденные sparsity распределения тоже близки метод сильнее прореживает текстовую модель в blip чем vision большее количество шумов для оценки градиента не влияет на качество на текущий момент при определении важности и влияния слоя на конечный выход модели человечеству приходится оперировать эвристиками основанными на величине активаций градиентов но исчерпывающего удовлетворительного решения пока нет разработка некоторого надежного подкрепленного теоритическими гарантиями метода позволит сделать большой шаг в сжатии моделей и построении эффективных архитектур странно что статья хоть и предлагает метод экономичный по памяти не валидируется на условной llamallama2
107,2023-10-23,линейные слои и свертки имеют в общем случае квадратичную сложность по числу каналов в сети аналогично операции обрабатывающие глобальный контекст attention свертки с большим ядром пространственные проекции в квадратичны по длине последовательности пространственной размерности для специальных классов матриц преобразований можно добиться субквадратичной сложности разреженных блочнодиагональных или обладающих определенной структурой и в качестве иллюстративного примера уместно привести имеющее сложность однако логично ожидать что специальные классы матриц будут обладать меньшей выразительностью по сравнению с линейными преобразованиями общего вида потому хочется найти такой класс который бы с одной стороны давал существенную экономию по вычислениям и в то же время не терял в качестве и в этой работе авторы предлагают заменить трансформерные блоки на поканальные и пространственные проекции параметризованные матрицами матрицы впервые появились в идея довольно простая каждая матрица параметризуется последовательностью блочнодиагональных матриц между которыми вставлены матрицы перестановок перестановочные матрицы эффективно обеспечивают dilation между размерностями матрицы вообще говоря можно рассматривать последовательность нескольких dilation вида но в данной для простоты работают с p 2 количество параметров в такой матрице и сложность вычислений пропорциональна в исходной статье про monarch замена стандартных линейных слоев на monarch в трансформерах сработала весьма недурно а в этой статье пошли дальше и используют монархов для обработки последовательности состоит из содержащего длинные свертки и gating и обычная mlp с монархами в качестве линейных слоев
108,2023-10-23,предложенную архитектуру валидируют на ряде задач и архитектур 1 masked language modeling на bert 2 классификации изображений с vit 3 causal language modeling по типу gpt при тех же скрытых размерностях m2bert m2vit имеет на меньше параметров чем базовая transformer модель при этом не уступая в качестве а если увеличить ширину модели чтобы сравняться по числу параметров то имеет место даже некоторый прирост на causallanguage modeling предложенная модель тоже показывает себя немного лучше gpt2 monarch mixer заметно лучше масштабируется с длиной последовательности в соотвествии с теоретическими выкладками на больших длинах последовательностей прирост порядка против bertbase с vanilla attention и против flash attention ускорение наблюдается как на gpu так и на cpu использование структурных матриц вместо матриц общего вида интересное направление в разработке нейросетевых архитектур при том же количестве вычислений возможно работать с картами признаков большей размерности однако без крупномасштабных экспериментов на современных больших языковых моделях нельзя доподлинно оценить обладают ли монархи и иные представители бабочек лучшей масштабируемостью по сравнению с первозданным трансформером кроме того в сравнениях по throughput везде фигурирует flashattention1 а есть flashattention2 с еще лушчей утилизацией железа
109,2023-10-29,кода нет но у вас все равно нет jft4b и tpuv4 чтобы воспроизвести экспы народная мудрость гласит что масштабируются лучше чем с при наличии и но убедительного экспериментального подтверждения в поддержку данной гипотезы никто не проводил в имеется сравнение эффективности maeпредобучения по сравнению с vit но на меньших масштабах данных исследователи из deepmind прогнали ряд сверточных сеток на большом датасете и продемонстировали что cnn масштабируются трансформеров в качестве семейства сверточных сетей авторы рассматривают ы одно из последних достижений в разработке cnn до того как vitы завладели нишей разные модели семейства отличаются глубиной больше номер больше глубина и шириной более широкая модель обучается все хозяйство на проприетарном jft4m и чипах tpuv4 сверточные сети удовлетворяют похожим убывания лосса от бюджета обучения что и трансформеры при сопоставимых бюджетах сверточные сети выдают сопоставимое качество авторы учитывают использование более новых tpuv4 против tpuv3 в прошлых работах и потому дают поправку при сравнении бюджетов самые большие модели из семейства обученные в течение 100k tpu часов выдают 90 топ1 точности на imagenet1k при дообучении на нем использование для самой большой модели накидывает аж на imagenet1k очередной голос в пользу того что важны не сколько архитектурные детали сколько количество данных и время обучения на большом объеме данных inductive bias уже не имеет особого значения но и трансформеры вероятно не обладают чудесными свойствами с точки зрения масштабируемости тем не менее для пущей убедительности в данном отчете было бы неплохо увидеть аналогичные кривые лосса против часов обучения для семейства vitов три точки отвечающие трансформерам недостаточно репрезентативны интересно если провести подобный эксперимент для lstm с поправкой на утилизацию железа будет ли наблюдаться подобная картина
110,2023-11-04,сегодняшний пост можно считать юбилейным мой скромный канал преодолел психологический рубеж в 100 подписчиков за что большое спасибо милане современные алгоритмы posttrainingquantization вполне успешно квантуют большие языковые модели в 34 бита благодаря чему многомиллиардные модели становится возможным помещать на одну видеокарту но что если поставить еще более амбициозную задачу уместить на одном хосте модель с триллионом и более параметров например гугловский c 16t весов сие чудище настолько огроменное что при квантизации в 4 бита не поместится даже на целую стойку a100 что уж говорить о более скромном сервере из rtx3090 оказывается что данные модели обладают замечательной сжимаемостью без заметной просадки в качестве их можно квантовать менее чем в один бит что позволяет поместить самый большой switch transformer на 8 rtx 3090 24 gb или 4 a6000 48 gb рассматриваемый в этой статье класс моделей смеси экспертов состоят из блоков которые используются для всех входов и множества реплик от 128 до 2048 в семействе switch transformer 2слойных сетей называемых каждый из которых обрабатывает только часть входных данных специальная сеть предсказывает к какому эксперту направить данный токен при обработке последовательности для каждого ее элемента используется только часть параметров что дает существенную экономию вычислений по сравнению с обычным трансформером эквивалентного размера сохраняя при этом его выразительность однако сама модель остается все еще очень большой и занимает много места в памяти деваться некуда надо сжимать
111,2023-11-04,ранее показал сильный результат при квантовании больших языковых моделей потому авторы статьи его же создатели берут его за основу однако наивное применение сопряжено с определенными сложностями для оценки гессианов нужно прогнать последовательности через каждого эксперта которые хоть и небольшие но их много авторы прогоняют последовательность через блок и для каждого эксперта сохраняют в оперативную память активации нужные для оценки гессиана и распределение токенов чтобы повысить скорость алгоритма квантизация нескольких экспертов проводится параллельно 16 экспертов обрабатываемых параллельно дают прирост 6x по сравнению с обработкой каждого эксперта по отдельности чтобы все завелось потребовался еще ряд мелких хаков 1 более сильная регуляризация 2 использование округления к ближайшему целому вместо gptq если матрица оказалась плохо обусловленной 3 ограничение на число токенов для каждого эксперта чтобы не создавать большие тензоры кроме того полезным оказалось не учитывать специальные токены при калибровке для сжатия весов используется поканальная т когда каждый вес принимает только значения минимальное 0 и максимальное обычно веса имеют нормальное распределение сконцентрированное около 0 потому значительная доля весов 8590 становится нулевой потому возникает идея както использовать это свойство стандартные форматы хранения sparse матриц с метаданными ограничены по сжатию а с переменной длиной кода не получается эффективно реализовать на gpu потому авторы предложили свою схему энтропийного кодирования с фиксированной длиной символа но кодирующее переменное число бит которое может быть эффективно реализовано на gpu сжатие не шенноноптимально зато процедура кодирования позволяет эффективно использовать gpu и далее описывается процедура и реализация cuda kernel
112,2023-11-04,авторы рассматривают семейство моделей switch transformer параметров по существу это encoderdecoder t5модель с параллельными feedforward блоками switch transformer обучался на на c4 потому метод тестируют на валидационной выборке из этого датасета замеряя перплексию вдобавок замеряют аналогичным образом качество на нескольких подмножествах из opensource версии датасета llama квантуются attention слои занимающие крохотную долю общего числа параметров держатся в исходной точности тернарная квантизация с энтропийным кодированием дает бита на параметр при этом полученная модель не сильно просаживается в качестве по сравнению с исходным представлением в bf16 таким образом имеем сжатие модели примерно в удивительно что и квантование к ближайшему соседу работает очень неплохо хоть и заметно хуже gptq прогонка алгоритма на одной a6000 занимает для самой большой модели предложенная процедура кодирования весов дает небольшой оверхед к общему времени вычислений и итоговый прирост времени выполнения всего 5 по сравнению с тем что было бы у исходной модели если ее можно было бы уместить на одну стойку как я понимаю оценивали скорость на основе одного блока attention полносвязные эксперты что же делает экспертов настолько легко квантуемыми по всей видимости сама процедура обучения и инференса предполагает что в большинстве вычислений данный эксперт не будет задействован потому модель устойчива к отсутствию операций в большинстве слоев кроме того накопление ошибки меньше так как есть эффект от округления только части весов а используемые во всех вычислениях attention слои неизменны крутой и практически полезный результат мотивирующий дальнейшее развитие и большее широкое применение смесей экспертов в то время как обычные языковые модели плохо сжимаются менее чем в 3 бита без значительной просадки в качестве moe допускают куда более агрессивное сжатие сама концепция moe и условных вычислений кажется более чем естественной для foundation моделей для решения задачи по алгебре вряд ли полезно знать название всех персонажей в сильмариллоне или формулу тринитротолуола gpt4 по слухам представляет собой смесь экспертов сдается мне llama3 тоже будет moe запомните этот твит
113,2023-11-07,в недавнем канал veritasium затронул важную проблему популяризации науки научный прогресс носит преимущественно инкрементальный характер большие прорывы если не говорить про dl происходят редко и ценность многих результатов трудно обьяснить неподготовленной публике потому популяризаторы науки и сми прибегают к обману чтобы набрать классов кликбейтные заголовки greatest breakthrough of the 21 st century future has come вечный двигатель возможен и тому подобное даже уважаемые издания например грешат этим в статье про кротовую нору они утверждают что ученые создали кротовую нору хотя в действительно речь идет о моделировании ее с помощью квантового компьютера результат на самом деле выдающийся но тем не менее до межгалактических путешествий доставки алиэкспресс с альфацентавры еще далеко результаты опровергнутые впоследствии цитируются в среднем больше недавняя история с вызвала бурный ажиотаж в научном сообществе множество лабораторий попытались воспроизвести результат в скорости громкий результат был опровергнут но внимание которое привекли к себе ребята из кореи среднестатический ученый не привлечет за всю свою научную жизнь причина данного явления понятна ученымпопуляризаторам науки выгодно поднимать хайп накручивать просмотры все мы в конечном итоге боремся за место под солнцем потенциальные дивиденды большие а в случае неудачи никто и не вспомнит как бороться с подобной ситуацией в науке автор ролика и его собеседники предлагают поднимать эту проблему говорить о ней держать сообщество в курсе окончательно побороть ее вряд ли возможно на мой взгляд основный потенциальный ущерб от недобросовестной популяризации науки в том что некоторые интересные и полезные отрасли могут не получать должного финансирования со стороны курирующих структур и насыщения кадрами тем не менее пассионарии и ученые верящие в перспективы полезность своей деятельности или просто получающие кайф от нее по всей видимости будут заниматься ей вне зависимости от внешних обстоятельств то же машинное и глубокое обучение до того как проникнуть во все отрасли жизни получить широкое признание долгое время развивалось в относительном андерграунде
114,2023-11-11,нет это не печать с раскладкой на другом языке и не вывод llm квантованной в один бит это список флагов интеловского процессора
115,2023-11-21,гугл выдаст кучу статей с дежавю в названии do ssl models have déjà vu a case of unintended memorization in selfsupervised learning dejavu conditional regenerative learning to enhance dense prediction deja vu continual model generalization for unseen domainsdejavu a glimpse on radioactive softerror consequences on classical and quantum computations déjà vu a contextualized temporal attention mechanism for sequential recommendation déjà vu an empirical evaluation of the memorization properties of convnets так что авторы немного прогадали с оригинальностью названия перейдем же к сути дела как известно нейронные сети перепараметризованы и имеет место значительная избыточность в весах моделей значительную долю весов можно отбросить без заметной просадки в качестве на этом свойстве основаны методы прунинга тем не менее чтобы решать широкий круг задач foundation модель должна обладать значительным количеством параметров чтобы хранить в себе большой обьем знаний потому добиться существенного сжатия и ускорения без просадки в качестве крайне затруднительно однако для конкретного запроса будь то последовательность или иной тип входа требуется лишь малая доля аккумулированного в модель знания чтобы взять интеграл от функции не требуется быть египтологом или знать всех представителей рода соколиных
116,2023-11-21,ускорить время работы сети без просадки в качестве можно не используя часть голов в или каналов в слоях первая часть работы посвящена анализу внутренних представлений внутри сети авторы делают два прямых прохода на первом определяют каналы и головы с наибольшими значениями активаций и на втором проходе используют только долю параметров соответствующую самым большим активациям оказывается что таким образом можно пропустить до голов в attention и до в mlp слоях без существенного изменения выхода модели кроме того оказывается что в большинстве голов attention размазан равномерно по токенам и эти головы не выполняют никакой полезной работы при определении важныхневажных для конкретного входа параметров описанным выше образом приходится делать один проход с использованием всех параметров потому толку от него как от козла молока можно ли както определить нужные каналы авторы рассматриваемой статьи делают следующее берут обученную языковую модель и поверх нее обучают слои предсказывать насколько релевантен вход данному каналу ffn голове трансформера на инференсе задается доля весов которую мы хотим использовать и берутся только измерения с самым высоким предсказанным скором выбор нулевых весов зависит от входа то есть контекста потому sparsity носит прилагательное предсказывать важность каналаголовы на основе активаций текущего слоя оказывается технически неэффективно так как определение используемых для данного входа параметров и прогонка через блок должны осуществляться последовательно что снижает утилизацию железа потому предлагается использовать активации с прошлого блока для выбора благодаря наличию residual connections активации слабо меняются от блока к блоку потому активации прошлого блока служат хорошим приближением активаций в текущем блоке и предиктор можно прогонять параллельно с attention и mlp блоком
117,2023-11-21,авторы валидируют свой подход на больших моделях из семейства 66b 175b и через замеры перплексии на wikitextc4 и zeroshot бенчмарках из lmevalharness contextual sparsity в районе даже слегка накидывает в качестве и до не просаживает качестве по сравнению с исходной моделью deja vu ускоряет генерацию с opt175b в по сравнению с fastertransformers и c реализацией трансформера в huggingface сильно неоптимальной при contextual sparsity 75 для результаты и выводы аналогичные затем авторы смотрят на contextual sparsity с ростом количества одновременно подаваемых последовательностей и оказывается что количество каналовголов с большим значением активаций растет медленнее чем линейно с размером батча причем в первых слоях активируются одни и те же каналы и головы во всех последовательностях и различие возникает в более поздних блоках deja vu можно совместить с другими методами сжатия и ускорения в частности квантизацией с одной стороны логичное и в то же время интересное наблюдение интуитивно понятно что все заложенное знание в foundation модель не требуется для конкретного запроса но вопрос о том как эффективно извлекать это знание не перебирая при этом половину книг в эдакой импровизированной библиотеке результаты для opt выглядят весьма впечатляюще однако отсутствие подобных экспериментов на более современных и эффективных моделях пресловутой llama к примеру вызывает смутные подозрения по всей видимости столь высокая разреженность активаций както связана с недообученностью и computeнеоптимальностью моделей тем не менее contextual sparsity перспективное направление для развития эффективных foundation моделей
118,2023-11-23,только мы на днях разбирали как dl сообщество сотрясла статья реализующая данную концепцию в экстремальном объеме согласно аннотации вариация bert предложенная в статье которую без лишней скромности нарекли выдает умопомрачительные показатели по эффективности прямой проход с использованием всего 03 параметров ускорение в раз по сравнению с оптимизированной реализацией прямого прохода ну все значится теперь bert можно гонять хоть на старой нокии или калькуляторе agi в кармане теперь дело времени так ли все замечательно ан нет первый нюанс заключается в том что оптимизируются только блоки а блоки остаются без изменения авторы мотивируют это тем что bert обычно обрабатывает последовательности длины порядка 128 где основные вычисления происходят в feedforward это действительно так тем не менее attention занимает все равно нетривиальную долю вычислительного бюджета потому всю модель в десятки раз ускорить не выйдет
119,2023-11-23,теперь про сам network которая предлагается в качестве альтернатива обычной ffn представляет собой следующее две матрицы размера где глубина дерева а embedding dim соответствующие входной и выходной проекции и функция активации вернее даже правильнее будет сказать что это последовательности матриц размеров отвечающие разным уровням дерева прямой проход выглядит следующим образом асимптотическая сложность алгоритма по промежуточной размерности против у обычный ffn из двух линейных слоев и активации красиво красиво несколько таких деревьев можно прогонять параллельно вариации описанной выше архитектуры называют где количество деревьев а глубина дерева внутренняя размерность таким образом равна в частном случае деревьев глубины 1 имеем привычную архитектуру трансформера
120,2023-11-23,бертоподобная модель с 12 блоками с деревьями глубины те 4095 212 1 нейронами в скрытой размерности ffn в качестве бейзлайна авторы берут crammedbert из где обучили бертоподобную модель на mlm до качества немного уступающему оригинальному bert за день на одной gpu сравниваются со стандартной версией со скрытой размерностью 3072 и версией в которой feedforward_dim4095 из этой же статьи берут гиперпараметры обучения для валидации замеряют точность на бенчмарках из авторы обучают вариации ultrafastbert разной глубины от 1 до максимально возможной при примерно постоянной ширине по качеству все смотрится довольно неплохо с ростом глубины среднее качество на glue просаживается что ожидаемо учитывая что все меньше и меньше параметров используется на инференсе но не драматически сильнее всего страдает качество на оно и тянет усредненный результат вниз гораздо веселее история с замерами свою реализацию fff авторы сравнивают со своими же самописными реализациями операций для бейзлайновой архитектуры сетап сравнения тоже довольно экзотический батч размера 128 последовательностей длины 128 и в качестве l1 бейзлайна они пробегают циклом по всем строкам матриц и вызывают скалярные произведения в качестве l2 бейзлайна копируют матрицы весов batch_size раз и запускают batched matvec что крайне неэффективно по памяти к тому же отсюда и берутся невероятные цифры по ускорению абсолютные цифры по времени инференса в статье тактично опущены идея статьи на самом деле не так уж безумна и если бы существовало железо способное претворить предложенную идею в жизнь эффективно то она бы могла найти широкое применение на практике однако у реальных ускорителей есть множество нюансов с доступами к памяти аппаратной реализацией операций если бы авторы честно упомянули все limitations и провели честное сравнение с бейзлайнами то была бы вполне неплохая статья уровня типичного постера neuripsiclr с разумной идеей но не оставившей большого следа но авторы решили сорвать хайп геростратова слава тоже слава по итоге имеем из мира dl
121,2023-11-23,от вашего покорного слуги сравнения на колабской цпушке их ffn с crammedbert торчовой реализацией в моих замерах время инференса ultrafastbert и baselinebert на батче размера 1 практически интересном совпадает в пределах стандартного отклонения точные цифры будут зависеть от железа и версий библиотек но вряд ли вы где пронаблюдаете 100 кратное ускорение ultrafastbert 438 82 ms not ultrafastbert 405 133 ms
122,2023-11-30,заветным желанием производителей и пользователей диффузионных моделей является генерация в один шаг с сохранением высокого качества в недавнее время появилось множество работ где были предложены решения той иной степени успешности ко всей движухе подключились метры из и выкатили при выборе названия явно вдохновлялись openai генеративную модель достигающую довольно хорошего качества генерации в один или несколько шагов обученную посредством о котором будет рассказано ниже в настоящий момент наиболее успешные подходы по одношаговой или малошаговой генерации сводятся к или старому доброму gan и в данной работе авторы по существу совместили дистилляцию и адверсариальное обучение за основу берут предобученные и дистилляционный лосс представляет собой взвешенный mse лосс между предсказаниями незашумленной картинки сетиученика и сети учителя взвешенный с некоторым коэффициентом зависящим от шага зашумления например alpha_t весом исходного сигнала в зашумленном сэмпле адверсиальный лосс hinge loss c для дискриминатора утверждается что регуляризация особенное полезна при обучении в высоком разрешении дискриминатор инициализируют весами одного из современных feature extractorов dino clip дискриминатор обуславливается на текстовый эмбеддинг промпта и картиночной эмбеддинг незашумленной картинки
123,2023-11-30,обучают две модели 1 addm c 860m параметров из stable diffusion v15 для честного сравнения с бейзлайнами v21 для ablation 2addxl из sdxl текстовый эмбеддинг для дискриминатора получают из и картиночный эмбеддинг из в качестве бейзлайнов выступают прогрессивная дистилляция sd и stylegant реимплементация достигающая даже более высоких метрик чем модель из исходной статьи и конкурентный сравнивают стандартные генеративные метрики fidclip score и пользовательские предпочтения по качеству изображений и соответствию запросу как нетрудно догадаться add разбивает конкурентные подходы в пух и прах один шаг уже работает хорошо а для sdxlturbo бьют даже базовую sdxl с сэмплирования примечателен дистилляционный лосс по отдельности работает плохо лучше всего работает взвешенная комбинация дистилляционного лосса и адверсариального лосса но что любопытно и адверсариальный лосс по отдельности работает почти так же хорошо выбор инициализации для дискриминатора существенно влияет на качество причем лучше всего себя показывает не самый большой vitsmall с обучением обуславливание генератора немного улучшает качество addm лучше большей addxl по fid но хуже по clip score случайно инициализированный студент не способен обучиться до приемлемого качества результат итеративного расшумления сетьюучителем картинки вместо предсказания в один шаг в качестве таргета дистилляции не накидывает результаты генераций в 1 2 4 шага вероятно черрипикнутые выглядят неплохо подход сравнительно простой по сравнению с типичными пайплайнами дистилляции при этом картинки остаются резкими и четкими однако за красоту и скорость инференса все же приходится платить определенную цену снижение разнообразия генераций по существу имеем некоторый tradeoff между gan и vanilla диффузионной моделью
124,2023-12-05,вряд ли кто станет оспаривать утверждение что сжатие и ускорение больших языковых моделей является одной из наиболее приоритетных задач для человечества большинство современных работ посвящего квантованию только весов сети так как в случае инференса с батчом размера 1 основную часть времени занимают операции с памятью а не вычисления однако в некоторых сценариях пользователи могут быть заинтересованы в том чтобы прогонять несколько последовательностей одновременно как в случае обработки промптов тогда ускорение математических операций начинает приобретать смысл в этой статье авторы квантуют и веса и активации в 4 бита для квантования весов используется симметричное квантование а для активаций динамическое границы квантования определяются по время инференса по токенам как известно существуют отдельные размерности где малые изменения значений весов и активаций могут существенно исказить выход потому предлагается их хранить в исходной точности определять их во время инференса неэффективно но оказывается что они находятся на тех же позициях потому их можно определить заранее в этом по существу заключается и основная суть метода кроме того для моделей семейcтва llama2 оказывается предпочтительным квантовать проекцию в mlp в 8 бит вместо 4х так как ее квантование сильно просаживает качество ускорение вычислений достигается за счет того что матричные операции проводятся в int4 название quik расшифровывается как antization to nt4 with gpu ernel support при числе токенов от 1 до 16 операции матричное перемножение memorybound с большим количеством вычисления начинают доминировать
125,2023-12-05,метод валидируют на моделях семейства и для оценки качества замеряют перплексию на wikitext2 и точность на zeroshot из lmevalharness во всех экспериментах берут оутлаеров примерно 3 измерений для opt66b достигает заметно лучшего качества по сравнению с бейзлайнами при квантовании в 4 бита просадка по качеству значительная но приемлемая для многих приложений квантование в 8 бит сохраняет исходное качество для всех рассмотренных моделей предложенный метод дает примерно двукратное ускорение по сравнению с fp16 при квантовании в 8 бит и до 34x при квантовании в 4 бита при 4кратном теоретическом пиковый расход памяти уменьшается от до 35 раз в зависимости от размера модели ablation study показывает что оутлаеров около оптимально квантование в 4 бита сильно ухудшает качество при этом инференс этой группы слоев в 8 битах не слишком сказывается на общем времени работы квантование можно совместить с sparsity но для сохранения качества приходится прунить только attention проекции практически полезный технический результат
126,2023-12-10,модели народ уже на протяжении долгого времени занимается поиском альтернативы архитектуре трансформера однако многочисленные попытки и заявления про рождение убийцы трансформеров выглядели недостаточно убедительными ибо полученные модели сильно не дотягивали до sota моделей и команда из совместно с hazyresearch выпустили которая оказывается достойным конкурентом современным lm уровня и архитектура модели составлена в основном из эффективных сверточных блоков умеющих эффективно обрабатывать длинный контекст и некоторого количества стандартных attention блоков авторы показывают что смесь attention и conv блоков в соотношении 2575 достигает наилучшего качества при заданном размере модели и бюджете обучения одни лишь сверки без attention работают однако хуже чем просто attention кроме того в свертки привносят multihead из attention и утверждается что это накидывает grouped convolution выпустили две модели hessian базовая модель и nous instruction finetuned sh7b опережает llama27b и слегка уступает mistral7b на openllm бенчмарке nous версия лучше файнтьюна llama213bopenhermes но слегка слабее похожего файнтьюна для мистрали на длинных последовательностях 128k токенов предложенная архитектура в 15 раза быстрее чем оптимизированный трансформер с flashattention2 и grouped query attention расход памяти тоже уменьшается до 2 раз на длинном контексте кроме того модель не проседает при увеличении контекста в 2 раза по сравнению с самым большим увиденным во время обучения а если в 48 раз неплохая попытка скинуть трансформеры с пьедестала больших языковых моделей однако наличие некоторой доли attention блоков в конечной архитектуре все же делает пока multihead attention незаменимой компонентой в дизайне llm
127,2023-12-11,прогресс в языковых моделях идет насколько быстро что ни день то новая sotalm которые некоторое время назад выкатили lm mistral которая несмотря на скромные размеры опередила более крупные версии llama2 теперь выкатили уже смесь экспертов где поменяли одну букву в названии mistral mixture в модели 45b параметров и на прямом проходе активируются 2 из 8 экспертов attention блоки задейсвтуются все и в итоге для каждого сэмпла задействуются 12b параметров чуть более одной четверти от общего количества модель обладает следующими фичами 1 хорошо умеет в языки 2 обрабатывает контекст до 32k 3 могет неплохо в код и математику слой распределяющий по экспертам обычная линейная проекция учится вместе с моделью детали обучения на каких данных сколько токенов неизвестны mixtral 8x7b на бенчмарках в основном бьет llama270b gpt35 хоть и в нынешнее время стоит относиться с некоторой осторожностью к подобным заявлениям ибо неизвестно какова вероятность что данные из бенчмарков или похожие на них не попадали в pretrain время инференса у модели как у llama213b при этом перформанс на порядок выше на ряде разных доменов и вроде бы меньше подвержена biasам и галлюнам похоже за смесями экспертов и правда будущее во всяком случае настолько насколько это обозримо в мире dl где революции случаются постоянно
128,2023-12-11,название стартапа кстати вдохновлено маркой риса
129,2023-12-14,imagen 2 гугол выкатил вторую версию своей знаменитой диффузионки на публику следую модной традиции хранить таинственную загадочность создатели предпочли не выдавать военных тайн ограничившись лишь высокоуровневым описанием возможностей модели из блога можно понять или предположить следующее 1 использовались более подробные синтетические описания картинок как в dalle3 2 черрипики выглядят вполне фотореалистично 3 была обучена aesthetics модель на предпочтениях пользователей которая была затем использована для conditioning imagen 2 4 imagen 2 умеет в генерацию аля по нескольким примерам 5 imagen 2 умеет в inpainting модификацию части картинки и outpainting продолжение картинки извне 6 imagen 2 идет вместе с synthid тулзой для обнаружения nsfw контента и ватермарок остальное остается за кадром латентная ли диффузия или каскадная информация о архитектуре и обучении
130,2023-12-19,разнообразные адаптеры и методы стали общепринятым способом дообучения хороший адаптер должен быстро и эффективно обучаться выучивать специфику при этом сохраняя с таким трудом в модели знание пресловутая обучает добавки низкого ранга к матрицам весов в этой статье же предлагается обучать ортогональное преобразование исходной матрицы весов сохраняя похожесть разных признаков между друг другом метод авторы мотивируют использование ортогональных преобразований обучают с уменьшением разрешения и последующим повышением не нормируя веса и активации если на инференсе модели вход и веса сверточного фильтра на их норму то результат реконструкции не испортится в то время как если сохранить только информацию о величине фильтра и входной активации то на выходе будет чтото не очень красивое то есть между векторами более информативен чем а конечный адаптер имеет вид где обучаемое ортогональное преобразование а w0 исходная матрица весов такой адаптер сохраняет сущность с устрашающим и внушающим благоговейный трепет названием гиперсферическую энергию однако суть на самом деле проста и по факту мы хотим сохранить попарные углы между весами матриц но как добиться условия на ортогональность матрицы r авторы используют c кососимметричной однако в исходной форме метод имеет слишком много обучаемых параметров примерно столько же сколько и исходная сеть потому предлагается рассматривать блочнодиагональные адаптеры тем самым снизив сложность до число блоков можно еще сильнее уменьшить число параметров продублировав адаптер во всех блоках до в качестве конкретного примера авторы сравнивают адаптер ранга 8 и с 8 блоками в первом случае 2048 обучаемых параметров против 960 во втором однако для больших матриц выигрыш будет не пользу oft ибо oft масштабируется квадратично по размерам исходных весов вообще говоря ортогональное преобразование может далеко увести от исходных весов потому авторы предлагают накладывать условие чтобы выученная матрица не сильно отличалась от единичной такая модификация называется constrained oft предполагая малое отклонение от единичной матрицы выполнение данного условия можно достичь с помощью проектированного градиентного спуска утверждается что данная опция обладает большей стабильностью
131,2023-12-19,метод валидируют на и других задачах conditional генерации на основе keypoints и сегментационных масок дообучение всей модели методом dreambooth скольлибо продолжительное время приводит к генерации сильных артефактов lora выглядит несколько лучше но все равно начинает генерировать артефакты oft и coft даже после большого числа шагов выдают хорошие картинки в приведенных примерах по метрикам качества похожести dino эмбедов соотвествия текстовому и картичному промпту oft опережает бейзлайны в задаче контролируемой генерации лиц по ключевым точкам oft сходится быстрее чем конкуретные адаптер и при этом достигая более высокого качества аналогичный результат достигается и для генерации по маскам сегментации выглядит как вполне неплохой и разумный адаптер основной областью применения скорее всего будут диффузионные модели тушки из компьютерного зрения и небольшие по современным меркам языковые модели а вообще интересный вопрос какой самый эффективный адаптер по числу параметров в зависимости от задачи и какие исходя из каких соображений следует выбирать тот или иной метод peft под конкретное приложение
132,2023-12-23,смеси экспертов для блоков были неоднократно с успехом применены в больших языковых моделях в частности в недавно вышедшей и gpt4 по тоже ей является однако кроме mlp в трансформере можно добиться ускорения за счет использования за счет части голов в на больших последовательностях attention слои требуют много вычислений и памяти и использование части голов даст некоторую экономию даже если бы статья была анонимной то по цитированиям шмидхубера при каждом упоминании attention можно было бы угадать одного из авторов наивный подход был бы следующий иметь предиктор который предсказывает скор для каждой головы и берет только с наибольшим скором однако проблема в том что при авторегрессивной генерации могут активироваться разные эксперты и в итоге придется все равно хранить кэш keys и values на все головы проблема правда нынче лечится авторы предлагают иметь экспертов каждой головы и активировать только часть экспертов на прямом проходе в этом по сути и вся суть метода подход валидируют на который обучают на 100к шагов не самый типичный setting для 2023 качество замеряют по перплексии на wikitext103 c4 pes2o в бейзлайновом трансформере 2 или 10 голов с одинаковым числом параметров в предложенном switchhead 2 головы с 5 экспертами во всех случаях смесь экспертов имеет столько же параметров сколько и исходная модель в разных конфигурациях q k v и o проекции могут быть как разбиты на экспертов так и нет перплексия везде примерно одинаковая и будто бы метод не проигрывает сильно в качестве исходному трансформеру с 10 головами и уверенно опережает двуглавый вопрос в том правда насколько хорош бейзлайн сравниваются с работой при том же качестве их подход гораздо экономичнее по памяти с экономией в раз в то время как у moa выходит раза далее можно сделать mlp тоже экспертами и получить модель и switchall при примерно той же перплексии расходует снова заметно меньше памяти специализация attention в трансформере полезное направление однако эксперименты выглядят не слишком убедительно нет и замеров ускорения а лишь сравнение между и бейзлайном да и в популярном нынче можно было бы активировать часть query проекций и иметь профит в плане памяти и вычислений
133,2023-12-25,не проплаченной рекламы пост год близится к концу и я захотел поделиться подборкой тг каналов по машинке и глубокому обучению с многими из них многие из вас уже полагаю знакомы но некоторые среди них могут стать для когото открытием список не имеет четкого порядка и элементы ниже в том порядке в каком доставались из сознания автора 1 классный канал с детальными и подробными разборами во многом вдохновивший меня на создание собственного канала кроме того там публикуются новости из мира ai математики философии и всякая сборная солянка 2 короткие разборы и анонсы разных вкусностей из мира cv nlp 3 краткий но исчерпывающий рисерч в области компьютерного зрения и не только если хотите в компактной форме но при этом в достаточном содержании понять суть статьи вам сюда 4 новости из мира nlp где иногда разьясняют всякие прикольные штуки и фишки в современном nlp новости из мира ai и еще про космос 5 классный блог с аннотациями статей по nlp cv находками автора и обзором полезных инструментов и хаков 6 классная подборка всякой всячины из мира ai от образовательных статей до социальных и житейских моментов культура мемы веселье 7 анонсы и новости из мира глубокого обучения и ai преимущественно про nlp но не только сборник новостей образовательных и обучающих материалов с разных источников 8 милый и уютный канал где время от времени появляются хорошие статьи проясняющие тот или иной концепт из области nlp саморазвитие опросы разьяснение и разбор распостраненных и не очень ошибок классные истории из жизни с счастливым концом 9 краткие анонсы и ссылки на разные новинки в области машинного обучения преимущественно прикладного толка и с ориентацией на репозитории где есть разные красивые демки 10 отличный канал с разборами статей из разных областей dl разной длины в том числе и с видеоразборами 11 преимущественно про nlp но и не только подборка новостей из разных областей мемы культура искусство 12 просто самый лучший канал
134,2023-12-27,человечество добилось значительных успехов в ускорении диффузионных моделей путем разработки более совершенных солверов и различных стратегий дистилляции по шагам однако методы генерирующие в один или малое количество шагов так или иначе уступают многошаговым моделям либо по либо по их отсюда возникает а что если не отправлять сразу учителя на пенсию а призывать по мере необходимости когда ученик не способен достичь желаемого качества за счёт этого можно убить двух зайцев иметь в среднем инференс и и на данном соображении построена рассматриваемая статья в качестве учителя берут предобученную sd v15 или sdxl модель ученика инициализируют весами учителя и проводят процедуру полученная модель может за малое число шагов в большинстве экспериментов используют выдавать генерации неплохого качества тем не менее дистиллированная модель все же уступает учителю по оценке разметчиков 50 за учителя против 30 за студента дальнейший анализ весьма примечателен и интересен чем меньше подражает учителю тем больше доля голосов ассесоров сделавших выбор в пользу моделистудента гистограммы отнормированы на суммарное число побед конкретной модели а не число побед для данного расстояния то есть 60 побед на правой гистограмме означает что среди побед студента 60 из них достигаются когда расстояние между студентом и учителем велико характерно что отходит от учителя сильнее на более сложных картинках для оценки сложности картинки используется модель и на длинных текстовых промптах по всей видимости и то и другое реже встречается в данных потому у ученика появляется больший простор для фантазии возвращаясь к исходной задаче и для этого используют модель которая оценивает качество генерации если качество генерации выше некоторого порога то используется генерация студента иначе прибегаем к помощи учителя оценивается по валидационной выборке как некоторый квантиль imagereward для учителя корреляция с человеческими предпочтениями не идеальная около 60 но лучшего результата текущие методы оценки качества изображений пока не могут достичь предлагается два варианта использования учителя 1 regeneration генерация с нуля 2 refinement неполное зашумление результата ученика с последующим расшумлением
135,2023-12-27,в первой серии экспериментов используют в качестве ученикаучителя consistency дистилляция проводится на подвыборке сравнивают модели на промтах из и тандем ученикучитель с 10 шагами генерации в среднем примерно равен и с 25 шагами и 50 шагами генерации учителя соответственно при 15 шагах даже чуть лучше при сравнении использовался подход адаптивный подход лучше чем просто что касается автоматических метрик и имеют примерно одинаковый image reward regeneration лучший alignment с текстом а refinement лучший мера соответствия распределению реальных картинок тандем поверх sdxl сравнивается с sdxl c 50 шагами солвера и опережает sdxlturbo эффективность метода ограничена в первую очередь процедуры оценки качества генерации студента авторы показывают что при наличии идеального асессора генераций можно было бы добиться значительного разрыва с учительским бейзлайном кроме того метод неплохо сохраняет разнообразие генераций учителя и хорошо себя показывает в textguided image editing и контролируемой генерации красивая идея и результаты подкрепленные занимательным анализом использование нескольких моделей для решения конкретной задачи выглядит действительно сильным подходом уже неоднократно возникавшим в основная сложность именно в том как оптимально выбрать ту или иную модель и сколько моделей и какие мы можем позволить держать в памяти и применять
136,2023-12-30,кода нет и не обещают диффузионые модели показывают на данный момент наилучшее качество во множестве задач и приложений однако сама постановка диффузионного процесса где пример из целевого распределения превращается в шум из нормального распределения а модель учится обращать данный процесс предполагает на одном конце распределение реальных данных а на другом чистый шум что не очень подходит для задач таких как и существуют подходы которые с тем или иным качеством справляются с вышеупомянутыми задачами зашумляет входной пример и расшумляет обратно обуславливаясь на целевую задачу и типичные архитектуры диффузионных sr моделей конкатенируют на входе шум и картинку низкого разрешения но эти требуют требуют тонкой настройки и выглядят несколько неестественно возможно ли дать такую постановку задачи которая бы и в данной статье предлагают ответ на данный вопрос сам подход основан на стандартной теории диффузионных процессов однако берется специфичная форма процесса такая что удовлетворяет заданным граничным условиям на обоих концах то что и хочется для задач перехода из одного распределения в другое в литературе такой процесс известен как при заданной начальной точке он называется диффузионным мостом далее авторы предлагают формулировку такого процесса для с постоянной дисперсией и с растущей дисперсией расписания шума предложенный подход обобщает генерацию из шума в целевое распределение и метод проверяют на задачах и как я понял это не про электронику а генерацию сцены по маске сегментации и unconditional генерации предложенный подход заметно опережает ganbased и подходы основанные на диффузионных моделях и работу использующую мосты шредингера по метрикам fid и lpips unconditional генерация из шума тоже работает весьма достойно на cifar и ffhq выдавая качество сравнимое с при том же количестве шагов генерации значение в vp постановке значительно влияет на качество и почти не влияет для ve диффузии кроме того метод имеет дополнительный гиперпараметр определяющий промежуточный шаг в солвере который тоже следует подбирать аккуратно для наилучшего качетства интересный и разумный подход адаптирующий имеющуюся диффузионые техники на задачи перехода из одного распределения в другое было бы интересно провалидировать подход на большем масштабе и более сложных задачах с широким распределением данных и высоким разрешением изображений
137,2023-12-31,вот и подходит к концу год выдавшийся очень насыщенным и непростым в различных сферах но богатый на научные открытия и яркие результаты крутые llmки запитанные разнообразными instruction finetuning техниками и датасетами улучшенные генеративные модели прогресс в компьютерном зрении аудио видео и мульмодальных моделях и многое другое вот что дал нам этот год а какие у вас ожидания от следующего года прорыва в какой области и какого результата вы ждете большего всего
138,2024-01-02,начнем год с маленькой викторинки создадим пустые тензоры из одного элемента следующим образом и заполним их следующим образом
139,2024-01-03,ответ на задачку и обьяснение за
140,2024-01-07,кода нет на текущий моментнаиболее успешные методы по прунингу и квантизации моделей так или иначе опираются на некоторое приближение матрицы вторых производных для определения важных весов и отпимальной сжатой конфигурации чем больше модель тем дороже становится вычисление любого приближения и многие подходы не масштабируются на современные llm и их производные оптимизируют квадратичную ошибку на выходе линейного слоя свертки но ничего не знают о целевой функции потерь чтобы учесть целевую функцию надо так или иначе вычислить градиент или его прокси и агрегировать и на помощь приходит приближение для данного слоя линейного или свертки первый фактор идентичен гессиану квадратичной ошибки для данного слоя второй фактор по выходу слоя кронекеровское разложение точно в предположении градиентов по выходу слоя от активаций которое вообще говоря не выполняется но попытка не пытка и в данной работе получилось успешно применить кронекеровское разложение в фреймворке суть метода в следующем берут где вместо настоящего гессиана используется кронекеровское разложение и выводят формулы для неструктурированного 24 и структурированного прунинга для более высоких уровней прореживания метод применяют итеративно между итерациями прунинга дообучают добавки и пересчитывают матрицу фишера до 40 раз при прореживании до 50 недешевое удовольствие но качество подымает почти гарантированно
141,2024-01-07,метод валидируют на моделях семейства и 13b в приложении для замера качества традиционно берут перплексию на lm_eval_harness 0shot в приложении уверенно опережает sparsegpt и диагональное кронекеровское приближение как структурированном так и неструктурированном пруниге конкурентные подходы ломают модель уже при слабом прореживании в то время как llmsurgeon с серьезной просадкой но тем не менее сохраняет какоето качество вплоть до 50 ожидаемо и дает заметный прирост в качестве так как метод оценивает важность параметров на уровне конечного слоя прореживать слои можно неоднородно однако полученное распределение слоев после фильтрации по глобальному порогу сравнительно однородное для llm surgeon первый и последний слой прореживаются несколько сильнее чем слои посередине немного неожиданно разные виды проекций q k v o proj и в fc12 тоже прореживаются достаточно однородно в отличие от global magnitude и диагональных приближений весьма достойный результат хоть и недешевой ценой выглядит на текущий момент как метод для языковых моделей размером порядка нескольких миллиардов параметров где блочнодиагонально фишеровское приближение становится непомерно дорогим а предложенный метод требующий дополнительной памяти порядка 2 размеров модели является еще подьемным пусть даже с шардингом но в пределах одной вычислительной ноды метод чем sparsegpt который отрабатывает за на llama2 на одной gpu против на 4x h100 для llm surgeon учитывая запрос сообщества на ускорение llm цена вполне приемлемая было бы полезно скомбинировать метод с квантизацией
142,2024-01-09,вот думаю разобрать ли у себя в блоге и статьи уже возникали в блогах уважаемых тг каналов но как мне кажется ряд интересных мыслей и нетривиальных наблюденийрешений все же были не затронуты в обзорах о которых было бы интересно поведать
143,2024-01-10,поиск архитектуры способной сбросить трансформеры с пьедестала по своей важности можно сравнить с поиском философского камня или высокотемпературного сверхпроводника время от времени появляются работы в которых предъявляют архитектуру конкурентную или превосходящую по эффективностимасштабируемости трансформер но по какойто причине научное сообщество и пользователи до сих пор сидят на трансформерах и все самые высокопроизводительные foundation модели так или иначе зиждятся на selfattention одна из наиболее интересных попыток предъявить альтернативу была серия работ по statespace моделям s4 однако показав хорошее качество на ряде задач связанных с обработкой последовательностей аудио они не стали сильны в общей задаче языкового моделирования в этой работе проанализировали недостатки и ограничения существующих statespace моделей и предложили модификацию устраняющую или минимизирующую эти недостатки
144,2024-01-10,модель по существу это рекуррентная модель вида в дискретной форме где вход скрытое состояние y выход обучаемые матрицы параметров параметризованные специальным образом есть еще шаг дискретизации и преобразование из непрерывной формулировки в дискретную с некоторым шагом дискретизации delta в отличие от типичной рекуррентной архитектуры аля vanilla rnn lstm между h и y нет нелинейности благодаря этому формулу можно развернуть и представить в виде свертки которая эффективно параллезируется на gpu важно что проекции a b c и динамика состояний сети обладает свойством данное свойство накладывает ограничения на возможность моделирование определённых зависимостей и авторы рассматривают пару синтетических задач которые принципиально не решаются statespace моделями с lti 1 где нужно запомнить и вывести несколько элементов последовательности с произвольным расстоянием между запоминаемыми элементами 2 где приняв начало словосочетания на вход на основе полученной ранее информации продолжить если на входе harry выдать следует potter прочитай другую книгу данные задачи требуют от модели способности работать с неоднородными временными последовательностями и адаптивно учитывать контекст которой нет в statespace моделях со статичными сверточными ядрами авторы вводят два понятия с похожим звучанием но разным значением компактное внутреннее состояние и дешевизна вычислений способность хранить релевантную информацию трансформер так как может хранить в себе во всяком случае в теории контекст произвольной длины но не ибо состояние кэш растет пропорционально длине последовательности что делает инференс трансформера дорогим по памяти и вычислениям причем большинство хранимой информации избыточно рекуррентные сети ибо хранят состояние постоянного размера не зависящего от длины последовательности но в это маленькое состояние не всегда удается на практике впихнуть идеальная модель должна потому отбирать нужный для задачи контекст чтобы добиться данного свойства авторы предлагают делать параметры statespace модели зависящими от входа проекции как и шаг последнее позволяет заодно учитывать и временную неоднородность развернуть рекуррентную зависимость как для lti statespace модели теперь не получится вместо этого авторы реализуют эффективный алгоритм на кастомных чудоkernel дающий высокую производительность чтобы иметь возможность запихнуть много информации в скрытое состояние используют большой expansion фактор внутри ssm 10100 современные gpu зачастую потому чтобы не тратить драгоценные миллисекунды на перекачку данных из hbm в кэш gpu это расширенное состояние материализуется только на gpu и в духе один из авторов автор чтобы дополнительно сэкономить на памяти и ее трансфере промежуточные состояния отправляются в после прямого прохода и пересчитываются снова на обратном проходе далее авторы предлагают полностью однородную архитектуру состоящую только из блоков в то время как трансформер перемежающиеся attention блоки и и прошлые statespace модели состояли собственно из statespace слоев и gated mlp так как алгоритм впридачу к свойствам новый тип statespace слоев обладает и использует для краткости авторы обозначают новый тип архитектуры
145,2024-01-10,авторы валидируют новую архитектуру на следующих задачах 1 selective copy и induction heads 2 language model pretraining 3 моделирование днк 4 моделирование и генерация аудио на s4 h3 и hyena выдают качество порядка в то время как s6 достигает почти качества на как трансформеры с разными позиционными энкодингами так и прошлые statespace модели не обобщаются далеко за пределы контекста увиденного на обучении а s6 mamba выдает стабильно хорошее качество на последовательностях любой длины достойно себя показывает на языковом моделировании при обучении на демонстрируя масштабируемость на уровне и даже чуть лучше чем трансформер архитектуры llama2 со всеми последними достижениями и наработками в развитии архитектур данного семейства h3 retnet hyena rwkv заметно отстают обучают модели следуя предписаниям размером от 130m до 28b заметно опережает по качеству открытые модели размера а иногда и на mamba превосходит уже заметно трансформер и statespace модели прошлых поколений модели кстати совсем небольшие 14m и 7m параметров на mamba сильно опережает по метрикам прошлую diffwave и иные подходы s4 mlp стоит отметить работает тоже весьма достойно на больших последовательностях scan из mamba работает куда быстрее чем даже оптимизированный flashattention2 авторская реализация scan в десятки раз быстрее той что в торче благодаря эффективности по памяти в одну gpu можно пихать гораздо больший батч чем в трансформер например 69b мамбе можно скормить 128 последовательностей длины 2048 и не поперхнуться в показывают что полезно иметь все параметры statespace модели a b c delta обучаемыми но больше всего накидывает delta комплексная параметризация a не накидывает по сравнению с нормальным распределением expansion внутреннего состояния улучшает качество чем больше фактор тем больше информации можно запихнуть в скрытое состояние по ходу работы пользуются n64 выглядит весьма впечатляюще крутая работа с хорошим анализом и мотивацией серьезными инженерными достижениями и убедительными результатами однако все же интересно сможет ли mamba родить или останется одной из многих пусть и сильных попыток ограниченной успешности самая большая модель которую использовали в бенчмарках по качеству имеет размер менее параметров что немного по современным меркам как признаются сами авторы масштабирование архитектуры дальше может породить новые технические сложности и вызовы поживемувидим как говорится
146,2024-01-13,за последние пару лет llmки забавно прибавили в весе и многие модели не влезают в типичную пользовательскую gpu а инферить умного ассистента локально ой как хочется как выход можно держать веса в озу или на диске и подгружать по мере необходимости то есть заниматься offloading есть offloading когда все вычисления происходят на gpu и используемый в где часть вычислений на gpu а часть на cpu основная проблема в том что передача данных с gpu на cpu занимает уйму времени и большая часть времени уходит на операции с памятью пока ядра простаивают и работает все дико медленно отсюда следует вывод что ключом к ускорению является минимизация передачи данных между cpu и gpu в ранее было замечено что в слоях трансформера активируется малая часть активаций для конкретного входа здесь же развили идею и заметили что нейроны активируются неравномерно и некоторые активируются часто так называемые а другие же только время от времени в частности для 26 для 43 наиболее часто активируемых нейронов отвечают за суммарной величины активаций статистики активаций считаются в режиме на подвыборке из c4 и wikipedia отсюда возникает идея держать горячие коих не очень много нейроны на gpu постоянно а холодные предсказывать с помощью специальных слоев и подгружать по мере необходимости оказывается даже что практичнее не выгружать холодные нейроны на gpu для малых батчей а считать на cpu следующий нюанс в том что делать со слоями предсказывающими активные нейроны среди держать их памяти накладно для больших моделей а наивное использование предикторов с меньшим числом параметров просаживает качество и оказывается что в слоях где больше sparsity можно использовать более слабый предиктор и потому размер предиктора подбирается для каждого слоя отдельно на gpu мы работаем с матрицами фиксированного размера и потому пользуемся эффективными ядрами для а для cpu обладающим куда меньше параллелизмом отдельные произведения считаются и так эффективно
147,2024-01-13,тестируют на моделях семества optllama 2 и falcon код основан на llamacpp рассматривают 2 конфигурации pchigh с intel и pclow и c нормальные такие геймерские компы достигает впечатляющего ускорения в раз на длинных последовательностях и моделях на ускорение не столько велико до 3х раз но тоже весьма достойно в powerinfer большинство вычислений происходит на gpu в то время как в llamacpp основную вычислительную нагрузку берет себя не столь быстрый cpu все компоненты метода полезны но наиболее важной по всей видимости является то что критичные для вычислений нейроны сидят все время на gpu метод дает ускорение и при квантизации в 4 бита оверхед от предикторов холодных нейронов по сравнению с общим методом инференса качество моделей просаживается статзначимо при используемой схеме fixed contextual sparsity а если проверить на mmlu метод настолько хорош что даже на относительно слабой rtx 4090 на коротких последовательностях метод уступает всего лишь на 2030 в скорости оптимизрованному gpu движку на a100 полезный и сильный результат основанный на наблюдениях из прошлых работ по существу сочетание fixed и contextual sparsity с эффективной реализацией учитывающей специфику вычислений на cpu и gpu
148,2024-01-15,moe использующие лишь часть параметров на инференсе дают значительную экономию вычислений однако sotaмодели оказываются слишком тяжеловесными для многих gpu и встает вопрос эффективного оффлоадинга наивная подгрузка экспертов работает довольно медленно изза передачи большого обьема данных с озуram на gpu возникает вопрос можно ли оптимизировать данную процедуру и ответ как можно было заранее догадаться да чтобы понять как оптимизировать загрузку экспертов авторы смотрят на то как активируются эксперты и оказывается что в конкретном слое один и тот же эксперт может активироваться 24 раза подряд потому если их оставлять в памяти они будут доступны при прогонке следующего токена отсюда возникает идея поддерживать кэш токенов и сгружать экспертов на диск по необходимости следующий нюанс в том как загружать блоки для обычных трансформеров никакой проблемы нет но для экспертов заранее неизвестно какие активируются но можно воспользоваться стратегией из где предсказывали активные нейроны по активациям предыдущего блока и отбирать экспертов подавая в gating функцию на следующем блоке текущие активации это работает в силу residual структуры трансформера чтобы уменьшить размер модели используется квантизация не самый очевидный выбор ибо это datafree квантизация уступающая более продвинутым подходам но раз работает то и ладно говорят не завелось может надо было квантовать в больше чем 1 бит квантуют в 4 бита экспертов в 2 и 3 бита рассматриваются две конфигурации системы условный colab с бесплатной gpu и хороший геймерский пк для gpu c 12gb vram можно держать кэш из 2 экспертов а для 16gb влезают 4 чем больше размер кэша тем больше вероятность того что эксперт лежит в кэше и точность удачного выбранная схема квантования более менее оптимальна так как квантизация attention в меньшее число бит уже сильно просаживает качество а 4 бита некритично хуже fp16 конечные конфигурации в 45 раз меньше по размеру базовой модели некоторая просадка по качеству на бенчмарках есть но приемлемая все компоненты метода экспертов заметно ускоряют инференс итоговое ускорение порядка раз на rtx3060rtx3080 и до раз на t4 по сравнению с наивной реализацией в accelerate для прикола замеряют еще и на a100 куда влезает квантованная модель и так для сравнения простой и разумный подход с практически полезным результатом по всей видимости умный оффлоадинг одно из самых если не самое перспективное направление ускорения моделях на пользовательских устройствах однако букву о гармонично вставить в название канала не получается кажется что результаты работы можно улучшить за счет более сильного алгоритма квантизации экспертов уменьшив просадку качества
149,2024-01-16,низкоранговые адаптеры уже давно используются при дообучении больших языковых моделей на downstream задачи эффективность данного метода основана на наблюдении что при дообучении на малые задачи разница между исходными и дообученными весами обладает низкоранговой структурой то есть основная масса концентрируется в нескольких первых сингулярных векторах однако данное утверждение приближенное и поэтому lora по всей природе неспособная учить высокоранговые добавки не всегда сравнивается по качеству с дообучением всей модели другая опция малопараметрической добавки к матрице весов разреженные матрицы а совместив низкоранговую с разреженной добавкой получаем своего рода метод принциапиальных компоненты с шумом который используется много где рекомендую со стивом брантоном основная техническая сложность использования разреженных матриц на gpu благо при сильном прореживании неплохо работают ядра из специальных библиотек в частности sputnik как можно догадаться суть работы в том что совместили низкоранговый и lowrank adapter и дообучили на downstream задачах разреженную маску получают на основе самых больших элементов авторы отмечают что отдельно диагональ для адаптера работает не очень но хорошо в связке с низкоранговым адаптером авторы анализируют точность приближения суммой низкорангового и sparse приближения на одном слое и оказывается что минимум ошибки достигается при сопоставимом вкладе от lowrank и sparse компоненты чем больше доля параметров от исходной тем эффективнее перекладывать в sparse который уже не такой sparse метод валидируют на и датасетах математика без подвоха при фиксированном бюджете некоторая смесь обычно с примерно равной долей sparse и lowrank компоненты дает лучшее качество по отдельности lowrank и sparse не так хороши больший адаптер работает обычно лучше и самой большой с суммарным числом обучаемых параметров 160m почти сравнивается по качеству с полным файнтьюном все эксперименты влезают на одну 24gb памяти выбор разреженной маски на основе усредненного градиента и диагонали фишера дает примерно качество в статье не говорится но знаю лично от авторов что случайная маска и по абсолютной величине весов работает хуже поиск эффективных и выразительных адаптеров архиважная задача ибо пользователи без highend gpu только так и могут обучать современные llmки на целевую подзадачу lora и ортогональные адаптеры показывают себя неплохо в конкретных приложениях но ограничены в выразительности по природе своей и оптимальный адаптер по всей видимости представляет собой некоторую смесь низкопараметрических матриц ее правда надо еще сначала найти
150,2024-01-18,статьи нет обычно наоборот marlin ixed uto egressive ear kernel новое быстрое ядро fp16xint4 для инференса llm с большими батчами как известно вычисления на современных gpu для современных gpu обычно memory bound и можно получить заметное ускорение даже с fp умножением за счет одной лишь подгрузки весов в кэши и идеальное ускорение которое дает int4 4x однако с увеличением размера батча мы начинаем выходить на computebound и выигрыш от имеющихся ядер не использующих tensor cores исчезает то есть они становятся медленее fp16 native матричных операций в этом репозитории выложена реализация нового ядра оптимизирующего работу с кэшами и позволяющего иметь значительное ускорение по сравнению с fp16 даже на больших батчах основные фичи следующие 1 префетчинг 2 использование активаций в вычислениях несколько раз до загрузки сгрузки 3 асинхронная загрузка весов модели 4 тонкая настройка порядка операций квантования деквантования и матричных умножений с tensor cores для максимальной утилизации железа 5 оптимизация конфигурации варпов групп потоков на gpu в итоге удается достичь почти идеального speedup 4x по сравнению с fp16 на батчах до 32 и иметь ускорение даже на больших 64128 в то время как безйлайны из становятся в 2x медленее fp16 с батчей 16 ускорение имеет место на нескольких gpu поколения ampere a10 a6000 rtx 3090 и сохраняется даже на пониженных частотах на поколении hopper пока нет реализации
151,2024-01-18,видала я статьи без кода но код без статьи такого я в жизни ещё не встречала
152,2024-01-23,кода нет как и моделей и датасета обучить здоровенную модель на здоровенном датасете здоровенное число итераций которая умеет во все и всея большого ума не надо поставил обучаться на десятках тысяч видеокарт на несколько месяцев и готово а вот получить небольшую модельку способную в сложные логические конструкции и заключения решение задач по математике вот это уже настоящее мастерство и искусство в разбираемой статье авторы обучили семейство сетей небольшого размера от до которые превосходят по доле правильных решений куда более крупных конкурентов в великом множестве статей ранее было показано что синтетические данные сгененированные могучей сетью аля gpt354 позволяют добиться значительно более высокой эффективности обучения по сравнению с типичным корпусами собранными из интернета в частности можно дотюнить и многое и многое другое или обучить с нуля серия моделей и математические датасеты невелики по размеру в частности имеет всего примеров в обучающей выборке и наивное обучение приведет к переобучению потому авторы аугментируют данные с помощью перефразируя вопросы и добавляя нерелевантный контекст чтобы обеспечить качество данных убирают слишком короткие и не содержащие числа задачи кроме того выфильтровали задачи которые оказались похожи на тестовые по nграмному сравнению итого вышло более 1 синтетических задач токенов однако одно лишь это не позволяет преодолеть порог в top1 accuracy на тесте следующим краеугольным камнем работы является использование проверятеля которая оценивает корректность каждого шага решения мотивация заключается в том что одно решение может быть неправильным но если сгенерировать несколько то хотя бы одно да залетит и модельverifier учится определять корректные шаги в решении обучают ее следующим образом берут 3 чекпоинта модели решающей с разных итераций обучения более ранние больше ошибаются и учат предсказывать корректность конкретного токена в решении если модельрешатель решила задачу верно то все токены в последовательности размечаются как верные label 1 иначе наоборот вся последовательность как неправильная сэмплируют решений на из 7к примеров в обучающей выборке таким образом рецепт успеха состоит из основных идей 1 аугментация датасета 2 обучение модели проверятеля оценивающей правильность конкретных шагов совместно с моделью решающей задачи
153,2024-01-23,модельрешатель и модельпроверяльщика инициализируют из преобученных одно лишь увеличение размера моделирешателя дает небольшой прирост качества потому добавления проверяльщика критично для достижения хорошего результата любопытно что увеличение дает качества чем модели решателя сэмплируются 48 решений из которых подается лучшее в качестве ответа самая лучшая модель достигает качества в что больше чем у gpt35 породившей в определенном смысле данную модель и открытых моделей дообученных на решение математических задач поверх llama 2 7b и 13b gpt4 для справки добивается точности метод проверяют и на другом известном бенчмарке и там тоже получают сильный результат ожидаемо инициализация работает хуже чем обучение с чекпоинта phi15 хоть я бы ожидал большей разницы прикольное и эффектное решение тем не менее уместно заметить что стоимость инференса при использовании моделипроверятеля возрастает кратно числу сгенерированных модельюрешателем ответов и выигрыш от использования меньшей модели надо умножать на число прогонов после чего сравнить с качеством при одноразовой прогонке через большую модель
154,2024-01-23,и из забавного на ночь от коллег вывод одной модели на задаче из gsm8k
155,2024-01-26,спасибо как известно дообучение на грамотным образом собранных инструкциях позволяет заметно повысить полезность llmок для создания широкопрофильного ассистента требуется обучение на широком и разнообразном наборе инструкций к сожалению не все модели обладают достаточной емкостью чтобы впитывать все подаваемое знание через трубочку и на практике происходит catastrophic forgetting при переходе от одного типа инструкций к другому естественным решением проблемы являются смеси экспертов moe каждый эксперт условно соответствует некоторому домену в работе наделали реплик ffn слоев и дообучили но такой подход довольно дорогостоящий по памяти для больших моделей суть метода проста как пробка раз полноценные ffn слои дорогие даешь смесь bottleneck экспертов аля lora c активацией посередине и чтобы необучаемые параметры было еще проще уместить на gpu заквантовать необучаемые параметры в 4 бита как в обучают все хозяйство на смеси 520к инструкций в сумме обучение длится одну эпоху в качестве основных моделей берут llama27b llama213b yi34b потому что llama233b не выпустили если я правильно понял модель дообученная на наборе инструкций c qlora называется camel а эксперты со смесью lora адаптеров как и в mixtral создают 8 экспертов из которых активируются только 2 модель валидируют на разнообразных бенмарчках знания в разных предметных областях hellaswag commonsense reasoning код gsm8k элементарная математика и др ожидаемо опережает базовую модель по метрикам и самая большая модель выглядит якобы даже сильнее большего с плотными экспертами и однако если присмотреться кажется что разница между camel и camelidae не оченьто велика и вероятно основной прирост от удачного выбора смеси инструкций вероятно в camelidae больше обучаемых параметров смеси экспертов в исходной формулировке или как адаптеры выглядят логичным решением для модели ориентированной на решение широкого круга задач однако в данном случае не очевидно насколько эффективен а не эксперимента благоприятствуют хорошим результатам
156,2024-02-02,многие из вас читают статьи про диффузионные модели в них регулярно возникает таинственная и загадочная внушающая благоговейный трепет сущность под названием stochastic differential equations и эти самые описывают диффузионные процессы в непрерывной формулировке потому разные продвинутые солверы и методы сэплирования в том или ином виде зиждятся на теории стохастических дифференциальных уравнений однако классическая литература по данной теме довольно сложна и оперирует множеством сложных понятий из теории меры разными борелевскими сигмаалгебрами и тяжела для освоения читателю без сильного математического бэкграунда двое авторов из aalto university откуда вышло немало ярких работ по генеративным моделям выпустили по sde доступную широкому кругу читателей достаточно иметь за плечами стандартный вузовский курс по диффурам и теорверу с упором на приложения написано очень доступно и понятно и упражнения в конце глав вполне себе посильные а не как обычно бывает рекомендую всем тем кто получить некоторую математическую базу для понимания диффузии
157,2024-02-03,кода нет было еще в последнее время был достигнут значительный прогресс в ускорении диффузионных моделей тем не менее инференс на мобильных устройствах работает все еще скорость генерации диффузионной модели определяется сэмплирования и стоимостью на текущий момент большинство внимания уделяется ускорению моделей за счет первого фактора изобретением новых солверов алгоритмов пошаговой дистилляции оптимизации архитектуры уделяется же значительно типичные модели хороши для gpu но тяжеловаты для мобилок потому имеет смысл сохраняя качество в этой работе основное внимание уделено как раз архитектуре но и про уменьшение количества шагов сэмплирования тоже не забыли за основу берут stablediffusion v15 первая оптимизация мотивированная прошлыми работами и уборка дорогих и не очень полезных attention с самого высокого разрешения кроме того на разрешении оставляют только crossattention с текстовыми токенами потому что selfattention с 1024 все еще дорогой таким образом selfattention есть только на разрешении кроме того объединяют проекции key и value чтобы еще чутьчуть сэкономить на параметрах заменяют на который более эффективен на мобилках и приятнее с точки зрения последующего квантования модели и на в последнем случае обученную с softmax модель файнтьюнят 10к шагов с relu и якобы не теряют в качестве и уменьшают expansion в слоях с 8 до 6 без просадки в качестве вместо обычных сверток 3x3 используют с меньшим числом параметров и операций утверждается что качество не просаживается кроме того уменьшили общее количество блоков с в sd до целевая модель должна иметь не более и на инференсе авторы запустили перебор разных конфигураций блоков и ширин каналов детали процесса опущены и отобрали самых удачных кандидатов для пошаговой дистилляции были рассмотрены две стратегии 1 в 8 шагов сэмплирования 2 с адверсариальным лоссом кроме того вместо исходного автоэнкодера из sd авторы обучили свой более компактный но кодирующий в каналов вместо х потом его еще запрунили непонятно как полученный vae и быстрее и лучше по метрикам
158,2024-02-03,модель обучали на 150m изображениях из шагов на разрешении 256x256 и шагов на 512x512 качество замеряют по fid и clip на 30k изображениях из mscoco следуя стандартной практике кроме базовой модели c 400m параметров обучают еще более компактную версию c 300m параметрами на приложенных картинках все модели генерируют примерно одинаково хорошо на одном уровне с sdxl что с 50 шагами сэмплера что с прогрессивной дистилляцией в 8 шагов что с в одношаговое сэмплирование по метрикам md сэмплирующая в 50 шагов на уровне sdv15 8 шагов имеет немного худший fid и одношаговая уже просаживается заметно по скорости выходит вполне себе здорово при сэмплировании в 8 шагов md почти в быстрее разобранной ранее snapfusion которую тоже дистиллировали в 8 шагов генерации а в режиме одношаговой генерации удается достичь скорости в на изображение при замерах на iphone 15 достойный технический результат с использованием разных техник и приемов из прошлой литературы однако для полноты неплохо было бы иметь sidebyside evaluation c sd и замеры разнообразия генераций
159,2024-02-10,альпаки бывают не только llmками но ещё и домашними питомцами
160,2024-02-10,толькотолько мы с коллегами выкатили как конкуренты нанесли ответный удар llmки становятся все круче новые модели датасеты с инструкциями выходят почти на ежедневной основе однако самые сильные из опенсорсных моделей llama270b с дохреналлионом файтьюнов микстраль и загадочное нечто под названием не влезают в колаб или условную rtx 30904090 квантизация в 4 бита до недавних пор бывшая недостаточна чтобы позволить уместиться такой здоровенной модели на щупленькую видеокарту а предыдущие методы квантизации в 23 бита ломают заметно модель и проще взять меньшую модель в большей точности но свежие работы открывают дорогу к инференсу оверсайзд моделей на хорошей геймерской gpu quip решетка потому что e8 решетка квантования стоит на трех 1 2 решеточная векторная квантизация 3 неквантованных параметров 1 как известно веса больших языковых моделей обладают выбросами outliers плохо поддающимися квантованию и в ряде прошлых работ было предложено их тем или иным образом изолировать здесь же следуя своей прошлой работе авторы домножают веса на случайное преобразование благодаря которому величины и чувствительности параметров растекаются равномерно по матрице отличие от прошлой работы в том что вместо произведения кронекеровских матриц для параметризации ортогонального преобразования используется случайное более эффективное вычислительно и не уступающее по качеству 2 поэлементная квантизация не совсем оптимальна в том смысле что оптимизирует ошибку в пределах некоторого гиперкуба если рассматривать группы весов а в действительности группы весов распределены в некотором шаре а объем шара того же диаметра что и сторона куба в большой размерности много меньше описанного вокруг него куба и при том же количестве кластеров квантованных значений можно добиться значительно меньшей ошибки для используемой в работе квантизации группами по 8 весов оптимальная решетка как было сравнительно недавно 3 чтобы уменьшить ошибку квантизации можно потюнить модель воспроизводить выходы слоев исходной модели как и в aqlm авторы сначала дообучают неквантованные модели на уровне трансформера а на обучают уже все неквантованные параметры на минизацию разницы между сжатой моделью и исходной fp16 для больших битностей 34 бита используется когда ошибка квантизации еще раз квантуется и конечный вес представляется в виде суммы квантованного веса и квантованной ошибки
161,2024-02-10,одним словом вышло здорово валидация стандартная бенчмарки по перплексии и подборка zeroshot из на битностях 234 опережают известные методы по качеству почти везде отрыв от aqlm небольшой но все же есть quip достигает при квантовании в отдельные конфигурации aqlm паретооптимальны в 2526 битах имеет влияние на качество модели но неквантованных параметров накидывает кроме того все это хозяйство можно вполне эффективно инферить с приемлемой производильностью при этом авторы отвечают что среди них нет мастеров по написанию cuda кернелов и можно еще ускорить генерацию сильный результат и красивая математика гонка по сжатию llm становится все захватывающее интересно какова же всетаки битность парето кривой при использовании самого лучшего из возможных методов квантования моделей и как скоро мы посадим почти lossless 70b модель на сolab
162,2024-02-12,возникла следующая мысль навеянная работой то minicpm отличный разбор у типичные scaling laws для обучения языковых моделей имеют следующий вид где лоссфункция на отложенной выборке ошибка идеальной модели число параметров в модели число токенов на обучении некоторые константы но в них никак не фигурируют детали обучения расписание вероятно они не явно зашиты в константы но зависимость нетривиальная при нулевом learning rate ничего учиться не будет очевидно при learning rate выше некоторого порога лосс улетит в стратосферу потому данный закон справедлив в некоторой области параметров при большем модель больше подстраивается под значение батча но в то же время больше информации чем было в train она постичь не может полюбому и для точной сходимости к оптимуму по всей видимости необходимо иметь малый learning rate в конце обучения чего можно добиться условным косинусом линейным или угасающим интересно насколько кроме самой архитектуры и данных качество полученной модели зависит от параметров обучения и насколько оптимально подобраны рецепты обучения всех этих шиншилл ллам мистралей и прочей живности
163,2024-02-16,полагаю все уже успели полюбоваться новым детищем от openai под названием sora классные разборы уже появились у светочей и рупоров отечетсвенного ai п как и веселые со своей стороны лишь накину пару мыслей и соображений после прочтения отчетов и залипания в видосики в отчете ожидаемо given the competitive landscape and the safety implications блаблабла не прозвучали какиелибо детали о сборе и фильтрации датасета архитектуре пайплайне обучения на основе отчета можно сделать 1 архитектура модели некий здоровенный работающий на патчах 2 модель есть энкодер превращающий видео с большим пространственным и временным разрешением в некое компактное латентное представление в котором и происходит диффузионный процесс и декодер превращающий латенты обратно в видео 3 модель можно обуславливать на текст текст изображение и использовать для продолжения коротких клипов кроме того можно редактировать видео с помощью промпта и интерполировать два ролика осмысленным образом 4 в процессе обучения модели по всей видимости на вход подавались и изображения и картинки картинка эквивалентна видео из одного кадра 5 модель обучалась на исходных необрезанных картинках благодаря этому она не генерирует обрезанный контект и способна работать в любом разрешении интересно как они батчевали кадры разного разрешения возможно encoderdecoder способен приводить все к одному разрешению обуславливаясь на разрешение входа а может модель настолько большая что батч больше одного все равно не лезет в сие чудище 6 модель обладает пространственновременной консистентностью закрытие одного обьекта другим occlusion не приводит к характерным артефактам 7 генерировать можно ролики и изображения с разрешением до 2k 8 как и в dalle3 большую роль играют синтетические описания сгененированные специально обученной для этого моделью 1 по всей видимости модель под капотом и обьем данныхвычислений потраченных на обучения модели реально колосалльны скорее всего сама модель значительно больше чем sdxl emu и иные модели о которых хоть чтото известно 2 полагаю что в обучении было задействовано много синтетики зарендеренной unreal engine 5 или чемто подобным многие ролики напоминают генерации движка 3d графики таким образом можно задавать пространственновременную информацию куда более явно чем weak supervision с огромного числа видеороликов и клипов
164,2024-02-21,добавили в transformers 4380 пользуйтесь наслаждайтесь делитесь впечатлениями кроме того мы потюнили end2end все обучаемые параметры на логиты fp16 модели и сравнялись по качеству на llama27b моделях с обновленные 7b модели уже лежат в хабе остальные будем добавлять по мере готовности wikitext2 ppl было 631 стало 592 wikitext2 ppl было 791 стало 669
165,2024-02-22,анонсировали пока без конкретных деталей но обещают в будущем папиру что известно на данный момент 1 будет несколько моделей от 800m до 8b как минимум 2 размера 2 по всей видимости в латентном пространстве работает некая вариация как в sora 3 еще чтото делает модель доступна только через waitlist для early preview типа из соображений безопасности чтобы никто не начал использовать ее во вред человечеству и для удовлетворения небогоугодных фантазий анонс на
166,2024-02-24,пока все ждут технического отсчета по stable diffusion 3 и стебутся над поделками от google рассмотрим свежую работу про дистилляцию диффузионок некоторое время назад выкатили дистиллированную с адверсариальными лоссами версию ребята из предложили альтернативный хоть и основанный на схожих принципах метод дистилляции в один и малое число шагов предложенный подход основывается на чуть ли ни самой первой технике уменьшения шагов сэмплирования однако в исходной работе качество сильно проседало при уменьшении числа шагов до однозначного авторы утверждают что диффузионные модели при многошаговой генерации обладают не путать с автором десятитомника по теоретической физике и могут аппроксимировать сложные траектории при малошаговой не могут и выдают лучшее что позволяет их емкость при оптимизации mse лосса мыльные генерации предлагается вместо mse лосса использовать чтобы дискриминатор не мог отличить результат расшумления учителем от ученика в sdxlturbo дискриминатор работает в пиксельном пространстве что требует декодирования картинки обработки ее vitом в довольно высоком разрешении и вся процедура выходит довольно дорогой здесь же авторы в качестве тушки дискриминатора используют sdxl и классификатору подают агрегированные признаки со всех уровней при обучении нет нужды декодировать картинку и можно обучать оценивать реалистичность промежуточных шагов в качестве функции потерь берут ванильный однако оказывается что полученная сеть подвержена генерации артефактов в стиле существа и объекты раздваиваются утверждается я не понял почему что это тоже проявление недостаточной емкости модели авторы дообучают модель еще некоторое время на adversarial лосс но без примеров родительской модели утверждается что это лечит дистилляция проходит в несколько стадий первая стадия довольно простая и даже обычный mse лосс работает а дальше используют предложенную технику применяется только на первом этапе дистилляции
167,2024-02-24,обучают на отфильтрованном классификатором эстетичности подмножестве из и при обучении вместе с генерациями учителя используют lora которую затем вшивают в веса модели и уже всю модель учат дурить классификатор интересно вывод о недостаточной емкости модели был сделан до или после решения обучать с кроме того обучают на расписаниях с разным количеством шагов обучения говорят стабилизирует результат визуально генерации в один и малое число шагов выглядят неплохо генерации в один шаг часто меняют существенно картинку не нарушая семантики для численной оценки качества используют fid на всех картинках fid на патчах и clip score для замера fid к целому вагону недостатков на целых изображениях надо приводить картинки к нестандартному разрешению 299x299 и это вносит сдвиг в результат замеров потому предлагается смотреть и на кропы по fid и clip sdxllightning сопоставим с конкурентными sdxlturbo и но несколько лучше по fid на патчах если не можешь явно побить бейзлайны по метрикам придумай правильную метрику все дистилляты несколько проседают по fid в первую очередь изза разнообразия по всей видимости логичный и интересный подход но не хватает sbs с асессорами кроме того неизвестно насколько sdxllightning подвержен проблеме снижения разнообразия генераций коей страдает sdxlturbo
168,2024-02-25,этическая дилемма это когда рецензируемая статья не очень но ее автор активно цитирует тебя
169,2024-02-27,с моего доклада про на сегодняшней встрече в точке кипения спасибо всем участникам за вопросы и интересные дискуссии и спасибо за приглашение и увлекательный рассказ про коннектом
170,2024-02-28,либо результат этой статьи невероятно крут и ваш покорный слуга может завязывать с сжатием и идти продавать пирожки с капустой либо бы имеем дело с обманом почище ultrafastbert утверждается ни много не мало следующее можно обучать llm с битами на параметр которые сходятся так же хорошо как и fp16 модели в линейных слоях все веса квантуются тернарно принимают значения скейл будто бы берется один на весь тензор даже не канал что вдвойне удивительно учитывая склонность llm к оутлаерам и неравномерность распреления весов активации же приводятся к 8 битам и чтобы не хранить zeropoint значения активаций симметризуются ничего не сказано про процедуру обучения был ли использован или чтото еще более хитрое безградиентное для обучения тернарных весов ибо наивное обучение всего этого хозяйства должно сходиться из ряда вон плохо за основу берут архитектуру и обучают модели разного размера от 700m до 39b параметров на redpajama в сравнениях с бейзлайновой llama fp16 все модели поглощают 100b токенов на этапе обучения модели сравнивают по перплексии на непонятно какая из них приведена в таблице 1 или средняя и zeroshot на lmevalharness на меньших моделях bitnet158 так называется семейство квантованных моделей лишь слегка уступает бейзлайну а на больших будто бы чуть лучше чем fp16 замена большого числа умножений на дает огромный потенциал для ускорения при том же размере модель в разы быстрее экономичнее по памяти и жрет куда меньше драгоценной энергии при обучении на 2t токенах bitnet158 на бенчмарках лучше обученной на том же числе данных практически полное отсутствие описания метода и протокола обучения делает результаты данной работы крайне сомнительными краткость сестра таланта только если ты openai в общем ждем дальнейшей информации может таки выложат модели и нечто большее чем readme
171,2024-03-05,кода нет но обещают таки выкатили обещанный отчет про обучения сама по себе процедура обучения представляет комбинацию большого числа техник из прошлой литературы авторы перебирают различные архитектурные конфигурации диффузионные процессы расписания зашумления и иные трюки чтобы достичь наилучшего качества авторы рассматривают следующие постановки диффузионных процессов 1 который как понятно из названия используется в конечной модели по существу между изображением и шумом 2 с variance exploding 3 косинусное расписание из модели предсказывают либо шум либо скорость кроме того рассматриваются различные в наивном подходе шаги сэмплируются равномерно по времени диффузии однако предсказание скорости в середине траектории вдали от распределения данных и распредения шума6 потому целесообразно сэмплировать чаще в середине желаемого эффекта можно достичь с помощью sampling log t 1 t и авторы ablaтят различные выборы смотря на метрики генерации clipscore fid в classconditional генерации на imagenet и cc12m оказалось что rf с логитнормальным распределением зашумления работает лучше всего
172,2024-03-05,показали что увеличение размерности латентных кодов повышает качество следуя данному наблюдению авторы попробовали автоэнкодер с 4 как и в прошлых sd 8 16 каналами увеличение ширины стабильно накидывает по метрикам самое примечательное архитектура в типичной диффузионной unet архитектуре условие подается в виде и через с патчами изображениялатента здесь же токены изображения и текстовые конкатенируются и обрабатываются совместно мультимодальным трансформером в данной работе авторы используют 3 текстовых энкодера разной размерности clipl14 clipg14 и t5xxl эмбеддинги первых двух энкодеров конкатятся и паддятся до размерности t5xxl которая больше чем у l14 и g14 вместе взятых то есть имеем 3 потока токенов текстовые токены от clipl14 clipg14 текстовые токены от t5xxl токены изображения причем feedforward слои нормализации и модуляционные слои имеют веса для каждого из трех перечисленных выше потоков токенов attention проекции общие для всех токенов иногда преattention выдает слишком большие значения и для повышения стабильности обучения авторы ключи и значения перед attention c помощью rmsnorm предложенная архитектура работает лучше чем unet с token merging и splitting crossattention и dit с общими mlp модуляциями для всех типов токенов иметь отдельные слои для разных текстовых энкодеров полезно хоть выигрыш сравнительно небольшой модели сравниваются на основе динамики валидационного лосса метрик на сс12m следуя прошлым работам авторы добавляют сгенерированные через в пропорции их добавление в обучение заметно улучшает согласованность текста и изображения по мнению разметчиков
173,2024-03-05,нащупав хороший сетап авторы запускают полномасштабное обучение самая большая модель имеет параметров с учетом t5xxl или без данные отфильтровываются по наличию nsfw контента эстетичности и дубликатам на начальной стадии обучаются на изображениях а затем переходят генерации на разрешениях до 1 с разными aspect ratio при дообучении на высоком разрешении оказывается важным изменить расписание шагов зашумления так как изображение более высокого разрешения имеют больше сигнала сдвинутое расписание улучшает качество согласно оценке аннотаторов после обучения на высоком разрешении модель дообучают с помощью dpo на улучшение эстетичности и пользовательских предпочтений примечательно что обучают не все параметры а lora адаптеры scaling модели стабильно улучшает качество разные модели отличаются и шириной и глубиной валидационный лосс score matching loss хорошо коррелирует с пользовательскими предпочтениями на geneval и t2icompbench stable diffusion 3 сравнивают на с прошлыми версиями stable diffusion pixartalpha и проприетарными моделями dalle3 midjouneyv5 ideogramv10 stable diffusion 3 заметно опережает прошлые sd pixart и слегка проприентарные модели основной выигрыш за счет typography разница по визуальной эстетике не столько велика большой текстовый энкодер t5xxl полезен при сложных промптах но особо не влияет на эстетическое качество сильная модель вобравшая в себя достижения современной науки и значительный инженерный труд с точки зрения науки никаких прорывных идей киллерфич не предложено ждем код и возможность поиграться с моделькой rectified flow постановка по идее должна благоприятствовать хорошим генерациям в малое число шагов
174,2024-03-07,gemma оказалась камнем с дефектами проблемы gemmы 1 нету bos токена я устал bos 2 очепятка в end_of_turn 3 sqrtd_embed sqrt3072554256 но bfloat16 555 4 layernormw1 не во float32 5 баг в bfloat16 rope 6 rope чувствительна к выбору между y 1x или yx 7 пофиксили rope должен быть в float16 rope может содержать числа выходящие за машинную точность потому обыкновенно применяется в float32 а затем выход кастуется к fp16bf16 при инференсеобучении в половинной точности а ято думаю чего это aqlm квантизация разваливает 7b модель
175,2024-03-07,квантуется кстати вполне сносно вчера выложили 2 модели 1 2 правда эмбеддинги и голова занимают большую часть памяти теперь планируем сжать их в будущих релизах чтобы уместить модельку в 1гб
176,2024-03-08,поздравляю дорогих подписчиц с профессиональным праздником желаю вам всех благ красоты счастья любви и здоровья вы настоящее украшение наших датасетов для генерации изображений и в топ классификаторов эстетичности
177,2024-03-10,забабахал архитектуру обучаю трансформер чет лосс почти не падает кручуверчу лернинг рейт один хрен смотрю на выход с zero инициализацией на последнем слое и он елееле колеблется в районе 0 чешу репу смотрю код и опа в трансформерном блоке было нечто такое для краткости опущены нормализации и rope добавил потерянный skip и все заработало мораль сей басни такова skip connections
178,2024-03-12,официального кода нет пойдем в обратном хронологическом порядке и откатимся на несколько месяцев и 058 битов назад по отношению к собственно здесь по существу и расписана вся экспериметнальная постановка на которой основывается статья с тернарной квантизацией метод линейные слои заменяются на где элементы принимают только два значения 1 и 1 если вес больше по матрице то он 1 иначе 1 и общий масштаб равен веса максимально дешево и сердито такая операция недифференцируема потому используется на этапе обучения авторы хранят полный тензор весов во активации квантуются в 8 бит с помощью равномерной сетки чтобы веса лучше квантовались перед линейным слоем стоит layernorm вычисление среднего значения для определения среднего значения по тензору требует синхронизации при и во избежание лишней коммуникации между процессами при распределенном обучении среднее считается по части матрицы лежащей на данном процессе странно что они не рассматирвают поканальное квантование или даже группами по 256 выразительность больше а размер модели все еще не сильно отличается от 1 бита далее авторы приводят табличку показывающую насколько энергоэффективнее int8 по сравнению с fp16 на разных техпроцессах на случай если грета будет ревьюером обучают на смеси pile commoncrawl ccstories realnews модели размером от 125m до 67b по перплексии полученные модели ожидаемо хуже бейзлайна в fp16 но зато если нормализовать на энергопотребление то bitnet смотрится весьма представлительно обучение bitnet с требует для адекватной сходимости 8e4 на котором fp16 расходится по 0шот бенчам якобы не слишком сильно уступают fp16 трансформеру однако вопрос в том насколько хорошо был затюнен сам бейзлайн и сами бенчи были выбраны похитрому bitnet 67b выдает 559 на бенчах но при этом качество случайной модели 438 и относительный отрыв от рандом выглядит не таким большим конкурентые ptq подходы стартующие с fp16 трансформера конкренто ломаются в низкой битности тогда еще не вышли quip и aqlm с одной стороны неплохо что это уже както работает но бенчи и бейзлайны были подобраны не слишком честно bitnet158 выдает уже значительно более высокое качество благодаря возможности принимать хотя бы 0ое значение
179,2024-03-13,забавный каламбур выйдет если relaxml выпустят quip версию свеженькой модель от подвергнется in nce processing
180,2024-03-17,маск долго тянул интригу но таки к исходу недели выпустил на волю 1 здоровенная херобора на 314b 2 код модели и инференса на jaxhaiku 3 8 экспертов на инференсе активируются 2 4 словарь 128к токенов 5 модель идет с 8битной квантизацией что все равно потребует 4 a100 чтобы только уместить 6 длина контекста 8к 7 архитектура навскидку та же что у mixtral llamaподобный трансформер
181,2024-03-18,в связи со стремительным ростом размера моделей все более острой становится проблема memoryefficient обучения llm по существу рядовому пользователю доступны лишь файнтьюны предобученных моделей в разобранной ранее статье было предложено обучать несколько низкоранговых добавок и вливать их в веса однако для достижения качества не сильно уступающему обучению всей модели целиком этап обучения всей модели целиком был необходим то есть обучения все равно не избежать в этой статье был предложен подход который позволил достичь с llama7bподобной моделью качества близкого к полному обучению доступный владельцам 1 с vram суть метода довольно проста и не нова по существу низкоранговые добавки неплохо работают на стадии но эффективный pretrain требует заметать в процессе обучения пространство однако сами могут и авторы даже дают некоторое обоснование данному явлению лежать в пространстве низкой размерности отсюда мысль проектировать градиенты и состояния оптимизатора в пространство низкой размерности а именно делают следующее 1 считают градиент по весу на nшаге 2 считают его откуда достают первые r векторов отвечающих главным сингулярным значениям 3 проектируют на полученные подпространства состояния оптимизатора в основе работы метода предположение о том что градиенты меняются между соседними итерациями прикрутить его можно болееменее к любому стандартному градиентному алгоритму оптимизации sgd adam хоть исходный хоть требует даже меньше памяти на линейный слой c n входными и m выходными нейронами чем lora с adam lora параметров состояний оптимизатора galore параметров состояний оптимизатора
182,2024-03-18,рассматривают две постановки 1 предобучение на с4 2 дообучение на glue обучают на c4 на бюджетах порядка токенов ибо авторы не слишком богатые на предобучении galore лишь немного уступает стандартному обучению всей модели целиком и заметно опережает relora просто низкоранговые веса работают ожидаемо плохо galore хорошо совмещается с давая значительную экономию в памяти и не просаживая качество экономия по памяти по сравнению с bf16 adam и c 8bit adam соответственно основные два параметра у алгоритма 1 низкорангового пространства и 2 обновления чем больше размерность тем ближе алгоритм к adam но и прожорливее по памяти слишком часто обновлять матрицы проекции вычислительно накладно и не очень хорошо по качеству слишком редко дешево но плохо по качеству лучше всего работает обновление раз в 100 шагов обучения полагаю оптимум зависит от размера батча и не увеличивает среднее время на шаг обучения дообучение на glue дает примерно то же качество что и lora при том же ранге по ощущениям хороший и полезный результат с серьезными перспективами на практике интересно насколько хорошо предложенный метод сработает на largescale обучении порядка триллиона токенов правда те кто могут себе позволить себе обучать на триллионах токенов имеют в распоряжении не один хост с highend gpu
183,2024-03-20,невероятный прирост производительности каждого нового поколения nvidia на графиках в презентациях gtc кроме объективного улучшения архитектуры обусловлен еще и переходом к типам все более точности что было остроумно подмечено как известно инферить нейросетки с весами низкой точности можно и нужно но вопрос в том насколько сложно обучать в числах с плавающей точкой пониженной точности наивное обучение в fp16 без как известно нередко приводит к неприятным сюрпризам типа nan в лоссах и градиентах градиенты могут быть слишком большими чтобы представляться в fp16 либо слишком маленькими и адаптивно подбираемый loss scale сдвигает гистограмму градиентов в нужный диапазон значений есть еще bf16 поддерживаемый начиная с архитектуры который имеет более широкий диапазон но большую погрешность представлений и в семействе добавили поддержку вычислений с на самое деле не 1 а 2 типа c 4 битами на экспоненту и 3 на мантиссу используется для весов и активаций которые обычно лежат в сравнительно узком диапазоне и важнее точность представления чем возможность принимать экстремальные значения бесконечность не представлена с 5 битами на экспоненту и 3 на мантиссу используется для градиентов и состояний оптимизатора где допустима большая погрешность в представлении и больше разброс принимаемых значений обычный как в half precision не работает и приходится иметь для каждого тензора масштаб как в квантизации чтобы загнать его в удобный диапазон определять его на лету накладно и потому предлагается хранить некоторое максимумов и на него масштабировать авторы тестируют эффективность обучения в fp8 на imagenet1k lstm на wmt и претрейне gpt3 подобного трансформера на pile fp8 почти везде смог показать конечное качество как half precision бейзлайн за исключением mobilenetv2 где точность просела на 05 на языковых задачах fp8 модели достигают примерно того же качества что и half precision в ablation показывают что адаптивный масштаб для каждого тензора при конвертации bfloat16 модели в fp8 иначе заметно проседает качество даже при оптимальном выборе сдвига экспоненты предобученный halfprecision bert без проблем представляется в fp8 fp8 тип выглядит вполне полезным и перспективным однако обучение llmок с ним по всей видимости требует дополнительной возни и несколько вагонов с и маленькую тележку потому на текущий момент известные открытые модели обучались в half precision вероятно openai и anthropic чтото пробовали шаманить в fp8 но кто об этом расскажет
184,2024-03-25,появилась мистрали7b которая является с настоящий момент болееменее sota среди публичных моделей в легкой весовой категории однако судя по замерам пользователей новая версия не то чтобы улучшилась по сравнению со своей предшественницей
185,2024-03-25,по собственным замерам наблюдаю небольшое улучшение перплексии на 8k контексте и нестатзначимую просадку на 0shot возможно при конвертации пользователь с ником alpindale немного повредил модельку
186,2024-03-27,позволяют обучать понастоящему глубокие сети впервые они появились в статьях про resnet и unet и с тех пор было предложено множество их вариаций connections и зоопарк вариаций тот же трансформер унаследовал структуру skip connections от resnet в этой работе по существу воплотили в трансформере densenetlike skip connections реализованы следующим образом ключи и значения в данном блоке получаются как взвешенная сумма ключей и значений с этого и всех или не только лишь всех предыдущих блоков дополнительного оверхеда по памяти так как kvкэши активации с прошлых блоков все равно придется сохранять для backpropagation или авторегрессивной генерации чтобы воспроизводить поведение стандартного трансформера веса с прошлых блоков инициализируются нулями однако все равно приходится делать массу сложений что замедляет работу сети потому авторы предлагают две модификации 1 считается взвешенная сумма только через kслоев 2 добавление kv проводится только для каждого kго токена обе модификации снижают объем вычислений не снижая качество модели обучают openwebtext и смотрят на перплексию на валидационной выборке бейзлайновые сети довольно глубокие блоков и узкие 8 голов по 64 канала при том же числе параметров denseformer немного лучше трансформера по перплексии но и медленнее потому авторы сравниваются еще и в сетапе с заданным временем инференса и denseformer оказывается все еще эффективнее и опции заметно ускоряют модель при этом не просаживая по качеству с полносвязным denseformer далее смотрят на величину выученных коэффициентов ожидаемо наибольший вес имеют коэффициенты в данном блоке но еще сильны skip connections с первым блоком многие веса малы по величине тем не менее важны и прунинг даже небольшой доли самых малых весов сажает качество дешевая и вроде бы эффективная настройка над трансформером однако постановка экспериментов изначально неблагоприятна для базовой модели так как глубокие узкие модели имеют свойство тяжело обучаться вероятно более широкий трансформер с тем же числом параметров отработал бы лучше а для неглубоких сетей скорее всего выигрыш от конструкции незначителен да и много что дающее выигрыш на малых экспериментах не воспроизводится на масштабе
187,2024-03-29,спасибо за наводку сжатие и ускорение llm нынче пользуется большим спросом и потому существуют кожаные мешки пытающиеся сорвать хайп на этой теме и вот недавно вышел воистину чудный образец 1 квантуют модель методом 2 дообучают адаптер поверх квантованной модели все вместе называется hqq все эксперименты проводятся с llama27b для дообучения базовой модели берут 28k примеров из wikitext2 для instruction finetuning cмесь из guanaco orcamath metamathqa ultrafeedback_binarized на базовой модели hqq ломает полностью модель при 1битном квантовании но lora адаптер якобы выравнивает модель по качеству с 2битным quip а 2битная по перплексии даже лучше исходной переходим на 1 и 2битные модели но есть нюансы 1 сравнение проводится со старой версией quip да и то почемуто перплексия хуже чем заявлено в а новые версии aqlm и quip достигают перплексии 62 на wikitext2 2 нет замеров на какихлибо других бенчмарках хоть c4 и 0shotах из lmevalharness потому наверняка просто имеем дело с оверфитом под датасет а сами модели не рабочие гделибо еще при aqlm квантовании в 2 бита у нас тоже улучшилась перплексия по сравнению с fp16 но качество на 0шотах было хуже чем у базовой модели так что нихрена это не улучшение при instruction finetuning 1bit hqq снова ломает модель до уровня рандома качество около 1 число ответов но адаптер позволяет оторваться на 10 от уровня рандома а 2битное квантование уже близко по качеству к llama27bchat однако снова нет сравнения с sota quantization methods и бенчмарки вызывают вопросы llama27bchat на truthfulqa имеет качество 5704 а здесь репортят 4532 по большей части лютый скам но все же некоторая мораль есть адаптер поверх сжатой модели дает серьезную компенсацию даже для поломанной модели идея применить адаптер поверх квантованной модели не нова и ведет начало как минимум от
188,2024-03-30,когда в торчовом модуле линейные слои реализованы через матричное умножение на параметер а не через nnlinear
189,2024-03-30,пояснение к приколу выше многие методы dataaware квантизациипрунинга зиждятся на forward хуках поверх nnlinear nnconvd
190,2024-03-30,что вам мешало cделать nnmodulelist из nnlinear
191,2024-03-31,очередная статья про квантизацию но в другом контексте вариационные автокодировщики с дискретными латентными кодами показали себя в качесте неплохих генеративных моделей кроме того их способность эффективно представлять высокоразмерные данные лежит в основе латентных диффузионных моделей однако обучение vqvae довольно нетривиальная задача требующая тонкой настройки в отсутствие специальных манипуляций активации отображаются в кодовых векторов в то время как основная масса кодов сидит и грустит приходится накладывать регуляризацию чтобы заметать кодовую книгу коды обновляются через также требует определенных вычислительных затрат в этой работе предложили использовать фиксированную решетку для квантования представлений входные активации отображаются в некоторый гиперкуб с заданным числом узлов вдоль каждой размерности чтобы ограничить диапазон принимаемых значений ко входу применяют округление к ближайшему целому всевозможные узлы в этой решетке те размер кодовой книги ld размерность латентного пространства причем размерность пространства небольшая не более 10 во всех экспериментах достоинством подхода является нужно просто найти нужный узел на решетке зная координаты и меньшее число хотя коды и так мало весят по сравнению с энкодером и декодером как и стандартном vae квантование недифференцируемо и потому градиент пробрасывается через ste метод валидируют на архитектурах генерация изображений и оценка глубины на малом числе кодовых векторов стандартная формулировка работает немного лучше с увеличением числа кодов vqvae общего вида начинает страдать в качестве в то время как у fsq метрики монотонно уменьшаются по fid оптимальное качество fsq на одном уровне с vq fsq задействует кодовые векторы в то время как у базового vq есть неиспользуемые коды на depth estimation метрики также на одном уровне с vqvae общего вида занятно что в области vqvae пошли в противоположную сторону от текущих трендов квантизации llm от более общей векторной квантизации к скалярной интересно насколько полученные коды хороши для обучения латентных диффузионных моделей
192,2024-04-01,уральский умелец собрал рассказы и премудрости своего деда перевел их на английский дообучил на этом mixtral8x7b и вырвался на первую строчку openllmleaderboard c большим отрывом
193,2024-04-02,хотите вы протестировать фунциональность вашего кода на какойто новомодной llmке но десятки а то и сотни гигов качать впадлу да и если раздебаг на колабе банально и не влезет кроме того при прогонке кода обычно приходится ждать пока чекпоинт подгрузится да и прогонка активаций через большую модель занимает время решение простое 1 cоздаете модель через 2 урезаете число слоев и ширину модели 3 вуаля можете хоть на cpu гонять допер только на днях до этого гонял llama27b в большинстве тестов
194,2024-04-05,не у одного меня бомбануло с этой особенности dbrx и ребята подняли в репозитории модели пользователи жалуются на отсутствие возможности конвертации в формат и обучения поверх экспертов потому один умелец приготовил модель где эксперты используют
195,2024-04-05,кода нет а и tpuv5 у вас есть такие согласно которым чем больше модель и чем больше количество данных на которых она училась тем лучше ее качество или некий эрзац качества для llm уже вышло множество работ по данной теме и выработаны computeoptimal рецепты обучения но для систематического исследования до сих по не проводилось и ребята с вооруженные решили заполнить зияющую пустоту в качестве базовой модели берут от 866m и меняют его ширину глубина постоянная и таким образом получают семейство моделей от до параметров модели обучают на проприетарном датасете из пар изображений и описаний к ним отфильтрованном по эстетике базовые настройки сэмплера солвер с шагами и исследуют следующие задачи 1 text2image генерация 2 superresolution через дообучение модели из 1 следуя постановке из 3 dreambooth поверх модели из 1 1 для оценки качества используют по традиции и в области некоторого среднего бюджета обучения модели разных размеров выдают примерно одни и те же метрики но при наличии значительного объёма вычислительных ресурсов размер начинает играть значение и самая большая модель побеждает своих меньших собратьев для 5b модели пришлось прибегнуть к так как при стандартном обучении с модель с состояниями оптимизатора не влезает в tpuv5 совсем маленькие модели генерируют дефектные картинки но начиная с нескольких сот миллионов параметров качество генераций моделей примерно на одном уровне по ощущениям 2 на downstream задачах размер моделей играет уже более существенную роль в 4x image superresolution большие модели даже при заданном бюджете обучения заметно выигрывают по fid однако по lpips модели разного размера с фиксированным training compute сравниваются 3 большие модели ожидаемо выдают более качественные генерации
196,2024-04-05,далее авторы исследуют зависимость качества генераций от cfg_scale для разных размеров моделей и количества шагов сэмплирования выводы следующие 1 чем шагов сэмплирования тем оптимальный cfg scale 2 чем модель тем оптимальный cfg scale при генерации с фиксированным ограничением на flops модели на малых бюджетах показывают себя больших то есть лучше сделать больше шагов меньшей моделью чем меньше большей выводы справедливы как для семплера так и для более навороченного возникает следующая мысль оказывается что модель с шагов сэмплирования может выдавать качество что и в 4 шага в качестве алгоритма дистилляции используют на downstream superresolution задаче при малых бюджетах малые модели предпочтительнее но с ослаблением ограничений ожидаемо выгоднее становится брать более крупные ибо малые упираются в потолок по качеству довольно интересное и содержательное исследование на мой взгляд самый практически интересный результат работы в том что при жестких ограничениях на бюджет генерации предпочтительнее уменьшать модель чем число шагов генерации однако исследование ограничительно тем что использует метрики которые могут плохо кореллировать с human perception и полноценная валидация предполагает sbsы тем более что у гугла деньги на это есть
197,2024-04-06,некоторое время назад здесь мы разбирали работу с довольно спорными заявлениями и сейчас откатимся немного назад в прошлое и рассмотрим исходный метод квантизации 1 есть методы квантизации roundtonearest bnb квантование не требующие входных данных 2 есть методы spqr quip aqlm и др которые ищут конфигурацию квантованных весов оптимизирующую ошибку на данных вторая группа методов обычно точнее по качеству но более вычислительно затратна кроме того имеет место переобучение под калибровочные данные в данной работе предлагают метод якобы гарантирующий одновременно хорошее качество и не требующих никаких данных исходная функция ошибки норма которая устойчива к выбросам характерным для больших языковых моделей квантизации фиксируют и оптимизируют однако в исходном виде задача плохо решается изза потому авторы вводят вспомогательную переменную которая в оптимуме равна разнице между исходными и квантованными весами и решают полученную задачу поочередно оптимизируя скейлы и фиктивную переменную согласно заявлениям авторов метод сходится за пару десятков итераций метод валидируют на моделях семейства llama2 и openclip vitах в первом случае замеряют перплексию на wikitext2 а во втором качество и 8битная квантизация работает без просадки по качеству 4битная с небольшой 2 и 3 битные квантизации уже заметно просаживаются по сравнению с fp16 но все еще выдают стабильный результат но тут есть нюанс 2х и 3х битные квантизации используют малые группы 64 для 3битной и 16 32 для 2битной в одной постановке scale хранится в 16 битах в другой квантуется в 8 бит потому это на самом деле 35 бита на параметр а 3 бита работает метод быстро 1 минута для llama27b13b и 4 минуты для llama270b на 1ой a100 vitb заметно просаживается по качеству при квантовании но его большие собратья vitl vith сохраняют неплохие метрики даже при 2битной квантизации размер группы не уточняется потому непонятно настоящие это 2 бита или 3 с лишним на текущий момент данный метод наверное лучший среди dataagnostic методов квантизации тем не менее без информации о входном распределении данных хорошее качество при высокой степени сжатия модели недостижимо
198,2024-04-09,исследователи со всего мира уже многие годы ищут оптимальную по скорости и качеству парадигму генерации нынче самая ходовая тема со своими достоинствами и недостатками но на ней клином свет не сошелся и целесобразно рассматривать и альтернативу некоторые время назад на свет появились для генерации мотивированные успехом больших языковых моделей однако они не приобрели столь большой популярности как диффузионки изза худшего качества и медленной генерации генерить токен за токеном долгую последовательность долго и утомительно вычислительная сложность у тех кто не знает что можно кэшировать keys и values и чуваки решили пересмотреть так чтобы они работали быстро и эффективно суть метода в следующем изображение кодируют автокодировщиком где каждый уровень представлен в виде некоторой в некотором смысле подобная схема напоминает сначала пытаются приблизить одним токеном насколько возможно исходную картинку затем увеличивают feature map и уже приближают residual и так вплоть до максимального разрешения токены лежат в некотором обучаемом кодбуке и на этапе квантования токен полученный некоторой проекцией приближается к ближайшему вектору из кодовой книги словарь общий для всех уровней иерархии автокодировщик обучается на взвешенную сумму l2 лосса адверсариального лосса и lpips генерация выглядит так на каждом уровне за раз генерируются токенов соответствующих данному уровню обуславливаясь на все токены с прошлых уровней таким образом нужно делать порядка logn проходов вместо n2 что является безусловным прогрессом в качестве модели берут обычный трансформер без модных финтифлюшек типа swiglu rope и прочих приблуд а старый дедовский gpt2like c разницей лишь в том что вместо обычных используются обусловленные на класс
199,2024-04-09,подход валидируют на classconditional генерации на imagenet1k в разрешении 256x256 и 512x512 если смотреть на графики fid на figure 1 складывается впечатления что вообще красота победа c разгромным счетом на ditбейзлайнами однако взгляд придирчивого читателя обнаружит некие нюансы в таблице их лучшая модель генерирует изображение 256x256 за 1 секунду а на графике будто бы за 03 сек если бахнуть stylegan2 styleganxl на этот график то те будут смотреться паретооптимальнее их модельки хотя о чем вообще базар мера оверфита под по неплохо но не лучше всех на 512x512 тоже все солидно опережают dit далее показывают что модель демонстрирует ярко выраженные хорошую масштабируемость встает вопрос а у чего нет визуально качество с размером модели тоже улучшается малые модели генерируют дефектные изображения а большие уже вполне добротные еще модель может в inpainting outpainting и classconditional editing в ablation показывают следующее 1 var парадигма рулит по сравнению с ar 2 adaln чутка накидывает по сравнению с unconditional ln 3 topk sampling тоже накидывает 4 cfgнакидывает хоть тут и не диффузия но тоже можно определить его подобным образом 5 увеличение размера тоже накидывает прикольная и интересная идея но непонятно насколько масштабируется на более сложную задачу text2image диффузии есть опасение что одношаговый алгоритм генерации будет страдать разнообразием по сравнению с современными диффузионными моделями а может регулирование сэмплирования даст разнообразие в той степени какой оно нужно для целевого приложения
200,2024-04-09,папира про на контент следующий 1 описание модели 2 описание пайплайна подготовки данных 3 описание сетапа обучения 4 scaling laws 5 исследование влияния качества данных fid не замеряем
201,2024-04-09,картиночки
202,2024-04-10,эксперименты и результаты в статьях по современным генеративным моделям будь то новая парадигма генерации постановка диффузии или архитектурное новвоведение натокнули на следующую мысль самый низкий хреновый показатель по упомянутым неоднократно причинам интуитивно хочется следующего чтобы при подаче конкретного класса будь то рыба собака или велосипед генерировалась эта рыба собака или велосипед без дефектов ну и на какомто разумном фоне вопрос эстетичности особо не стоит ибо imagenet1k не проходил фильтрации по эстетичности и иным аспектам а требовать генерировать от модели лучше чем то что она видела в данных мы обьективно не можем что посеешь то и пожнешь как говорится в идеале хороший классификатор должен распознавать генерации как целевой класс но добиться этого со точностью нельзя изза ошибок в разметке imagenet и спорности самой разметки на мой взгляд нельзя утверждать что конкретная модель является по classconditional генерации на imagenet1k можно лишь утверждать что она решает задачу или
203,2024-04-11,конечно сказать наверняка нельзя но сильные результаты моделей большая из которых вошла в десятку на и даже обошла некоторые версии gpt4 наводят на мысль что и сама gpt4 может быть не такой уж огромной и даже одного порядка по размеру с gpt3 параметров а помните что gpt4 это якобы смесь экспертов с параметров инферить которую придется на нескольких хостах полагаю что все же основной прирост был за счет большего количества данных лучшей их фильтрации и предобработки маленьких секретиков по instructiontuning rlhf с достаточным количеством разнообразных инструкций и обширным подбором ассессоров из разнообразных групп
204,2024-04-11,к дискуссии о размере модели и стоимости инференса и отличаются в 3 раза по размеру примерно столько же по flops но по стоимости генерации входных токенов в 6 раз а выходных в 10 по всей видимости скейлинг стоимости модели и инференса полагаю уместно предположить что как более известный бренд берет большую наценку чем таблица стоимости токенов ниже 1 gpt35 turbo gpt35turbo0125 2 gpt4 gpt4 не 32к кпд думайте подписаться
205,2024-04-13,выложили квантованную версию меньшей из не без просадки в качестве но зато замерили на чуть более пацанских бенчмарках почему чекпоинт весит целых 1 хотя больший по числу параметров 45b параметров занимает все дело в она же
206,2024-04-14,by локальный инференс больших языковых моделей на пользовательском железе пользуется большим спросом и за последние два года на свет появилось множество движков для локального инференса и так как llmки нынче большие приходится пользоваться их версией один из самых известных и популярных движков от turpoderp библиотечка суть standalone реализация на llama и некоторых ее производных 1 использует vanilla для квантования моделей 2 в отличие от первой версии позволяет квантовать слои в бит по отдельности и иметь разную битность даже в пределах одного слоя потому можно произвести модель любой битности от 2 до 8 метод создает несколько сжатых версий данного слоя и в итоге выбирается конфигурация ошибку квантизации на выходе при заданном целевое железо серий на более старых моделях движок не так эффективен как утверждает сам творец на хабе лежит в данном формате поддержка формата добавлена в
207,2024-04-17,очередная статья от гугла без кода в попытках найти эффективную альтернативу стандартному механизму внимания в трансфромере человечество перевело не одни джунгли в амазонии выбросило тучу co2 за последние годы было предложено множество интересных механизмов эффективеого attention statespace модели и переосмыслены рекуррентные модели но все подходы так или иначе уступали в выразительности трансформеру и группа из google предложила очередную модификацию внимания способную работать с длинным контекстом с асимптотической линейной сложностью по вычислениям и компактной памятью не зависящей от длины за основу берут еще старый добрый который считает внимание в пределах некоторого сегмента фиксированного размера однако ограниченность такого подхода в том что кэши считаются только для последнего сегмента потому нет возможности учитывать контекст с прошлых окон и суммарный контекст ограничен размером сегмента на глубину сети в данной работе предложили привнести рекуррентность в transformerxl и поддерживают два состояния памяти 1 m размера в числителе 2 z размера i в знаменателе и некая комбинация составленная из этих сущностей и query в текущем сегменте выступает в качестве одного из членов в итоговой формуле attention которая будет определена чуть ниже состояние m обновляется после каждого сегмента через некоторую формулу с внешним произведением key valuе в текущем сегменте а z через сумму от ключей к которым применили функцию активации в данном сегменте те z является по существу скользящей суммой итоговый контекст получается как в данном окне и полученного выше относительный вес каждого слагаемого получается из обучаемого скаляра отдельного для каждой головы внимания метод валидируют на бенчмарках по языковому моделированию с длинным контекстом passkey retrieval и суммаризации книг 500k контекста в первом эксперименте берут трансформер с 12 слоями и hidden_dim 1024 по перплексии метод заметно опережает конкурентные подходы transformerxl при этом имея значительно меньшее потребление памяти c контекстом вплоть до 1m токенов на infinitransformer так же бьет и и обе с опцией будто бы существенно но не радикально идея объединить локальное контекстное окно с рекуррентной памятью не выглядит принципиально экспериментальная валидация в статье недостаточна не хватает очевидного сравнения с теми же моделями как мне кажется предложенный подход вряд ли сможет успешно решать с хорошей точность задачи требующие и способности запоминать несколько фактов одновременно из далекого прошлого разнесенных по времени с произвольными интервалами между ними
208,2024-04-18,после сегодняшнего релиза меты
209,2024-04-18,релиз 3 таки не оказался 18апрельской шуткой выпустили ровно спустя 8 месяцев после 2ой версии что известно на данный момент 1 15 t токенов на обучении в 7 раз больше чем у llama2 2 8к контекстное окно 3 95 обучающей выборки на английском и остальные 5 на других 30 языцех 4 instructionfinetuning включает sft dpo ppo 1 архитектура не поменялась не moe 2 8b тоже gqa 3 размер токенизатора увеличили до 128к 1 8b модель бьет модели аналогичного размера mistral gemma на бенчах 2 70b модель бьет geminipro1 15 mixtral 8x22b и claude 3 sonnet в ходе разработки собрали свой датасет из 1800 разнообразны инструкций на котором замерялись 1 400b модель которая еще учится предьявили метрики на чекпоинте от 15 апреля 2 будет техрепорт 3 накатят еще более длинный контекст
210,2024-04-21,aqlm квантизация добежала спустя 2 недели и 50 перезапусков сбоев инфраструктуры и прочих шалостей полтергейста ценность релиза еще дня 34 назад была бы на порядок больше но надо же было довести до ума давно обещанное история к сожалению не про локальный инференс на consumergrade gpu ибо даже сжатая модель весит 319 gb целевая аудитория обладатели gpu уровня от a40 a6000
211,2024-04-24,спасибо за наводку недели не прошло как llama3 появилась на свет как группа исследователей из китая с присущей только азиатам скоростью опубликовала исследования про то как разные методы квантования и дообучения квантованных моделей просаживают качество авторы рассматривают 8 болееменее современных методов ptq posttraining квантования rtn gptq awq quip billm smoothquant и два метода дообучения квантованных моделей qlora качество замеряют на стандартных бенчах по перплексии и 0shot на lmevalharness 5shot mmlu в первом случае рассматривают weight only квантование в 1 2 3 4 8 бит для всех методов окромя smoothquant и w4a4 w6a6 w8a8 квантование и весов и активаций для smoothquant для калибровки используют 128 последовательностей из wikitext2train длины 2к мало заметная просадка наблюдается уже в битах а при приближении к битам большинство методов полностью ломают модель или поднимают значение перплексии до 2значных значений против 1значных замеряют на 2к контекстном окне хотя было бы логичнее использовать 8к длине контекста на обучении для дообучения с qloraми используют инструкции из alpaca lora адаптеры применяют поверх квантования из bitsandbytes rtn в normalfloat4 что любопытно все файтьюны портят качество по сравнению с просто квантованной моделью варианта здесь два либо инструкции из alpaca слишком плохи либо авторы криво завели дообучение учитывая что можно было запустить дообучение с lr0 как минимум реально не просадить качество авторы явно предпочли скорость публикации ее качеству и полноте методы векторной квантизации никак не отражены в работе справедливости ради стоит заметить что они требуют значительно больших вычислительных затрат и времени по сравнению с рассмотренными выше тем не менее основное заключение статьи про то что качество сложнее сохранить при сжатии модели по сравнению с ее предшественниками и иными семействами открытых моделей похоже действительно имеет место полагаю что это логично ибо веса модели обученной на токенов в какомто смысле должны быть более насыщенными информацией
212,2024-04-25,вы llmками пользуетесь нет только квантуем красивое
213,2024-04-30,почему интеллектуальные люди часто близорукие они повышают frames per second за счет прорисовки текстур
214,2024-05-02,aqlm приняли на icml
215,2024-05-02,в основе всех ну почти всех современных архитектур лежит многослойный перцептрон mlp с обучаемыми матрицами сдвигами и фиксированными активациями и некоторым механизмом агрегации для пространственных входов свертки attention statespaces мамба гласит что при некоторых на целевую функцию и функции активации в сети достаточно большой сетью можно приблизить эту самую целевую функцию возникает вопрос ли такой подход по вычислениям точности и нельзя ли изобрести нечто лучшее в данной статье авторы переосмысляют и в некотором смысле обобщают парадигму построения многослойной сети в основе идеи лежит знаменитая что непрерывную многомерную функцию на ограниченной области можно всегда представить в виде композиции функций от одной переменной однако при этом теорема не дает явного вида этих функций которые могут оказаться сколько угодно плохими потому не реализуема на практике в данной статье предлагают выучивать сами параметризуя их некоторым образом каждое ребро между входным и выходным нейроном задается некоторой довольно общего вида традиционный mlp является одним из предлагаемой парадигмы в оригинальной теореме перцептрон всего с но ничто не мешает технически настакать их побольше на практике kanслой реализуется как с residual connections домноженный на константу такого сплайна довольно нетрививальна и для улучшения сходимости сплайн инициализирует так чтобы быть близким к нулю в начальный момент времени и сетка с узлами сплайна обновляется на лету при той же глубине и ширине в kanсети больше параметров чем в классической mlp в g g размер сетки раз но мотивация работы в том что kan требуется меньшая ширина для достижения сопоставимого качества далее авторы обосновывают что kan обладает значительно в сравнении c mlp и обходит проклятие размерности за счет того что представляет многомерную функцию в виде композиции одномерных тем самым переводя задачу эффективно в низкоразмерное пространство и выводят степенной закон убывания функции потерь для kan в однослойной сети аппроксимирующие функции могут быть очень плохими но с ростом глубины существуют все более способные решать целевую задачу по ходу дела для повышения выразительности сети можно добавлять в сплайн
216,2024-05-02,предлагаемую парадигму валидируют на аппроксимации явно заданных и еще в ряде физических приложений решении уравнений в исследовании в разных решеточных системах при том же числе параметров kan сходятся значительно и достигают качества по сравнению с традиционными архитектурами проблема переобучения тем не менее все равно может иметь место кроме того для повышения интерпретируемости авторы предлагают накладывать регуляризацию для удаления побочных связей в сети и полученная спарсифицированная сеть на игрушечных примерах действительно вполне интуитивна работа весьма занятная и интересная однако преждевременно утверждать что предложенный подход тут же возьмет и вытеснит mlp вопервых требуется валидация на реальных данных удивительно что авторы не показывают эксперименты на том же mnist или табличных данных которые должны быть по зубам даже исследователям с ограниченными ресурсами возможно kan просто имеют хороший для решения определенного круга задач кроме того текущая парадигма построения сетей хороша тем что удачно ложится на возможности современных gpu способных параллелизовать эффективно операции с большими тензорами kan же не настолько в этом отношении но могут найти свою нишу при инференсе на cpu и fpga для приложений математики и физики kan безусловно представляют интерес но можно ли ими заменить mlp в трансформере поживем увидим
217,2024-05-03,абитуриентов в принципе можно евалить как llmки вместо егэ давать ru а на вступительных давать проверяющему которого можно заменить gpt4 или зелибобой ответы двух абитуриентов без указания их имен и затем отсекать по elo score
218,2024-05-04,1x16 квантизации 3 на хабе 1 2 3 4 дело заняло несколько дольше времени чем предполагалось новую линейку llmок от меты оказалось сложнее квантовать по сравнению с предшественниками с приемлемой просадкой в качестве а выкладывать шлак под красивой этикеткой не позволял кодекс чести самурая пришлось улучшить процедуру файнтьюна больше токенов больше компьюта но в итоге добили до приемлемого качества пользуйтесь делитесь впечатлениями 8b версия великовата 4gb изза больших эмбедов и lm_head так как словарь большой в будущем планируем попробовать посжимать и эмбеды языковую голову
219,2024-05-13,полагаю многие уже успели восхититься выдающимися возможностями нового детища от openai детальный разбор и сводка имеющейся информации приведена в постах так что дабы сэкономить на генерации собственных токенов я не буду повторяться на мой взгляд самое удивительное во всем этом что новая модель не только стала лучше обрела новые возможности и кучу мелких и крупных плюшек но и стала дешевле в использовании безумно любопытно в чем же секрет столь эффективного и быстрого инференса при сохранении высочайшего качества подозреваю что там весьма нетривиальное сочетание алгоритмов сжатия и ускорения разных оптимизаций движка вероятно и архитектурные навороты смесь несмесь экспертов может в ширину а может и в глубину а может просто большой трансформер без причуд и возможнось бесплатного пользования моделью хоть и в ограниченном количестве очень неожиданный ход от openai которые хоть немножечко но приоткрылись но думаю что все продумано не разорятся на этом жесте доброй воли
220,2024-05-13,судя по имеет место не только по размеру модели и обьему данных на обучении но и числу рисерчеров при обучении llmок
221,2024-05-22,с приближением дедлайна neurips частота сообщений про новый revision стремительно нарастает
222,2024-05-24,квантование kvкэшей добавили в чтобы не пересчитывать каждый раз все заново при авторегрессивной генерации в трансформерах используются ключи и значения посчитанные для прошлых токенов сохраняются в памяти и используются при подсчете attention для каждого нового токена однако есть проблема если последовательность длинная кэш слишком тяжелый становится для 1 или 2 7b для 10к токенов потребуется 5gb памяти 2 2 32 32 128 10000 5gb а для более крупных моделей и если контекст переваливает за миллион страшно представить сколько существуют разные подходы к сжатию kvкэшей и один из наиболее очевидных квантизация кэша преставление ключей и значений в более низкой битности в работе предложили 2битное ассиметричное с zero point квантование кэшей ключи квантуются поканально значения по токенно поканальное квантование помогает бороться с outliers в hf использует потокенное квантование для ключей и значений из соображений скорости несколько жертвуя качеством кроме того чтобы немного улучшить качество самые свежие токенов держат в исходной точности они обычно самые важные для текущего токена и не добавляют много памяти для длинного контекста hf поддерживает два метода квантизации простой rounttonearest оба метода на замерах перплексии на llama27b на pg19 4битный hqq не просаживает качество а умеренно просаживает 2битные квантизации сильно роняют качество на в 4 битах почти нет просадки по качеству на длинных последовательностях удается достичь до 25x экономии потребляемой памяти скорость генерации однако замедляется так как на текущий момент нет оптимизированных ядер просадка по скорости 3050 судя по графикам крайняя правая точка на графике number_of_tokenssec явно забагованная кроме того данная стратегия помогает именно на а заполнение промпта prefill приходится делать стандартным способом метод совместим с flash attention достаточно установить и прописать аргументы в методе
223,2024-06-01,llmки не квантовал за последние годдва только ленивый потому назрело время осваивать и другие ниши квантование диффузионных моделей на текущий момент пока не столь исследовано как llmки в связи с тем что сами модели не доросли до своих собратьев из nlp потому и не было столь острой необходимости тем не менее прогресс не стоит на месте и стоит быть готовым к дальнейшему масштабированию диффузионных тушек в рассматриваемой статье авторы перенесли метод тернарной квантизации quantizationawaretraining из на ditы для classconditional генерации на imagenet квантуют только веса активации остаются в исходной точности по существу ничего нового по сравнению с веса обучаются через estimator ste с большим learning rate единственное нововведение нормализация на выходе adalayernorm авторы обнаружили что тернарные веса выдают большие активации и scaleshiftgate модуляции слишком велики чтобы сетка могла нормально сходиться навешивание rmsnorm на конец mlp для получения модуляций решает проблему метод валидируют на ditах двух размеров с 600m параметров примерно как ditxl из и 42b параметров на classconditional генерацию на 256x256 по метрикам тернарная 42b модель примерно равна ditxl 600m несколько хуже то есть для большой модели близкое качество к floating point модели при чуть меньшем общем размере модели параметров больше в 7 раз бит на параметр 10 меньше чем в fp16 справедливости ради стоит заметить что terdit обучался меньшее число итераций по сравнению с моделью из статьи фейсбука с инференсом немного грустненько получилось для работы с тернарными весами берут кернелы из hqq и деквантизуют на ходу квантованные модели медленнее на 2025 а при опущенном сравнении с fp16 замедление было бы порядка 3 раз зато неплохая экономия по памяти 42b моделька есть 3gb видеопамяти на пике при инференсе в приложении еще зачемто показывают что существующие 4битных квантизации ломают полностью dit берут правда smoothquant который в отсутствие квантования активаций вырождается в roundtonearest rtn те самый наивный и грубый метод при существовании куда более сильных ptq методов для диффузии с одной стороны очередное подтверждение того что тернарный qat както да работает однако результат куда скромнее того что получили для llm майкрософты и с таким замедлением инференса вряд ли интересен практикам неизвестно масштабируется ли он на случай более сложной задачи text2image генерации тем не менее деятельности представляет определенный интерес и развитием эффективных алгоритмов qat вероятно тернарные модели вполне могут быть около паретооптимальными во всяком случае в некоторых приложениях
224,2024-06-03,не проплаченной рекламы пост хочу порекомендовать канал на данном канале вы можете найти сборную солянку интересных и полезных материалов во всякой всячине связанной с нейронками и более чем годные туториалы от самого автора в частности внимания заслуживает недавняя ветка постов про и с нуля
225,2024-06-08,yet another свежая статья про экстремальную квантизацию диффузионок от snap research утверждают что 2битная модель даже исходной если у вас завалялся старый игровой пк года так 2007 можно его не выбрасывать а генерить картинки стейбл диффузией по существу предложенный метод представляет собой qat квантизейшн эвер трэйнинг с mixedprecision квантизацией и дистилляцией каждый слой квантуется в 1 2 3 бита или не квантуется вовсе в прошлых работах было показано что диффузионные модели очень чувствительны к сжатию timestep проекций обуславливающих на текущий шаг расшумления поэтому следуя стандартной практике их не сжимают авторы анализируют чувствительность разных слоев к сжатию замеряя mse с картинкой сгенерированной оригинальной моделью и clip score при квантовании слоев по отдельности и замечают что данные две метрики не всегда скоррелированны в частности сжатие слоев в cross attention слоях не сильно так сильно влияет на mse но при этом временами ломает семантику изображения свертки очень важны каждому слою сопоставляется важность в зависимости от mse и числа параметров в нем и подбирается порог такой что достигается целевая степень сжатия стратегия по определению масштабов квантования не учитывает наличие выбросов в распределении весов поэтому авторы применяют итеративный алгоритм для минимизации ошибки кроме того важно учитывать симметрию весов относительно нуля и явно накладывать ее далее авторы квантованную модель на первой стадии сжатая модель пытается воспроизвести выход и промежуточные активации учителя авторы отмечают что важно учитывать используемый при инференсе распределение шагов сэмплирования смещено в область где ошибка квантизации больше поздние шаги диффузии на второй стадии модель учится на noise prediction как самая обычная диффузионка берут sd v15 модель и учат 20к шагов на первой стадии и 50к на второй на некотором проприетарном датасете замеряют clip score и fid на mscoco и пользовательские предпочтения на partiprompts сжатая модель после второй стадии по метрикам несжатой на sbs sidebyside сравнении на parti prompts bitsfusion модель побеждает sdv15 с win rate против странно что sbs который самый показательный учитывая несовершенство текущих генеративных метрик скромно запрятан в приложение в ablation показывают что болееменее все компоненты метода важны смешанная точность дистилляция признаков сэмплирование шагов и двустадийное обучение довольно хороший инженерный результат использующий сочетание разных идей разве что без специализиованного железа вряд ли удастя выжать ускорение однако вызывает почему была выбрана sd v15 хотя статья свежая и уже почти как год существует sdxl можно ли их так же легко сжать полагаю что хорошее качество еще во многом обусловлено тем фактом что загадочный проприетарный датасет неплохо отфильтрованных и дообучение несжатой модели могло бы ее тоже улучшить ибо sd v15 училась на довольно шумных данных из laion
226,2024-06-12,оценивать интеллект по iqтестам это как оценивать качество изображений по fid
227,2024-06-12,про релиз medium обсуждение на по всей видимости выложили pretrain чекпоинт до всяких rlфайнтьюнов
228,2024-06-12,в процессе случайного блуждания по интернетам наткнулся на занятный от некоего аспиранта в калтехе по специальности математика в частности у него есть весьма годные посты по разным вопросам из линейной алгебры разреженным и низкоранговым матрицам теории вероятностей марковские цепи случайные алгоритмы в линейной алгебре есть концептуальные посты и более специфичные итон первый автор статьи sota алгоритма стохастической оценки следов больших матрицы сходящихся быстрее к истинному значению по количеству сэмплов в общем любителям прикладной математики рекомендую
229,2024-06-14,пока все находятся в томном ожидании релиза 3 400b сделали ход конем и выкатили семейство здоровенных херовин знакомьтесь nemotron4 идет в трех комплектациях 1 претрейн 2 дообучение на инструкциях 3 reward model для rl alignment обученная поверх base предобучалось оно на токенах из человеческих языков и языков программирования для alignment использовали набор из 20к инструкций который выложили в контекст коротковат по современным меркам всего токенов небось больше не лезло во время обучения скоры на бенчах весьма достойны на свежей от lmsys уступают лишь свежим версиям чатгопоты и клод опус на lmsys arena было бы еще интересно глянуть elo score ну и самая интересная деталь размер параметров те 640gb на одни лишь веса в fp16 как вы ее будете инферить это ваша проблема задача экстремальной квантизации уже не вместить llmку именно на consumergrade gpu а хоть на какуюто но зеленым конечно спасибо за такой вклад в опенсорс
230,2024-06-17,бог создал слабые и сильные методы сжатия но файтьюн уравнял их
231,2024-06-17,данный пост был в замыслах уже пару месяцев но все никак не доходили руки ниже приведен разбор нескольких статей посвященных одной и той же теме так как задача и мотивация у всех статей общая то и введение дедуплицированное общее для всех работ при структурированном прунинге веса отбрасываются не поодиночке а группами поканально поголовно трансформерно поголовно иногда даже весь слой целиком отправляется в царство аида структурированный прунинг хорош тем что явно уменьшает размерности матриц и тензоров в операциях потому дает неплохое ускорение на разнообразном железе и движках увы серьезного сжатия без серьезной просадки в качестве обычно сложно достичь без серьезных вложений в дообучение трансформеры содержат как в attention так и в mlp residual type архитектуры как было замечено еще являются в некотором смысле где конечный выход можно воспринимать как агрегацию знания от нескольких неглубоких экспертов одна голова хорошо две лучше еще больше еще лучше но и иногда и пары специалистов достаточно потому есть основания полагать что прунинг в глубину прореживание целых блоков имеет шанс завестись и сжатая модель будет выдавать адекватную точность
232,2024-06-17,первая статья в хронологическом порядке и с открытым исходным кодом хотим мы значится выбрасывать блоки целиком но какие брать авторы рассматривают 3 критерия 1 среднюю величину весов magntude pruning 2 taylor не свифт а норму градиента на вес кароч 3 изменение перплексии при выкидывании каждого блока по отдельности в первых двух случаях временами выпиливаются первые и последние блоки которые оказываются позарез важными поэтому предлагается убрать их рассмотрения чтобы восстановить качество модели после сжатия навешивают на то что выжило и дообучают на небольшом количестве данных берут llama1 и vicuna7b 13b для калибровки оценки важности блоков берут 10 последовательностей длины 128 из и потом дообучают на alpaca предложенный метод не хуже а то и лучше и структурированный wanda где критерием важности параметра выступает норма веса на норму активации просадка при 20 прореживания заметная но все же довольно умеренная на малых степенях сжатия лучше всего работает критерий по градиенту умножить на вес при запрете на отбрасывание первых четырех и последних двух блоков при больших оценка важности по перплексии файнтьюн с lora неплохо восстанавливает метрики после сжатия особенно с увеличением степени сжатия тем не менее разрыв между исходной моделью и сжатой все еще существенен просадки больше чем при sota 2битной квантизации данных требуется совсем немного для калибровки качество существенно не меняется если взять больше чем 10 примеров запрещать сжимать для taylor и magnitude первые и последние блоки важно иначе перплексия взлетает до нескольких тысяч метод прост и логичен однако просадка на самом деле несколько больше чем декларируется на собственных замерах с l поверх дообученных чекпоинтов 26 блоков против исходных 32 замерил просадку в 3 по сравнению с исходной моделью
233,2024-06-18,нет кода трансформерные блоки как известно обладают аддитивными skip connections следующего вида для простоты изложения забьем на нормализацию выход mlp attn нередко инициализируется нулем и остается небольшим по норме в процессе обучения следовательно каждый блок меняет понемножку внутренне представление в данной работе предлагают выбрасывать те блоки которые изменяют меньше всего внутреннее представление на некоторой выборке примерах из pg19 в качестве расстояния используют среднее косинусное расстояние между токенами перед и после данного блока в итоге убираются блоки с наименьшим косинусным расстоянием так как работа от китайских коллег то замеряют качество как на llama2 так и на baichuan для оценки качества используют стандарнтые 0шоты mmlu и cmmlu предложенная метрика выбирает блоки ближе к концу сети но не самые последние результаты бенчмарков вызывают вопросы утверждается что качество на mmlu почти не просаживается вплоть до 28 sparsity при этом перплексия на wikitext2 возрастает довольно заметно при таких значениях обычно сеть с трудом два слова связать может не то что решать задачи на логику нигде не цитируется по всей видимости используют какойто иной или самописный фреймворк безлайны методы структурированного прунинга в ширину естественно бьют с shortenedllama вышедшей чуть раньше не сравниваются
234,2024-06-18,нет кода идея по существу та же что и выше но выбрасывают блоки не по отдельности а пачкой те находим группу последовательных трансформерных блоков которые минимально меняют представление критерий тот же самый косинусная близость как и в shortenedllama для восстановления качества сжатой модели дообучают с lora и дообучение называется лечением healing замеряют качество языкового моделирование вместо перплексии смотрят на изменение кроссэнтропии и mmluboolq рассматривают llama2 mistral phi2 кросс энтропия просаживается но неплохо лечится после файнтьюна результаты на mmlu вызывают большие сомнения спад качества имеет скачкообразный характер по личным наблюдениям спад mmlu довольно монотонный постепенный и коррелировал с ростом перплексии утверждается что некоторые модели можно сжать до 40 без просадки на этом бенчмарке далее смотрят на корреляцию признаков в разных блоках наибольшая корреляция наименьшее расстояние в блоках ближе к концу согласуясь с наблюдениями из двух прошлых работ отсюда предлагают простое правило пруньте блоки с конца исключая последний и типа хорошо работает
235,2024-06-18,интересное релевантное исследование от коллег из airiсколтеха кратко ибо все хорошо и доступно изложено самим автором в постах в телеге и на хабре оказывается что активации трансформера в соседних блоках связаны почти что линейным преобразованием в качестве меры линейности используется linearity score который по существу является коэффициентом детерминации сам по себе факт не столь удивителен ибо норма обыкновенно значительно больше преобразования в attnmlp но даже если вычесть skip connection выделяется явная линейная и нелинейная компонента проанализировав ряд промежуточных чекпоинтов открытых моделей авторы замечают что норма линейной компоненты уменьшается в процессе предобучения но возрастает в ходе файнтьюна и предложенная регуляризация на нелинейность улучшает качество исходя из наблюдений предлагается прунить самые слои а для восстановления качества заместо запруненного блока учится линейный слой и такой адаптер неплохо восстанавливает качество рассматривают большое число открытых моделей от gpt2 когдато openai был действительно открытым bloom до свежих phi3 рост линейности имеет место для всех моделей и задач прирост качества от регуляризации хоть и невелик но все же заметен интересно насколько он будет иметь место на большом масштабе несколько нетипичный сетап замера перплексии короткие последовательности вместо обычно используемого 2к4к контекста как на обучении но общая суть от сего не меняется
236,2024-06-18,трансформеры вероятно являясь в какомто смысле ансамблем устойчивы как печень к ампутации крупных кусков однако просадки там где результатам можно доверять первая и последняя работа все же значительны при степени сжатия более 20 как ни крути scaling laws существуют не просто так и представления выученные разными блоками хоть имеют и разную полезность но все же содержат в себе релевантное знание хоть для какогото входа отсюда и успех контекстуальных методов сжатия вроде и
237,2024-06-23,т было очевидно что после выхода обновленного и что ответочка от конкурентов из cornell university это лишь вопрос времени тем более что в первый автор дал ясно понять что новый более совершенный метод квантизации скоро выйдет на свет и тихо незаметно так что сам я обнаружил лишь по чистой случайности на архиве появилась статья и классическая репа с ридмишкой квантизация при той же битности точнее если реализована не криворуко чем скалярная раньше я думал что данное явление обусловлено некоторыми корреляциями между измерениями но на самом деле векторная квантизация работает точнее даже для последовательностей iid хорошее обьяснение чем больше группа тем более высокой точности при заданном числе бит можно добиться потенциально однако большие кодбуки становятся довольно обьемными и вносят нетривиальный вклад в общую битность в aqlm 1x16 кодбуки накидывают 03 дополнительных бит на параметр и не влезают в кэш gpu в идеале бы хотелось иметь компактный кодбук и большие группы и авторы названного в честь прибегают к красивой идее из теории кодирования trellis coding обычно при кодировке последовательностей в бит каждый элемент может принимать любое из значений и последовательность длины кодируется tl битами если представить последовательность в виде блуждания по графу то граф полносвязный из любой вершины можно прийти в любую другую в названном в честь каждый узел графа соответствующий одному из значений соединен только с где ребрами с другими вершинами последовательность кодируется как индекс начального узла и индексы ребер при блуждании по графу итого расход памяти что может быть заметно меньше однако наивная реализация будет чрезмерно тяжеловесной для произвольного trellis при достаточно большом и так как гдето надо хранить граф связей кроме того декодирование для произвольного trellis много последовательных операций что непозволительно медленно на практике и qtip предлагает несколько хитрых идей чтобы это работало эффективно как и в quipquip квантуется не исходный вес а обработанный адамаровыми матрицами так что элементы матриц близки к iid нормальному распределению далее чтобы не хранить полный произвольный граф авторы рассматривают специальный анзатц bitshift trellis где связаны только вершины с индексами отличающимися на битовый сдвиг однако такой выбор слишком ограничителен и авторы предлагают перемешивать связи некоторой хитрой случайной перестановкой и предлагают три метода 1 1mad 2 3inst 3 hyb в первых двух случаях в качестве индексов в случайной перестановке выступает некоторое фиксированное преобразование использующее случайных чисел и побитовые операции а в третьем случае некоторое для определения оптимальной конфигурации минимизируется ошибка на выходе слоя с помощью динамическое программирование полученный метод выдает mse ошибку близкую к минимально
238,2024-06-23,метод валидируют на 2 3 и в качестве бейзлайнов выступает quip и aqlm первой версии с pvtuning шельмецы хоть и лежит он на архиве 3 недели с их публикации квантуют группами число весов по входной и выходной размерности с l16 и q9 гиперпараметр в обучаемом гибридном коде разные битности отличаются числом ребер k из каждой вершины метод демонстрирует выдающееся качество даже без какогото дообучения в 4 бита почти а в 2 бита почти на уровне aqlm и quip c endtoend finetuning жулики в ft репортят цифры aqlm только с блочным файтьюном с полноценным файнтьюном дообучение знаков в адамаровых матрицах и параметров hyb преобразования метод вырывается далеко вперед при низких битностях от quip неупомянутый pvtuning работает несколько лучше на меньших моделях 78b 13b и сравнивается на 70b используя правда больше данных и вычислений на 3 просадки больше данное семейство моделей согласно наблюдениям извне и личному опыту тяжелее квантизуются скорость инференса замеряют на rtx4090 на уровне quip сравнение с aqlm полная шляпа чуваки небось коряво поставили либу для инференса либо что еще наворотили ибо в нашей статье на более медленной rtx3090 куда больше несмотря на корявое сравнение с aqlm в общем и целом конкуренты проделали отличную работу здорово когда красивые идеи из математики находят применение на практике кажется что стоит реально засесть за ботанье матана и чтение статей из 80х
239,2024-06-27,торч недавно выкатил блог где показывает что 24 sparsity паттерн может быть полезен не только для инференса но и на стадии обучения модели напомню что sparsity это когда из 4 подряд идущих весов 2нулевые начиная с ampere нвидиевские gpu имеют поддержку такого формата веса хранятся в виде сжатого вдвое тензора и еще есть тензор индексов элементов было показано что такой паттерн выдает ускорение 1030 по сравнению с fp16 на инференсе но чтобы добиться эффективного обучения необходимо учесть еще ряд нюансов обучение с фиксированной маской работает не очень и нужно ее обновлять по ходу дела чтобы не просесть по качеству наивный kernel слишком и по порядку величины близок к времени forward пасса и заслуга авторов в разработке эффективной процедуры прореживания 1 на обратном проходе при подсчете градиента участвует не w wt которая вообще говоря не 24 авторы ограничивают паттерн прореживания так что 24 паттерн имеет место как для w так и для wt прореживая блоки 4x4 еще есть нюанс в том что на уровне железа реализовано умножение sparse матриц на dense но не наоборот потому предлагается считать транспонированное умножение а затем его транспонировать в cusparselt есть опция выбирать индексирование в памяти как по колонкам так и по столбцам что позволяет иметь непрерывное расположение элементов в обоих случаях 2 потоки cuda считывают за раз по 128 байт и чтобы отпимизировать операции чтениязаписи каждый поток берет на себя по 4 блока 4x4 4x4x4x2 fp16 128 байт 3 ifelse операции когда разные потоки в группе warp попадают в разные условные ветви работают сильно неэффективно на gpu используется для определения важныхневажных элементов без условных операторов 4 сжатые матрицы и метаданные индексы ненулевых элементов хранятся в columnmajor для оптимизации операций записи обучают vitl c разными пропорциями sparsedense обучения сначала идет sparse обучение а затем dense для оценки качества смотрят на точность логрегрессии на imagenet1k поверх признаков обученной модели обучение в sparse режиме на протяжении обучения не просаживается по качеству по сути по сравнению с dense обучением всегда sparse теряет полпроцента качества ускорение времени обучения при sparse обучения порядка на а100 24 паттерн не дает какогото впечатляющего ускорения но может слегка повысить эффективность интересно насколько полученные результаты масштабируются на llmки с миллиардами параметров будет ли выигрыш от 24 компенсироваться более медленной сходимостью
240,2024-06-27,cтатьи нет в ряде отечественных тг каналов упомянули проект самой папиры еще нет и потому какието нюансы могут быть неизвестны от себя добавлю несколько деталей суть подхода заключается в следующем в видео диффузии есть 3 вида attention операций 1 пространственное 2 временное 3 перекрестное внимание на condition ранее в было замечено что карты attention между соседними шагами диффузии мало меняются между соседними шагами на большей части процесса за исключением начала и конца при картиночной генерации в данном проекте авторы замечают что для скорости изменения attention карт справедливо следующее неравенство и соотвественно чем меняется attention тем он переиспользуется за счет переиспользования карт attention можно параллелить эффективно между разными gpu разные шаги генерации по времени уменьшении оверхеда на 50 без переиспользования карт переиспользование карт дает ускорение а распаралелливание на дает почти линейное ускорение по сравнению с генерацией на наивным способом то есть ускорение достигается в первую очередь за счет тем не менее достойный инженерный результат
241,2024-06-29,некоторое время назад я затрагивал быстрый кернел для батчового инференса пару месяцев назад коллеги из ist выпустили где дополнительно к квантизации весов добавляется 24 sparsity за счет чего достигается еще большее ускорение по сравнению с fp16 как я понял основные инженерные наработки следующие 1 эффективная обработка 24 метаданных позиций нулевых и ненулевых весов 2 использование sparse tensor cores sptcs 3 умное расположение квантованных весов метаданных для sparsity и статистик квантования итоговое ускорение до 53x на rtx3090 у marlin 4x при перемножении больших матриц интересно какое end2end ускорение может быть достигнуто на эффективном движке для инференса типа vllm на hopper и более новые архитектуры пока не завезли таким образом 24 может давать определенный выигрыш по скорости основная проблема в том что на текущий момент 24 прунинг posttraining сильно просаживает качество llm
242,2024-07-04,кода нет подбор шага обучения занятие довольно потное и утомительное learning rate хороший для меньших моделей зачастую приводит к расходимости для больших потому большие модели обычно обучают с меньшим learning rate но подбор по сетке или с помощью байесовской оптимизации может оказаться слишком накладным на практике а иметь хорошие значения гиперпараметров всетаки хочется в предложили параметризацию и μp такую что оптимальные гиперпараметры обучения переносятся с маленьких моделей на большие динамика обучения в разреженных сетях существенно отличается от плотных потому естественно предположить что обобщаемая параметризация на разные степени прореживания потребует дополнительных телодвижений и в данной статье выводят такую параметризацию параметризация при которой learning rate и прочие гиперпараметры обучения будут переноситься на разные размеры моделей и степени прореживания должна удовлетворять следующим свойствам 1 нормы весов не зависят от ширины и sparsity 2 нормы градиентов по весам не зависят от ширины и sparsity 3 изменения весов для произвольного алгоритма оптимизации не зависят от ширины и sparsity из нехитрой математики следует что variance весов при иниациализации и learning rate следует масштабировать как чем сеть тем е разброс параметров и learning rate чем тем наоборот и learning rate при sparsity 0 предложенная sμpar вырождается в μp метод валидируют на lmках c swiglu и alibi обучаемых на токенах из slim pajama в качестве маленькой прокси модели для тюнинга гиперпараметрво берут lmку с 40m параметрами а основную серию экспериментов проводят на 610m модели замеряют loss на обучении датасет большой потому переобучения нет при стандартной параметризации и μp оптимальный learning rate приходится подбирать для каждой степени сжатия свой в то время как для sμpar оптимальные learning rate зафиксирован перенесенные с меньшей модели параметры на большую для разных степеней прореживания позволяют достичь лучшего качества по сравнению с попытками затюнить стандартную параметризацию и μp авторы ребята из cerebras потому умеют вполне эффективно эксплуатировать разреженные операции полезная идея при обучении разреженных сетей ранее в было показано что при очень большом числе данных sparse модели более compute оптимальны чем плотные если в будущем будут активно учить llmки с использованием специализированного железа sμpar будет весьма кстати для подбора гиперпараметров
243,2024-07-09,без кода chita это гепард а не административный центр забайкальского края статья 2023 года от google research про прунинг optimal surgeon использующий приближение 2 порядка для определения оптимальной сжатой конфигурации лежит в основе многих методов прунинга и квантизации алгоритм iht в котором на каждом шаге делается шаг оптимизации и прунинг самых маленьких весов лежит в основе некоторых методов в частности в данной статье решили поженить и и назвали полученную сущность combinatorial hessianfree iterative thresholding algorithm основная проблема с гессианом нейронной сети что его хрен посчитаешь честно существуют различные приближения в данной статье опираются на фишеровское оценка гессиана сумой внешних произведений градиентов обычно градиентов много меньше чем параметров потому полученная матрица низкоранговая можно ее даже не материализовывать так как в итоговом алгоритме потребуются только матричновекторные произведения а хранить лишь градиентов в матрице фишеровская матрица выражается как chita на каждом шаге делает шаг градиентного спуска для квадратичного разложения в окрестности оптимума те с последующим прореживанием как в iht метод требует времени и памяти по числу параметров и сохраненных градиентов оптимальный шаг оптимизации learning rate в iht находят с помощью умного алгоритма поиска на прямой метод валидируют на небольших cnnках на и t метод быстрее и при этом достигает лучшего качества однако честность сравнения в oneshot pruning учитывая что метод итеративный в отличие от вызывает вопросы метод можно применять итеративно постепенно повышая степень сжатия и дообучая сеть с фиксированной маской лучше повышать линейно уровень прореживания линейно чем экспоненциально или за раз пытаться сжать все при итеративном сжатии удается добиться просадки на при cжатии и просадки при в этом режиме почемуто забывают в сравнениях про mfac который достигает близких результатов по качеству метод довольно неплох и на resnet50 с умеренными просадками при 9095 sparsity метод неплох и не слишком сложен в реализации основная проблема необходимость хранить много градиентов что не позволяет масштабировать метод на llmки и интересные в 2024 году модели у вашего покорного слуги есть одна идейка как это обойти но это пока секрет
244,2024-07-11,вышел не с выхода flash attention 2 как вышло продолжение доведя серию до трилогии основная идея первого в уменьшении между hbm и кэшом gpu а второго в отпимизации не matmul операций позволяет довольно эффективно использовать вычислительные возможности a100 но для более современной h100 все еще имеет место сильная недоутилизация всего 35 в flash attention 3 отпимизируют attention с учетом новых архитектурных особенностей hopper новых тензорных ядер с большим throughput тензорным ускорителем памяти tensor memory accelerator и поддержкой fp8 основных источника ускорения два 1 паралеллизация вычислений между gemm general matrix multiply и иными операциями softmax rmsnorm хоть по flops softmax и нормализации кажутся мизерными они выполняются в 256 раз медленее на h100 потому могут занимать до половины времени матричного умножения за их выполнение отвечают другие компоненты gpu чем тензорные ядра для перемножения матриц потому их можно обрабатывать параллельно 2 использование fp8 fp8 позволяет почти удвоить скорость однако есть нюанс выбросы приводящие к большим ошибкам квантования чтобы избавиться от них используют incoherence processing из благодаря этому ошибка квантизации уменьшается до 26 раз с fp16 удается достичь ускорения до 1618 раз по сравнению с flash attention 2 и под 12 pflops c fp8 на больших последовательностях и с большими головами трансформера обещают в будущем накатить в торч сильно ждем через год flash attention 4
245,2024-07-19,наряду с чисто языковыми моделями в последнее время интенсивно развиваются и модели умеющие работать с текстовой и визуальной информацией одновременно флагманские проприетарные модели неплохо умеют решать нетривиальные задачи связанные с обработкой визуальных данных понимание содержимого сцены нахождение нужного объекта извлечение информации из графиков и табличек и самое главное понимание мемов однако данные задачи еще более менее имеют формулировку с точки зрения естественного языка кроме того имеет место утечка в процессе обучения и авторы задаются вопросом причем те что под силу даже детсадовцу и оказывается что несмотря на простоту рассматриваемых задач нынешняя sota не то чтобы блестяще справляется с ними и gpt4o не всегда лучшая предложенный бенчмарк состоит из 1 подсчет числа пересечений двух графиков из 3 точек от 0 до 3 2 определение того пересекаются ли 2 круга или нет 3 определение буквы на которую наложили круг 4 подсчет числа пересекающихся геометрических примитивов 5 подсчет количества вложенных квадратов 6 определение числа строк и столбцов в таблице 7 умение следовать за разноцветными ломаными сколько путей следует из одной вершины в другую путей от 0 до 3 рассматривают 4 прориетарные vlm vision language model модели 1 gpt4o 2 gemini15 pro 3 claude sonnet 3 4 claude sonnet 35 несмотря на простоту перечисленных выше задач модели справляются с ними далеко не идеально в частности в задаче определения числа пересечений gpt4o достигает качества 48 а лучшая из моделей claude35 sonnet 77 что далеко от 100 чем графики и чем линии тем ошибка модели хорошо понимают что такое круг и умеют читать текст без ошибок но тем не менее определение того какую же всетаки букву закрыл кружочек оказывается не такой уж простой задачей и здесь лидирует gemini15 pro со средней точностью около 93 vlmки умеют хорошо считать разнесенные в пространстве объекты но как только они пересекаются или оказываются вложенными качество заметно проседает и здесь снова побеждает в claude35 sonnet такая элементарная задача как подсчет числа столбцов и строк тоже дается нелегко при том что перечисленные сетки умеют решать куда казалось бы более сложные задачи про обработку табличных данных умница claude35 sonnet выдает точность в среднем 7426 в то время как остальные модели между 35 и 40 если в ячейках есть текст точность немного выше определение числа путей по картинке тоже дается нелегко claude35 sonnet не подкупили antropic авторов снова побеждает с точностью в среднем 50 если путь один то claude35 с большим отрывом точнее всех на большом числе путей 3 gpt4o лидирует любопытное исследование хоть результаты и во многом ожидаемы текущие модели основаны преимущественно на достижениях в nlp способности llm улавливать сложные закономерности в последовательностях но задачи опирающиеся сугубо на зрительную информацию по всей видимости требуют от модели обучения с упором на геометрию выявления пространственных закономерностей вопрос в том насколько это важно в прикладных задачах тем не менее разработка такой процедуры обучения может стать следующим шагом в развитии vlm
246,2024-07-20,современные нейронные сети обрабатывающие данные различной природы будь то текст изображения аудио и видео так или иначе обладают механизмом перемешивания каналов обрабатывающим независимо признаки для каждого элемента последовательности и механизмом обработки последовательности использования взаимосвязей между элементами в сегодняшнем рассказе речь пойдет про существуют разнообразные опции sequence mixing операция смешивания может от входа как например свертка или обучаемая матрица l x l l длина последовательности в и statespace модели или зависеть механизм в трансформерах или selective state spaces кроме того разные механизмы обладают от длины последовательности sequence mixing в attention или mlpmixer требует по длине последовательности числа элементарных операций с плавающей точкой flops так как используют матричную операцию довольно общего вида sequence mixers обладающие некоторой низкоранговые матрицы позволяют добиваться субквадратичной сложности обычно с некоторой просадкой в качестве и sequence mixing может быть как causal attention большинство ssm в частности модная нынче где текущий элемент последовательности может смотреть только в прошлое и как в masked language modelling и большинстве задач с vitами где элементы последовательности могут изменять свое состояние как глядя как на прошлые так и на будущие токены и задача которую перед собой ставят авторы в данной работе получение механизма двунаправленного sequence mixing такого чтобы он был с одной стороны субквадратичным в идеале линейным по длине последовательности и в то же время
247,2024-07-20,общий фреймворк выглядит следующим образом функция преобразования данных входные данные функция конструкции смешивающей матрицы которая может быть постоянной или зависеть от входов результат sequence mixing имеет вид далее авторы вводят термин sam еще один означающий что матрица смешивания зависит от входных данных такие sequece миксеры хороши с одной стороны тем что более адаптивно подстраиваются под входы и кроме того работают с последовательностями разной длины авторы рассматривают разные механизмы из литературы mlpmixer s4 h3 monarch vandermonde и cauchy миксеры не attention linear attention s6 потому хороший двунаправленный sequence mixer должен быть и представлять собой некоторую матрицу в частности предлагаются способы сделать vandermonde и cauchy миксеры зависящими от входов но основной упор делается на прокачку не твердотельного жесткого диска а механизма в под двунаправленность напомним что во второй мамбе является полуразложимыми semiseparable матрицей каждый блок является матрицей для двунаправленности можно было бы чередовать слои ssd один бегущий слева направо другой справа налево но здесь предлагают использование одной матрицы смешивания такой что любой ее блок в верхнетреугольной и нижнетреугольной части является низкоранговой матрицей иначе говоря получается нечто типа суммы исходной ssd из mamba2 нижнетреугольной матрицы и транспонированной по длине последовательности верхнетреугольной матрицы и диагональной части такие матрицы называют qs данная модификация требует всего пары дополнительных строчек в реализации по сравнению с исходным ssd слоем сдвиг на один элемент разворот последовательности задом наперед dx диагональная добавка называют гидрой потому что как в ssd и звучит красиво метод валидируют на задаче где в качестве бейзлайнов берется bert обученный по рецепту от mosaicml и иные варианты sequence mixerов из литературы для оценки качества моделей смотрят на валидационную кроссэнтропию на на train set которого обучают и точность на бенчах из все модели имеют размер порядка 70m параметров несколько меньше чем bertbase так что хрен вам sota на lmsys hydra модели трансформеров примерно в 2 раза при примерно том же числе параметров модели стабильно опережают свои не версии toeplitz cauchy vandermonde с параметрами зависящими от входа заметно точнее версии с обучаемыми не зависящими hydra естесна лучше всех и на втором месте любимый нами трансформер однако памятуя о недавнем результате где более глубокие и тонкие трансформеры оказываются лучше по качеству более коротких и жирных при том же числе параметров задаешься вопросом можно ли устранить разрыв в качестве за счет изменения конфигурации трансформера исходная mamba2 не очень сильна в mlm так как умеет обрабатывать информацию только в одном направлении и предложенный способ hydra лучше вариантов с суммой конкатенацией и перемножением результатов двух мамб2 далее метод проверяют на задаче классификации imagenet1k где обучают модели размера порядка vitbase 8791m параметров и hydra опережает vitb hyena и s4 однако вызывает вопросы ибо согласно их результатам vitb имеет top1 точность 788 а его ema 806 в то время как c рецептом обучения из на который они ссылаются унаследованный в свою очередь из deit выдает их лучший результат 816
248,2024-07-20,данная статья полезна с формальной точки зрения ибо дает некоторый взгляд на современные варианты нейросетевых архитектур обрабатывающие последовательности и немножечко вкусненькой математики результаты экспериментов вызывают некоторые вопросы да и валидация на небольшом ограниченном сетапе как мне кажется пока преждевременно говорить об убийце трансформеров тем не менее интересно и занимательно
249,2024-07-21,вот мой скромный канал преодолел рубеж в 1000 подписчиков когда я создавал его я и не мечтал о таком количестве думал что это будет глубоко андерграундная гаражная телеграмканава на пару десятков заинтересованных в силу специфики тематики и моей лени в отношении заморочек с pr но несколько упоминаний на знаменитых и влиятельных отечественных телегах про dl и связанные темы за что огромное спасибо открыли мои безделицы широкой аудитории всем спасибо за то что вы здесь мне очень приятно видеть ваши реакции комментарии к постам желание опубликовать разбор очередной статьи наверное лучший стимул поглубже в ней разобраться будем дальше стараться быть добру
250,2024-07-23,31 стала доступна широкой публике на к версиям 8 и 70b добавилась 405b из основных фичей 1 мультиязычность english german french italian portuguese hindi spanish and thai 2 128k окно контекста 3 обрезка знаний по декабрь 2023 4 умеет с тулами работать обучалось на 15t неизвестно насколько токенов из публичных както собранных данных базовые меньшие модели иногда кста просаживаются по сравнению с v3 по метрикам но instruct стабильно лучше большая модель вполне себе бодается с gpt4omni и claude35sonnet посмотрим что выдаст lmsys арена на хабе есть и fp8 версии
251,2024-07-23,боль
252,2024-07-24,кванты llama31 уже появились на на текущий момент выложены 1 awqint4 gptqint4 квантизации llama31405binstruct 2 bnbnf4 квантизация llama31405b 2 awqint4 квантизация llama3170b 4 awqint4 bnbnf4 квантизации llama318binstruct судя по скорости выкладывания для калибровки моделей использовалось довольно мало данных поэтому я бы ожидал значительной просадки в качестве по сравнению с исходной моделью int4 требует 203 gb vram для 405b модели а bnbnf4 220 gb
253,2024-07-30,совместно с регулярно проводят так называемые где рассказывают о новых фичах в релизах vllm и проводят туториалы по пользованию своими либами сжатию и ускорению моделек в частности заслуживают внимания 1 выступает мой соавтор eldar kurtic 2 в общем рекомендую
254,2024-08-01,на свет появился еще один стартап про генеративное ии помните из облы чуваки привлекли инвестиций и в дело включились такие серьезные люди как timo aila и vladlen koltun команда выпустила семейство моделей из трех моделей 1 проприетарной pro доступной через их api и на 2 открытой dev с 12b параметрами 3 открытой schell тоже с 12b параметрами но нацеленную на генерацию в малое число шагов 14 по всей видимости дистиллированную из dev c помощью утверждают что по elo score бьют opensource и closedsource модели типа mjv6 sd3ultra ideogram сами модели по себе представляют ditы обученные на модель умеет генерировать изображения разного разрешения с разными aspect ratio техрепорт обещают выложить в будущем на 1 2
255,2024-08-05,после долгой борьбы с инфраструктурой богами хаоса и разрушения удалосьтаки квантизовать ом и дообучить алгоритмом меньшие версии 31 1 2 3 4 70b в процессе самая интересная из безусловно 405b но пока развлекаемся с тем как поместить сие жирное чудище на машину
256,2024-08-05,мы настолько тормозили что instruct 2битную модель люди из community выложили раньше нас любопытно откуда такое разночтение в метриках для исходной модели если конкретно по нашим замерам неквантизованная llama31instruct выдает 6817 точности в среднем на mmlu а квантизованная aqlm в 2 бита и зафайнтьюенная нами pv модель 5917 то есть просадка у нашей модели меньше но и бейзлайн выше
257,2024-08-12,в webui накатили квантизованные версии недавно нашумевшей 1bnb nf4 4 c небольшим бит на параметро 2fp8 если считать fp8 квантизацией в зависимости от железа рассматривается случай gpu от ampere и новее и версий bnb nf4 быстрее от 13 до 4 раз так как модель довольно здоровая 12b параметров то трансфер памяти по всей видимости играет существенную роль кроме того после недавних обновлений bitsnandbytes 042043 скорость матричных операций сильно выросла кроме того утверждается что ошибка квантизации nf4 даже меньше благодаря более адаптивной квантильной квантизации чем у fp8 e4m3fne5m2 далее пост содержит обсуждение хаков как эффективно запускать flux на винде и примеры генераций
258,2024-08-12,кроме того в свежем отличная работа в bitsandbytes добавили классы в современных llmках идет тренд на рост размера токенизатора и пост на показывает что это даже положительно влияет на качество вдобавок к уменьшию длин последовательностей для экстремально сжатых моделей в 1 и 2 бита эмбеды и занимают значительную долю в суммарном обьеме памяти на хранение модели в недавней показано что эмбеды можно сжимать в 4 бита простой bnb квантизацией без просадки а l в 8 бит в 4 с небольшой просадкой данное обновление весьма полезно для дальнейшего движения в сторону экстремального сжатия и посадки llmок на мобилки
259,2024-08-15,недавно ребята из провайдера инференса llmок выпустили занятный блог про то как следует оценивать сжатых моделей главные выводы и предписания следующие 1 tradeoff между степенью сжатия и качеством определяется конечным приложением не существует универсального предписания для всех задач что мол во столько раз можно сжать с умеренной просадкой в качестве а дальше нельзя 2 использование для оценки степени расхождения квантизованной модели от исходной 3 не полагаться на точность на стандартных бенчмарках а диверсифицировать протокол замеров по первому пункту вывод довольно логичен некоторые задачи менее чувствительны к ошибкам чем другие где один косячок и весь дальнейший вывод сыпется как домино перплексия тоже плоха так как ниже у моделей с низкой энтропией но не всегда более точных ссылаясь на свежую работу от microsoft авторы заявляют что точность на бенчах оценки качества может быть так что исходная модель гдето ошибалась а квантизованная случайно стала предсказывать нужный токен и прирост качества скорее всего мизерный обусловлен случайностью а не тем что модель стала лучше между сжатой и несжатой моделью предлагается в качестве альтернативы точности на наборе задач как устойчивая к шумам если конкретно предлагается генерировать несжатой моделью forced generation и смотреть насколько различаются вероятности следующего токена предсказанные fp моделью и квантизованной кроме того еще можно смотреть на насколько отличается выбор top n токенов в дальнейшем между двумя моделями ниже под квантизацией подразумевается переход от fp16 к fp8 братве явно завезли баржу с h100 рассматривают 4 уровня квантизации по возрастанию степени сжатия 1 квантизуем только mlp без первого и последнего блока трансформера 2 квантизуем все веса 3 квантизуем вдобавок еще kvcache 4 квантизуем attention k v на prefill точность на просаживается немонотонно уровень 2 даже оказывается лучше fp16 на тютельку же монотонно растет со сжатием далее авторы смотрят еще на ряд задачек из gsm8k naturalquestions wmt2014 и другие сравниваясь с другим известным провайдером что fireworks что together выдают качество близкое к fp16 при инференсе в fp8 а то и лучше некоторых задачах но fireworks якобы чуть лучше однако радоваться рано говорят авторы из нужен замер на чемто как можно более похожем на людские предпочтения на томже lmsys alpacaeval слишком маленький и не использует современные методы промптинга потому предлагают смотреть на от создателей s fp8 модели от fireworks и together будто бы чуть хуже fp16 на 31405b но не статзначимо вопрос оценки качества llmок довольно нетривиален ввиду того что современные модели умеют решать широкий круг задач и сложно объять всю полноту возможных задач и областей знания академические бенчмарки из нескольких задач и замеров перплексии на паре датасетов хороши для статей здесь и ваш покорный слуга согрешил но не всегда удовлетворительны для реальных приложений и запросов пользователя потому следует полагаться не на чужие обещания а самому всю тщательно прощупывать
260,2024-08-15,как сделать text2image модель которая по sbs бьет midjourney 1 учишь модель которая хоть иногда лучше midjourney 2 собираешь большой набор промптов 3 прогоняешь sidebyside comparison 4 отбираешь промпты где ваша модель бьет midjourney 5 утверждаешь что на внутреннем бенче добился победы над mj 6 stonks
261,2024-08-16,многие современные методы сжатия моделей что квантизация что прунинг оптимизируют некоторую меру ошибки на репрезентативной выборке данных интуитивно понятно что эта выборка есть приближение целевого поэтому желательно чтобы этой самый калибровочный датасет как можно точнее и полнее приближал его на текущий момент в сообществе не сложилось четких правил и предписаний по сбору калибровочных данных преимущественно исследователи и практики полагаются на то что было предложено ранее в работах или тому что и в этой работе авторы исследуют вопрос влияния калибровочных данных на качество методов сжатия llmок берутся 2 метода квантизации 1 gptq 2 spqr я польщен и 2 метода прунинга 1 sparsegpt 2 wanda модели квантизуют в 4 бита spqr в 45 по факту и прунят в 24 sparsity в качестве калибровочных данных рассматривают следующие источники 1 c4 бессмертная классика 2 cnndm новости длинный текст хорошего качества именно текст не сами новости 3 redpajama 4 refinedweb 5 wikipedia английская отовсюду берут 128 сэмплов длины 2048 как в статье gptq качество замеряют на десяти 0shot бенчах берут 9 моделей 1 llama1 7b 13b 33b 2 vicuna полученные из выше 3 opt 67b 13b 33b
262,2024-08-16,1 точность может сильно различаться между разными подвыборками из одного датасета 2 есть более и менее шумные задачи boolq и rte показывают наибольшую дисперсию 3 одни датасеты лучше других в среднем refinedweb показывает лучшее качество а wikipedia худшее но разброс значителен между моделями и методами и нет четкого ранжирования 4 прунинг сажает качество моделей сильнее потому и влияние данных более заметно по всей видимости разброс для sparsegpt больше ожидаемо так как sparsegpt не только прунит но и обновляет оставшиеся веса 5optы более чувствительны к выбору данных чем парнокопытные модели 6 качество быстро насыщается с количеством калибровочных примеров для всех методов кроме sparsegpt по нашим наблюдениям чтобы выжать максимум из качества пара тысяч последовательностей все же нужна 7 sparsegpt лучше wanda хотя исходная статья утверждала обратное логично ибо wanda есть дефакто кастрированный sparsegpt мораль сей басни такова подбирайте осмысленно полезное исследование но чувствуется что для полноты картины нехватает более сложных задач и других областей математика код как и более свежих моделей
263,2024-08-16,в более свежей кстати утверждают что данные вообще не важны и случайные последовательности работают якобы так же хорошо за исключением opt но кажется что авторы набагали гдето
264,2024-08-17,с этими всеми квантизациями мы стали забывать про прунинг в отличие от довольно бюджетных техник квантизаций дающих значительное сжатие за дешево прунинг тем более структурированный требует серьезного дообучения но если вы не гаражный стартап а компания с триллионной капитализацией вам он под силу подданые кожаной куртки выпустили запруненные версии nemotron15b под названием и которые по качеству якобы конкуретноспособны текущей sota в легком весе у трансформеров можно прунить целые блоки про что был обзор некоторое времчя назад размерность признаков был у вас d_model 768 стал 512 головы в attention внутреннюю размерность mlp первый метод про прунинг в глубину а остальные про ширину для оценки важности используются критерии основанные на активациях средний attention по голове при прунинге головы средняя активация по каналу для mlp средняя активация layernorm для оценки важности признаков для прунинга в глубину смотрят на среднее косинусное расстояние между входом и выходом блока и изменение перплексии при выкидывании блока рассматривают оценку важности oneshot когда выкидываем все за раз или итеративно по чутьчуть далее выбирают некоторое около 20 число конфигураций сжатых моделей с целевой степенью сжатия сжимают модели и дообучают на 18b токенов берется самый перспективный кандидат и дообучается уже на 100b токенов немало но все же на 2 порядка меньше чем у современных llm на дообучении рассматривают сумму следующих лоссов 1 кроссэнтропию исходный лосс 2 klдивергенцию на логитах с несжатой моделью 3 l2 лосс между промежуточными признаками так как сжатая модель может быть уже есть матрица переводящая в размерность учителя
265,2024-08-17,качество замеряют на широком круге бенчей всякие mmlu arc winogrande humaneval и mtbench ifeeval основные выводы следующие 1 сжимать большую модель дешевле чем обучать новую на том же числе токенов хрен вы получите тоже же качество с нуля после дообучения нет разницы между oneshot и итеративным сжатием 2 в ширину сжимать эффективнее чем в глубину блоки прунятся хуже чем каналы 3 базовый лосс не нужен используйтк только дистилляционный 4 итеративная оценка важности не накидывает после дообучения кандидатов 5 если глубина уменьшена дистилляция промежуточных признаков важна иначе хватает только логитов 6 прунить лучше модель как можно ближе к целевой по размеру логично 7 короткое дообучение 18b токенов дает более надежных кандидатов моделей для дальнейшего дообучения 8 если нужна instruct модель лучше запрунить instruct модель чем base и дообучить на инструкциях minitronы выдают качество не хуже других opensourcr моделей того же размера 8b как llama38b и qwen2 4b как phi2 и gemma2it только безлайны поменьше будут конкурентные методы сжатия поверх других моделей сильно уступают структурированный прунинг перспективная стратегия сжатия llm если есть ресурсы сильно не хватает экспериментов с квантизацией комплиментарны ли оба подхода или прунинг усложняет квантизацию
266,2024-08-18,типичный area chair и рецензенты
267,2024-08-19,если в названии статьи встречаются вместе и t это наверняка чтото из области физики но нет это снова про ранее в ряде работ было показано что поворот на сильно облегчает квантизацию и в этой работе ребята из meta предложили развитие идеи дающее лучшее качество на cache квантизации наличие выбросов в отдельных каналах активаций сильно затрудняет квантизацию так как perchannel квантизация обычно не поддерживается в железе ортогональные матрицы размазывают выбросы в весах и активациях между каналами так что все активации имеют примерно один и тот же порядок но конкретный выбор ортогональной матрицы имеет значение авторы замечают что качество от конкретного выбора матриц если сэмплировать их произвольным образом и редко бывает достаточно хороших случайные адамаровы матрицы из quip гораздо лучше но все разброс существенен и нет гарантий на оптимальность потому предлагается искать хорошие матрицы поворота с помощью оптимизации на параметризующих ортогональные матрицы оптимизация представляет собой градиентный спуск на многообразии а именно есть 4 места куда можно вкрутить поворот r1 в residual перед attention и ffn r2 между v и o матрицами в attention r3 в querykey r4 между активацией в ffn первые два поворота можно влить в параметры модели без изменения выходов модели последние же придется считать на каждом прогоне потому предложенная процедура оптимизации применяется для поиска оптимальных r1 r2 матриц а в случаях r3 r4 используют дешевые адамаровы матрицы кои практически не замедляют работу llmки и почти ничего не весят для оптимизации используют небольшой набор из wikitext2 процедура оптимизации матриц занимает час с небольшим на меньших представителях семейства на 8 a100 и полдня для 70b моделей замеряют по классике качество на перплексии и 0шотах рассматривают 3 сценария 1 weight only quantization 2 weight activation quantization 3 weight activation kv cache quantization по качеству операжают бейзлайны ближайший quarot примерно на 1 в среднем в weightonly quantization гдето бага при замерах aqlm и quip ибо эти методы сжимают модели почти без просадки если верить бенчам а не здравому смыслу почти без просадки метод работает как с rtn roundtonearest так и c gptq важны все матрицы но большой прирост дает будто бы добавление r4 квантизация весовактиваций и кэша в 4 бита на текущий момент наверное то что дает самое большое ускорение инференса на современных gpu потому и представляет значительный практический интерес и улучшение качества это хорошо при квантизации активаций и кэша просадки все еще значительные и есть куда улучшаться
268,2024-08-20,занятный бложек про то как можно сделать sparsityinducing regularization увидев слово я триггернулся на адамаровы матрицы но нет это идея в следующем для пар тензоров u и v коэффициент регуляризации определяет степень разреженности получившейся матрицы подход валидируют на табличной задаче с 500 признаками и двумя классами сначала рассматривают логистическую регрессию реализованную в затем в торче потом рассматривают применительно к нейронным сетям групповая регуляризация зануляет целые каналы и имеет меньше дополнительных параметров для неструктурированной маски число параметров удваивается что может быть накладно в эксперименте берется небольшая mlp обучается на известном всем табличникам с некоторым коэффициентом регуляризации исходно было 4500 весов в полученной сетке остается около 600 ненулевых коэффициентов большинство каналов можно взять и вырубить вся кухня имеет довольно прикольную реализацию через модуль см после задания такой параметризации сети можно обучать не задумываясь вашим любимым оптимизатором с weight decay разве что в данном случае логичнее было брать adamw а не adam в общем прикольно но интересует масштабируемость
269,2024-08-24,если гитхаб и гитлаб заблочат телегу с функционалом чатов можно использовать в качестве альтернативы сообщения с файлами коммиты разные чаты разные ветки pr пересылка файлов из одного чата в другой прямо как в старые добрые времена на заре систем контроля версий
270,2024-08-25,из double blind review вы пока полайкайте а я потом шутку придумаю
271,2024-08-30,bibtextidy наткнулся в интернетах на офигенную для наведения красоты в bibtex если вы ссылаетесь на дохренища работ в частности пишете диссер или обзор данный инструмент позволяет убирать дубликаты чтобы overleaf не ругался при компиляции кроме того можно красиво отформатировать все как отсортировать в алфавитном так что любодорого будет глядеть
272,2024-09-01,когда собираешь свои статьи в один диссер
273,2024-09-03,и два понятия которые за последние пару лет стали неразрывно связаны с большими языковыми моделями особенно в контексте обучения и инференса в условии ограниченных ресурсов и в сегодняшнем эфире будет сказ про немного необычное сочетание этих идей в работе от ребят из cколтеха airi и вышки вкратце напомню что суть peft в дообучении небольшого по сравнению с размером исходной модели числа параметров эти параметры могут быть как адаптерами к весами как lora кои можно или нельзя влить в модель а могут быть некоторым подмножеством весов исходной модели второй вариант еще называется в литературе и про него сегодня пойдет речь метод основан на двух основных идеях 1 определении весов которые с одной стороны не квантуются а с другой являются тем самым подножеством обучаемых параметров 2 добавлении шума в процесс обучения наличие выбросов в весах затрудняющая квантизацию известная особенность llm временами оспариваемая и потому для улучшения качества их обычно держат в исходной точности в данном случае следуя llmint8 и quik выбросы определяются как уровне колонок матриц весов входных размерностей в качестве метрики берутся комбинации вида где rho норма весов в колонке а gamma степень ½ 1 ½ d_i это пертурбация весов и в качестве пертурбации рассматривается операция квантования скалярные однородная симметричная выбросы для которых значение данной метрики самое большое не квантуются градиентный спуск с шумом примененным к весам до или после шага градиентного спуска как известно из литературы помогает избегать седел и плохих оптимумов повышает устойчивость в данной работе шум применяется шага sgd итоговый метод выглядит следующим образом рассматриваются 3 стратегии 1 pregiftsw сначала учим с шумом и выбросам типа готовим модель к квантизации а затем квантизуем 2 salient ft квантизуем и дообучаем только выбросы без шума 3 postgiftsw квантизуем и дообучаем выбросы с шумом
274,2024-09-03,эксперименты проводят на llama23 7b 8b 13b дообучают на инструкциях tulu2 и openorca обучение идет 500 шагов те довольно короткое выбросы определяются на основе 500 сэмплов из pile для оценки качества берут среднюю точность на 5 бенчмарках из lmeval есть 2 сценария 1 в качестве бейзлайнов рассматривается файтьюн всей модели и loradora адаптеры с примерно тем же количеством обучаемых параметров непонятно однако какой шаг квантизации для шума в данном сценарии если есть 2 в качестве бейзлайнов берут lora и ste где выбросы обучаются обычным бэкпропом а квантизованные веса через ste я только не понял quik без или с квантованием активаций больно уж грустно смотрится предложенный подход достигает лучшего качества чем адаптеры и как утверждается ведет себя гораздо стабильнее от количества примеров см figure 1 из статьи данный график вызывает вопросы ибо обыкновенно адаптеры демонстрируют довольно стабильную динамику обучения будучи малыми возмущениями весов если только не вкрутить чрезмерно большой шаг обучения при квантизации предложенный метод опережает quik и ste правда есть более свежие и сильные бейзлайны и из ablation study следует что добавление шума во время дообучения накидывает в качестве при квантизации в 2 бита большой разницы нет при разных опциях метрики определения выбросов l работает чуть лучше в среднем pregiftsw немного лучше при 4 битном сжатии но при более аггресивном сжатии уступает альтернативам при квантизации в 2 бита postgiftsw дает лучшее качество интересно и неплохо по результатам сравнение с адаптерами я бы рекомендовал перепроверить не шумят они так на дообучении еще любопытно помогает ли добавление шума при дообучении статистик квантования в методах типа и ptuning без v
275,2024-09-05,вчера наша статейка попала в на про диффузию что большая честь для нашего скромного авторского коллектива из yandex research такто не то чтобы science по существу перенесли aqlm на задачу text2image генерации с помощью диффузионнок с учетом специфики и нюансов диффузионных архитектур плохой из меня бизнесмен проверяли подход на sdxl и sdxlturbo в целом вышло сносно получше скалярной квантизации по качеству c в качестве бейзлайнов в 4 бита удается даже достичь паритета по sidebyside с оригинальной fp16 моделью здесь уместно заметить что sbs куда более репрезентативная и содержательная характеристика чем все эти ваши fidы и clipскоры 3битные модели по метрикам почти не отличаются от fp16 по метрикам но просадка качества налицо не у всех есть толока за пазухой справедливости ради с практической точки зрения пока есть над чем работать имеет место замедление инференса на 50 ибо в отличие от огромных llm sdxlsdxlturbo малипусики c 25b параметрами которые кроме того обрабатывают большие тензоры активаций за раз а не токен за токеном потому вычисления computebound а не memorybound процедура деквантизации начинает сказываться на времени прогона через сеть потому модельки пока не выкладываем нынче модный flux1 выглядит более перспективным кандидатом для прогонки метода там и трансформер с большими матрицами и 12b параметров как руки дойдут попробуем и его посжимать кроме того запилили еще аля credits to
276,2024-09-09,lk99
277,2024-09-12,во на была разобрана статья про то насколько хорошо нынешние llmки умеют в генерацию наукоподобных статей и результат такой вкратце что в некоторых аспектах вездесущей аморфной плохоопределенной научной новизны по мнению ассесоров статзначимо лучше человеков в то же время технические аспекты и экспериментальная постановка в иишных статьях слишком размыта и расплывчата да и по факту сгененированные статьи являются некоей сборной солянкой из известных фактов отсюда возникает идея давать вступительный экзамен рецензентам core a конференций где им будут даны несколько публикаций от кожаных мешков и сгененированных условной гопотой или sonnet если рецензент не способен адекватно отличить зерна от плевел отфутболиваем проблема правда одна кто ж рецензировать то будет хотя было бы интересно посмотреть на рецензию gpt4 с системным промптом
278,2024-09-13,есть просто банк есть банк столбов и матриц а есть банк оптимальных решеток для квантования данных из нормального распределения если конректно чуваки собрали mseоптимальные решетки для числа точек от 1 до 5000 для одномерного нормального распределения и для всех размерностей от 2 до 10 и числа точек от 1 до 1450 для многомерной гауссианы с нулевым средней и единичной матрицей ковариации решетки были найдены через зачем оно надо если ваши данные нормально распределены или вы можете какимто образом привести их к этому виду то такая квантизация при заданной битности будет оптимальной с точки зрения квадратичной ошибки
279,2024-09-16,квантизовать можно веса можно активации а что еще можно правильно градиенты и это очень даже нужно и полезно в контексте распределенной оптимизации если обучение на многих хостах а скорость передачи информации между серверами не очень высока то на синхронизацию между процессами может уходить основная доля времени а не на сами вычисления дабы уменьшить трафик можно сжимать градиенты прунить квантовать что душе угодно при этом само собой хочется их не слишком испортить иначе все обучение пойдет прахом идея квантовать градиенты для распределенного не нова датируется еще 2016 годом но данная работа предлагает ряд интересных идей некоторые из которых мы узнаем в литературе вышедшей позднее а сама статья 2021 года 1 случайные повороты 2 оптимальные решетки квантования 3 случайный прунинг 4 энтропийное кодирование в тензоре значения могут быть распределены вообще говоря самым произвольным и поганым с точки зрения удобства квантования образом но известно что если повернуть тензор некоей случайной матрицей то с большой вероятностью полученные значения будут распределены как n0 sigma2 в качестве случайных поворотов используют кто бы мог подумать случайные каждый процесс хранит seed генерации а главный сервер зная этот seed на деквантизации может применить обратное преобразование решетку квантизации а тут дело идет о квантовании в 1 и 2 бита надо выбирать с умом потому берут не абы какие а mseоптимальные для 1 бита находится аналитически для 2ух численно дабы увеличить сжатие можно еще случайным образом запрунить часть весов маску передавать при этом она как и вращение задается состоянием случайного генератора решетки квантизации можно еще дотюнить при заданном ограничении на битность посредством таким образом каждый процесс квантует градиенты посылает главному серверу тот их усредняет и получает средний градиент по всем батчам и вся эта кухня идет с теоретическими гарантиями на ошибку аппроксимации метод валидируют на синтетических распределениях и распределенном обучении в качестве бейзлайнов рассматривают qsgd hadamard sq stohastic quantization в качестве бенчей для распределенного обучения рассматривают emnist с маленькой cnnкой и shakespeare с lstm не самые свежие и впечатляющие бенчи но что поделаешь eden наиболее точно приближает исходные данные в сравнении с бейзлайнами при этом по скорости не такой быстрый как qsgd но быстрее kashin sq ближайший с точки зрения качества подход в 4 бита разницы вообще нет между eden и float обучением в 2 и 1 бит появляется некий зазор в качестве но не критичный и eden стабильно лучше конкурентных подходов интересная с точки зрения математики и методологии статья кажется что если бы публикация чуть дотерпела до тех времен и у авторов был на то ресурс когда квантовать llmки стало мейнстримом предложенные идеи были бы обязательно применены в контексте сжатия llm
280,2024-09-23,s некоторое время назад сь мы разбирали где авторам удалось завести c тернарной квантизацией и добиться вполне сносного качества при обучении с нуля лицехваты недавно опубликовали весьма занятный и содержательный длиннопост про дообучение llm в 158 log23 бит напомню что при тернарной квантизации каждый элемент тензора принимает одно из трех значений 1 0 1 и еще есть floatingpoint масштаб вместо absmax берут среднюю абсолютную величину элемента тензора дабы квантовалось лучше перед каждым линейным слоем ставится и это накидывает на предобучении при попытке наивно затюнить llama38b в bitnet сходимость по лоссу не сильно лучше чем у случайно инициализированной модели и авторы начинают копать глубже тернарная квантизация это довольно болезненное надругательство над весами после которого сетке тяжело оправиться потому авторы предлагают переходить к квантованной модели не сразу а интерполировать из fp16 состояния в квантизованное постепенно пробуют разные функции степенную экспоненциальную интерполяцию одна опция степенной в итоге завелась лучше всего шаг обучения тоже пришлось основательно подбирать чтобы выжать приемлемое качество после подбора удачных гиперпараметров на коротких запусках лучшую конфигурацию учат уже 100b токенов 06 бюджета обучения llama38b тем не менее итоговая просадка по сравнению с исходной моделью довольно значительная для smollm 125m разогрев постепенная квантизация не дает улучшений по бенчам просадки значительные по сравнению с fp16 но лучше чем у прошлых квантизаций в похожую битность сам протокол сравнения спорный не у всех вариантов есть результаты сравниваются перплексии разных версий llama с разными словарями да и llama3 выбивает 65 а не 49 на mmlu 5shot авторы реализовали kernel для инференса на тритоне который дает некоторое ускорение а с библиотекой ускорение становится еще более заметным однако работа с ней требует небыстрой компиляции неплохой результат и исследование но есть вопросы к протоколу замеров качества и пока bitnet теряет слишком много в качестве для того чтобы стать общепринятой практикой чтобы пользоваться модельками надо поставить версию transformers из pr
281,2024-09-25,смотритека что
282,2024-09-25,зальем чемнить на iclr и засядем за квантование тока сначала надо раздобыть веса
283,2024-10-02,пристегните ремни а лучше прячьтесь в бомбоубежище будет горячо на текущий момент векторная квантизация является наиболее эффективным с точки зрения качества методом сжатия llm и ребяты из китайского отделения мелкософта выкатили по существу метод представляет собой gptq с векторной single и multi codebook квантизацией с рядом нюансов 1 эффективной инициализацией центроид векторов в кодбуках используют hessianweighted kmeans для полного гессиана считать сложно и дорого потому в этом месте прибегают в диагональному приближению подобное я в свое время заводил когда работали с коллегами над spqr и думали про неоднородную 1мерную квантизацию 2 точность квантизации за счет повышения битности можно повысить за счет нескольких кодбуков используют residual quantization как в quip где новые кодбуки приближают ошибку квантования с прошлых шагов 3 outlierы находят outlierные колонки входные размерности которые выдают большую ошибку на выходе слоя и квантуют их отдельно часть без outlierов обрабатывается стандартным алгоритмом далее прогоняется поблочный файнтьюн как в aqlmquip и endtoend дистилляция метод валидируют на моделях семейств 2 3 и mistral в качестве бейзлайнов берут gptvq aqlm quip и далее начинается самое интересное они применяют поблочный и end2end finetuning как в quip и обновленной версии aqlm но при этом сравниваются со старой версией aqlm а надо тогда с таблицей 4 где метрики на 7b и 13b примерно такие же и немного даже лучше на 70b при сравнении скорости инференса разных подходов криво завели quip при этом мотивируя низкую скорость тем что перемножение на адамаровы матрицы требует операций хотя любой детсадовец знает что статья вышла в конце сентября но про pvtuning и вышедшие в конце мая и июня соотвественно ни слова как будто весть еще не успела дойти по великому шелковому пути в поднебесную и вишенка на торте отсутствие aqlmquip среди бейзлайнов мотивируют тем что модели новые никто не выложил а самим впадлу и напряжно квантовать на что можно тактично заметить что aqlm модели лежат и не просто лежат а там еще есть и метрики с которыми можно сравниться только одна беда они несколько получше будут что делать притворимся что мы ничего не видели моделей кстати неплохо так
284,2024-10-04,лернинг рейт бач сайз и вейт декей звучат как отличные имена для героев фантастического романа
285,2024-10-06,за что большое уважение компании помимо многого прочего так это за реально подробные и занимательные технические отчеты техрепорты по м были довольно содержательными и полезными и полагаю во многом опредилили направления развития llm хоть и основывались по большей части на идеях из прошлых работ по не уступает по качеству и полноте содержания техрепортам по llama и может служить своего рода обзором с рассмотрением и ablation современных техник и достижений в области генеративных моделей прекрасная работа сам бы у себя разобрал в серии постов но лучше чем причастный к созданию moviegen это никто не сделает
286,2024-10-09,спрашивает народ по какойтакой присвоили нобелевскую премию хинтону и хопфилду ну смотрите есть и вряд ли бы она появилась без трудов хинтона и хопфилда хинтона уж точно а в самой премии не указано по какой конкретной физике надо её давать или не давать все формальности соблюдены
287,2024-10-09,раз уж пошла такая гулянка предлагаю дать нобелевскую премию мира суцкеверу за вклад в ai safety или юдковскому
288,2024-10-12,из в diffusers
289,2024-10-12,полагаю что уже немного помнят но когдато выбить sota или создать мега эффективную по параметрамфлопсам на imagenet1k было модно и престижно в данной статье авторы предлагают метод структурированного прунинга который достигает хорошего качества на ряде vitов сnn и их гибридов при структурированном прунинге отбрасываются целые каналы слои или размерности в сети однако ранжировать конструкции разной топологии между собой непонятно как потому предлагается ранжировать только среди структурных элементов одинаковой топологии каждому нейрону сопоставляется некий граф зависимости из входящих и выходящих в него ребер кои погибнут если выдернем нейрон собираем все изоморфные графы в соответствующие кучки например с одинаковым числом входныхвыходных ребер для прунинга нейрона из mlp считаем для них некоторый importance score и для тех у кого он меньше выбрасываем для случая трансформера естественные структуры следующие 1 внутренние каналы в mlp 2 головы трансформера 3 размерности голов трансформера 4 размерность признаков во всей сети пробегаемся по всем возможным структурам и везде выбрасываем одну и ту же долю самых маловажных то что получилось потом дообучаем дабы восстановить качество валидируют метод на convnext deit resnetах и mobilenetv2 в плане критерия не шибко парятся берут просто модуль веса на градиент посчитанный на 100 батчах из 64 сэмплов не то что я пол imagenetа прунят довольно агрессивно в 15 4 раза восстанавливаться надо достаточно долго и основательно потому обучают 300 эпох с дистилляцией на regnety в результате удается добиться небольшой просадки на resnetах по сравнению с базовой моделью правда протокол сомнительный берут модели обученные по старым рецептам без аугментаций на 90 эпох а сами тратят гораздо больше вычислений с использованием современных методик выжимания качества на imagenet на deit и convnext стартуя с deitbaseconvnextbase удается получить запруненные сети с лучшим качеством чем small и tiny модели с тем же количеством параметров и flops те достичь паретооптимальности по качеству на процентдва превосходят в среднем бейзлайны из литературы нравится идея с ранжированием структур с одинаковой топологией но такое ощущение что хороший результат обусловлен преимущественно дистилляцией с длительным обучением на llmки к сожалению масштабировать будет тяжко простым смертным но всякие там nvidia могут позволить для условного
290,2024-10-14,почему дают токены на всякую фигню а не на o1
291,2024-10-16,24 она же semistructured sparsity дает какоеникакое ускорение на gpu от ampere и новее однако просадка при прунинге обычно слишком велика для llmок дабы быть интересной на практике в этой статье предлагают метод обучения хороших 24 масок через маска суть дискретная сущность потому ее просто так не отпизируешь градиентным спуском и авторы предлагают моделировать распределение масок через gumbelsoftmax с вариантам на обучении оптимизируются логиты вероятности сэмплирования одного из вариантов масок те маска есть взвешенная сумма возможных вариантов а на инференсе берется наиболее вероятный обучение суть просто оптимизация кроссэнтропии как на pretrain веса при этом заморожены если какойто вес зануляется или близок к нулю то логиты маски почти не получают градиентов потому авторы добавляют регуляризационный член как weight_decay но со знаком чтобы расталкивать веса от нуля тем самым поддерживая не нулевую норму у немаскированных весов кроме того маски полученные условным sparsegptwanda являются хорошей инициализацией для масок и позволяют чуть улучшить результат метод валидируют на 2 nemotron4 15b и двух маленьких проприетарных gpt3 замеряют по классике перплексию на wikitext и 0шоты по метрикам опережают уверенно все бейзлайны sparsegpt wanda magnitude sparsegpt правда можно завести и получше в отличие от алгоритмов oneshot прунинга которые быстро насыщаются от количества данных maskllm продолжает улучшаться при большем и большем количестве данных что неудивительно ибо это есть по сути метод оптимизации с большим количеством обучаемых параметров ablations 1 инициализация маской от oneshot прунера накидывает в конечном качестве 2 достаточная степень стохастичности сэмплирования важна для хорошего качества дабы модель могла попробовать разные варианты масок 3 не то чтобы сильно но улучшает качество 4 кроме того полученную маску можно оптимизировать на downstream и даже временами улучшить перплексию по сравнению с floatingpoint моделью вполне годная стратегия для обучения но требующая определенных вычислительных затрат те прилично дороже чем прогнать sparsegpt результат достойный но все же просадка остается довольно заметной больше чем у sota методов 2битной квантизации вероятно если еще оптимизировать веса вместе с масками можно выжать больше
292,2024-10-18,ребята из unsloth выкатили про раздебаг а наивное усреднение батчей при gradient accumulation приводит к тому что результат отличается от того чтобы прогнать за один шаг большой батч причем как утверждают авторы лосс всегда больше в стандартном способе вычисления кроссэнтропии есть деление на длину последовательности и если они разные в разных микробатчах результат усреднения будет не такой как лосс на суммарном батче потому предлагается избавлятьcя от усреднения утверждение про то что лосс при naive gradient accumulation больше вообще говоря в конце блога авторы доказывают что лосс при наивном gradient accumulation больше и в доказательстве делают внезапный переход от лосса по микробатчам к среднему лоссу по батчу можно подобрать пусть на первом батче лосс l_1 1 и в батче 4 токена а на втором батче l_29 и 6 токенов тогда усредненный лосс на всем батче а при наивном gradient_accumulation то есть меньше
293,2024-10-19,обсуждать по существу тут нечего чисто поугарать хотим мы прунить llmки текущие методы сжимают слои равномерно однако одни слои могут быть чувствительнее других как эффективно распределить степени прореживания между разными слоями для максимизации качества часть авторов сего опуса годом ранее предложила owl ed где чувствительность слоев определялась на основе доли выбросов процента весов с активациями существенно отклоняющимися от среднего значения в этой же статье предлагают использовать коэффициент в законе убывания сингулярных значений матриц весов w логика такая у случайных матриц закон при иниализации весов элементами из iid нормального распределения убывания собственных значений есть c ограниченным спектром а степенная зависимость типа отвечает сигналу чем медленее убывание тем типа больше сигнала потому матрицы с меньшей степенью предлагается сжимать слабее задают некий порог минимальной и максимальной степени сжатия и распределяют степень сжатия между слоями в зависимости от того насколько великмал коэффициент по сравнению с минимальныммаксимальным по всем слоям модели стоял на дворе 2024 год а ребята основную часть экспериментов делают на llama1 и vicuna валидируют по сложившейся традиции на перплексии и 0shots отдельного внимания заслуживает утверждение при том что перплексия переваливает за 200 в лучшем случае что на практике означает что модель галлюцинирует чуть менее чем полностью на любой запрос предложенный метод по качеству по перплексии несколько лучше uniform критериев основанных на спектральной норме и норме фробениуса предложенного ранее owl но тем не менее просадки остаются весьма значительными на уровне 1eps битных квантизаций при сжатии на 70 есть результаты и на 3 которая почемуто называется llamav37b там просадки значительнее еще и на convnext прогнали для разнообразия найденные профили для llm обычно следующие пруним меньше первые блоки сильнее последние за исключением самого последнего выглядит как использование некой взятой из воздуха характеристики матриц весов для оценки важности без внятной мотивации тем не менее в упорстве и умении себя хорошо подать авторам не занимать потому их и взяли на neurips а вообще прунить llmки тяжело дается както по сравнению с квантизацией в 4 бита просадка на простых бенчах почти и не видна а 50 sparsity сжатие в 2 раза уже существенно ломает модель
294,2024-10-21,нейросеть многоловая по госту
295,2024-10-21,внимание всему голова
296,2024-10-21,у нас в компании оплата по количеству строчек написанного кода отлично по рукам в
297,2024-10-22,сжимать можно не только матрицы весов llmов но много еще чтото другое не без пользы для человечества болееменее все современные sota или около того диффузионные модели работают в латентном пространстве с меньшим пространственным разрешением от автокодировщика тандема из сети кодирующей из исходного пространства данных в латентное называемой энкодером и декодирующей обратно декодера хочется одновременно высокой степени сжатия и качественной реконструкции одновременно наиболее популярный пространственный фактор сжатия stable diffusion flux dalle3 по всей видимости при таком сжатии особенно если еще и поднять число каналов в латентном пространстве до 16 скажем реконструкции почти не отличаются от оригинала однако карты признаков остаются все еще достаточно большими при подаче картинок высокого разрешения и требуется немало вычислений отсюда возникает желание двигаться в сторону больших факторов сжатия при сохранении качества и авторы данной статьи предлагают несколько интересных архитектурных решений по сути в предложенной статье ключевых идеи 1 skipconnection residual autoencoding при downsample патчи 2x2 сворачиваются в 4 канала а зачем группы по 2 канала усредняются ребята изобрели average pooling при upsample 4 подряд идущих канала разворачиваются в 2x2 каждый канал дублируется 2 процедура обучения предобучение на низком разрешении только на реконструкцию без адверсариального лосса дообучение на низком разрешении верхних слоев декодера с gan лоссом для получения резких деталей дообучение глубоких слоев энкодера и декодера на реконструкцию на высокое разрешение дабы адаптировать латенты под highresolution c другим распределением частот плотностью обьектов
298,2024-10-22,авторы тестируют vae на непосредственно задаче реконструкции и применительно к латентной диффузии dituvit на imagenetffhq mapillary vistas я тоже впервые слышу про такой рассматривают сжатие в 3264128 раз по пространству с количеством латентных каналов 3264128 для повышения эффективности vanilla transformer блоки заменяют на residual autoencoding критичен при высоких факторах сжатия сеть сама не может выучить этот skip connection sdlike vae оказывается беспомощен жесткие дефекты и размытие на экстремальных факторах а им удается иметь метрики болееменее на одном уровне с ростом downsampling при фиксированном количестве патчей в dituvit качество выше при более сильном сжатии за счет автоэнкодера и меньших патчах из латента размера 1 против меньшего сжатия и больших патчей утверждается что с dcvae можно получить качество не хуже а то и лучше чем в сетапе как в оригинальной статье при этом имея 4кратное ускорение инференса предложенная процедура дообучения на высокое разрешение критична для адекватного качества на высоком разрешении кроме того dcvae проверют на коротком 100к итераций 2image сетапе результаты выглядят весьма достойно прокачка энкодера это комплиментарное направление повышения эффективности диффузионных моделей вкупе с архитектурной оптимизацией и дистилляцией по шагам похожую идею во всей видимости реализовали в свежей
299,2024-10-23,выбираете сторону силы статьи на оказывается можно читать в инвертированной цветовой схеме черный фон белый текст для этого надо просто в pdf заменить arxiv не знаю зачем но прикольно
300,2024-10-27,решил я запустить генерацию с с то есть без всякого контекста с температурой 1 и было любопытно что первое взбредет сеточке в голову ответ оказался немного неожиданным 慢性胰腺炎ci是一种慢性胰腺炎其特点是胰腺炎症的持续存在和慢性损伤胰腺炎是肾上腺皮质激素cort过量释放的副作用之一与ci相比其作用更持久 если вбить в переводчик выдает следующее
301,2024-10-29,кода нет многие современные методы сжатия моделей используют некоторую для приближения чем ближе эта выборка к целевому набору задач тем интуитивно лучше качество но хороших предписаний по отбору последовательностей до сих пор не существует ранее эти же авторы перебрали несколько вариантов и обнаружили что некоторое хоть и не столько значительное различие в качестве есть в зависимости от источника данных в этой же статье авторы предлагают сжимаемой модели сгенерировать данные для калибровки калибровочные последовательности генерируют начиная с токена дабы повысить качество данных предлагается динамически менять температуру сначала больше чтобы было разнообразие а затем снижать по мере увеличения количества сгенерированных токенов рассматривают несколько небольших llm gemma 2b phi2 opt 67b mistral 7b llama318b которые сжимают при помощи gptq 4битная квантизация и sparsegpt wanda 24 прунинг в качестве бейзлайнов берут wikitext2 c4 рандомные токены сosmopedia качество замеряют на стандартных бенчах из синтетические данные сгенерированные моделью почти всегда лучше выборок из датасетов различие заметнее на прунинге где просадки больше сгенерированный текст обыкновенно довольно связный грамматически корректный по статистикам довольно близок к реальному но менее разнообразный снижение температуры генерации от 2 до 1 по ходу генерации дает самые лучшие результаты хоть и без значительного отрыва от фиксированной температуры 1 идея прикольная и для используемого количества данных 128 последовательной длины 2048 достаточно дешева в проверке эффективность метода во многом зависит от качества базовой модели что впрочем для современных llm интересных сообществу верно интересно справедливы ли полученные выводы для более сложных задач
302,2024-10-29,соавтор и коллега по выкатил демку aqlm на ржавчине можно просто взять и запустить квантованную в 2 бита aqlmpv в браузере на cpu со скоростью токена в секунду на m1 lm head квантизуется в int8 отличная работа вова
303,2024-10-31,breakpoint уровень сеньор
304,2024-11-04,я ожидал от статьи с названием что в ней квантизованные веса будут реально флексить а оказался просто поблочный qat
305,2024-11-08,если вам приспичит инферить llmку на микроволновке пока разогревается еда мы как раз подогнали 2битные меньшие версии llama32 основное место занимают уже эмбеды которые хоть shared но с fsdp немного покорячиться пришлось ждем выкатки квантизации эмбедов в
306,2024-11-08,ребята из linkedin написали кернелы для разных операций в llmках на когда нибудь я научусь писать на нем а не про него которые ускоряют процедуру обучения и снижают расход памяти по сравнению с ванильной торчовой реализации в частности liger kernel предлагает следующее зафьюженные и на прямом и обратном проходе зафьюженные и на прямом и обратном проходе оптимизированный r и самое интересное оптимизация вычисления кроссэнтропии по поводу последнего словари нынче у моделек перевалили за 100к и на скольлибо длинных последовательностях матрица логитов будет весить десятки гигов потому авторы реализовали прямой проход вычисление кроссэнтропии и обратный проход в одном kernelе без необходимости материализации одновременно матрицы логитов и градиентов по логитам кроме того считать логиты можно не разом для всей последовательности а чанками замеряют на a100 предложенные нормализации дают хорошее ускорение по сравнению с торчом не хватает сравнения с apex rope прямо знатно ускорился swiglu и geglu по скорости такие же как в торче но снижают расход памяти в 16 раз в end2end сценарии гоняют finetune на 4a100 на alpaca на llama38b qwen27b gemma17b скорость обучения возрастает от до и пиковый расход памяти уменьшается в среднем на метод также валидируют в связке с методом где llm учится предсказывать несколько следующих токенов за раз и на каждый n1 токен своя обучаемая голова liger kernel снижает заметно расход памяти как с замороженной так и обучаемой тушкой трансформера благодаря трюкам с вычислениями логитов liger kernel можно вызвать по щелчку пальца через или патчинг модели из а можно импортировать модули и из них собрать модельку liger kernel уже интегрирован в t и
307,2024-11-09,в комментариях был задан резонный вопрос а сравнивали с с или без будучи лучшего мнения о людях я полагал что да но если посмотреть в в репозитории проектов compile не фигурирует нигде rmsnorm берут сырой из в частности хитрые собаки
308,2024-11-10,написание кода предоставляется читателю в качестве несложного упражения низкоранговые приближения применяются много где в deep learning и не только про lora не слышал только ленивый в данной статье группа миньонов кожаной куртки предлагает компенсировать ошибку методов сжатия прунинга и квантизации за счет вставки низкорангового разложения ошибки сжатия как адаптера паралелльно весам метод прост как пробка 1 были у нас исходные веса w 2 после сжатия имеем wc 3 svd матрицы w wc 4 берем сколькото главных компонент и вставляем параллельно сжатым весам как lora адаптер при этом ничего не надо обучать метод тестируют поверх 50 60 unstructured sparsity 24 sparsity с sparsegpt в качестве алгоритма сжатия и 34битной квантизацией gptq в большинстве экспериментов берут ранг добавки равным 128 проверяют на llama2 7b и 13b llama3 8b метод дает ожидаемо небольшой прирост по качеству на стандартных бенчах но не слишком выдающийся чем больше ранг добавки тем ожидаемо лучше качество в случае необходимости добавки можно дотюнить аля чиста lora файнтьюн и это еще немного накинет простой и бюджетный способ накинуть немного качества за счет небольшого количества параметров и дополнительных вычислений так как базовые методы сжатия dataaware sparsegpt gptq то вместо svd можно было бы использовать и это бы работало скорее всего чуть лучше
309,2024-11-10,будь бы у меня дети я бы их побенчмаркал на mmlu
310,2024-11-12,продолжение истории с от microsoft 1 в была предложили бинарную квантизацию весов 2 во вместо бинарной квантизации тернарная но зато качество заметно возросло при этом активации квантовались absmax в int8 в данной статье фокусируются на сжатии активаций в частности kvкэшей в 4 и бита и меньше авторы строят гистограммы весов активаций после линейных слоев выходы q k v gate и up неплохо описываются нормальным распределением а вот out down имеют тяжелые хвосты потому тяжело квантизуются поэтому предлагается квантовать в 4 бита активации только первой группы слоев а для выхода out_proj следуя работе которую сам пока не читал прореживают активации через с 50 sparsity кроме того вместо swish в ffn gate применяют чтобы форсировать прореживание активаций в down projection впрочем так как паттерн случайный использовать его для ускорения сходу не получится также рассматривают fp4 квантизацию вместо int4 с большим динамическим диапазоном обучают семейство подобных моделей от 700m до 7b параметров на красной пижаме причем обучают в 2 стадии сначала с 8битными активациями большее время обучения как в bitnet158 а затем в перечисленных выше группы слоев квантуют в 4 бита или прореживают bitnet a48 int4fp4 почти не уступает по качеству fp бейзлайну не очень сильному впрочем relu2 дает значительную около 80 степень разреженности в down_proj обучение чисто в int4 не сходится в принципе с fp4 гораздо хуже чем предложенный вариант relu2 дает тоже качество что более традиционный swish кэши квантизуют в 3 бита кроме bos eos токенов которые более чувствительны и поддерживаются 4 битными без просадки в качестве по сравнению с 4битными кэшами определив хорошую конфигурацию 2b модель обучают на 2t токенах и получают метрики конкурирующие с хорошими fp16 моделями в той же весовой категории llama32 gemma2 qwen25 впрочем ребята из мелкософта известны тем что добавляют тест в train как было в случае с моделями phi история с bitnet начинает выглядеть все более жизнеспособной несмотря на весь мой скептицизм по поводу кустарности метода во всяком случае история выглядит уже вполне рабочей тем не менее было бы важно проверить насколько хороши модельки в качестве чатботов ежели выкатят веса
311,2024-11-12,процесс чтения рецензий на iclr впрочем как и на любой другой core a конфе
312,2024-11-15,кода нет как и ресурсов у вас чтобы воспроизвести известно что с увеличением размера модели и количества данных качество моделей в некотором смысле обычно по val лоссу растет причем не абы как а по простым степенным законом аля также известно что инферить большие модели тяжело и дорого а методы квантизации позволяют существенно сжимать модели в пределах умеренной просадки качества есть что более современные llm llama3 gemma2 qwen2 сжимаются заметно чем предшественники отсюда вопрос при заданном бюджете на обучение какое отношение модели и квантизации и в рассматриваемой работе авторы проводят детальное и масштабное исследование делая целый ряд нетривиальных выводов ниже количество данных количество параметров модели precision на обучении авторы обучают тучу моделей 465 штук аля разного размера битности от 3 до 16 и с разным бюджетом обучения вплоть до отношения числа токенов к параметрам 105 тем самым авторы учитывают случай характерный для современных моделей где перекос в сторону данных на chinchillaoptimal закон у llama3 против по шиншилле рассматривают сценария 1 posttraining quantization учим во bf16 и квантизуем после обучения берут gptq как ходовой и рабочий метод 2 quantizationaware training квантизуем по ходу обучения но только веса 3 lowprecision training квантизуем во время обучения веса активации и kvкэши предложенный scaling law для posttraining квантизации имеет вид p precision она же битность где прирост лосса вызванный квантизацией для qat и lowprecision training то есть некоторые модификации исходного scaling law
313,2024-11-15,с увеличением количества данных на обучении и качества модели ошибка квантизации кроме того ошибка для больших моделей фиксированном отношении у моделей самый любопытный и шокирующий вывод в том что начиная с какогото момента у модели обученной на большем количестве данных качество после квантизации будет чем у модели на меньшем то есть при квантизации в низкую битность бессмысленно учить модели до упора и в какойто момент придется остановиться чтобы получить лучшее что в принципе возможно проанализировав зависимости авторы выводят функциональную форму для где ошибка растет степенным образом по количеству данных убывает степенным образом по размеру модели и растет экспоненциально с уменьшением битности далее авторы обучают модели квантизуя либо только веса либо веса и активации оказывается что эффект от квантизации можно описать как некую мультипликативную добавку к количеству параметров вида те модель с квантизацией чего либо и эквивалентна модели с параметрами коэффцициенты p gamma свои у весов активаций и кэшей кэши легче всего квантизуются веса посередине а активации тяжелее всего причем ошибки квантизации w a kv независимы и предложенная зависимость хорошо фитируется на основе экспериментальных точек предполагая что у нас есть вычислитель умеющий эффективно работать с любой точностью авторы фиксируют бюджет обучения определяемый как 16 от bf16 и ищут оптимум по лоссу наблюдения следующие если учить в низкой точности масштабировать размер модели надо быстрее чем количество данных в шиншилее напомню оптимально пропорционально computeoptimal precision не зависит от бюджета обучения и примерно равно на параметр эксперименты проводят с intx fpx floatingpoint чуть лучше но общий вывод справедлив отсюда замечание что обучение в и более низких битностях может быть
314,2024-11-15,как мне кажется это одно из самых интересных и фундаментальных исследований в области которое обрисовывает возможностей методов квантизации интуитивно понятно что сжимать модели до бесконечности невозможно ибо так или иначе сеть должна в себе както хранить все знание о мире но вопрос стоял именно в определении самих границ крайне любопытно что отношении оптимальной полученной битности к 16ти близко к computeoptimal sparsity в sparsity scaling laws совпадение ли кроме того интересно насколько полученные выводы справедливы для более навороченных векторных квантизаций и что будет если поменять точность обучения на ходу учить в fpbf16 прогнать ptq и далее qat lowprecision training еще кажется что полученные выводы будто бы находятся в расхождении с результатами bitnet который исходя из полученных зависимостей должен быть дохрена не оптимальным и выдавать слабое качество
315,2024-11-15,теперь с чистой душой можно лечь досыпать
316,2024-11-21,я добрая училка
317,2024-11-21,в настоящий момент является одним из самых популярных методов квантизации весов llm в 4 бита как дающий некий хороший баланс между качеством и временем работы он выдает стабильно лучшее качество по сравнению с наивным roundtonearest и иными datafree квантизациями при этом масштабируется сравнительно легко на огромные llm тестирует сугубо работоспособность метода и не годится для приложений значительную популярность 45к на гитхабе набрала библиотека c аля лицехватским интерфейсом и поддержкой различных моделей а так же кернелов аля marlin для эффективного инференса к сожалению maintainerы забросили либу потому самые новые модели через нее квантовать не получится ребята из modelcloud продолжили их дело создав gptqmodel куда добавлены llama32 qwen25 и другие сравнительно новые модели кроме того авторы обещают более быструю калибровку до 50 быстрые замеры перплексии и немного лучшие по качеству квантизованные модели
318,2024-11-25,зубы заговаривают
319,2024-11-28,введение llm и прочие foundational модели хранят в себе массу полезной информации однако некоторые факты могут быть довольно конфиденциальными или чувствительными и мы бы предпочли чтобы моделька не проворонила их невзначай в диалоге например в трейн сэт могла утечь ваша неуклюжая переписка с девочками в тиндере а конфуза хотелось бы избежать потому были разработаны разные техники unlearning например градиентный подъем и negative preference optimization которые понижают правдоподобие нежелательных концептов но действительно ли сеть забывает про них оказывается что если достаточно но не слишком агрессивно заквантовать модель то она чудесным образом вспоминает все то что ее так тщательно учили забыть авторы рассматривают два семейства методов забывания фактов где оптимизационный процесс повышает кроссэнтропию нежелательного концепта форма direct preference optimization с занижением правдоподобия того что хочется забыть чтобы сеть не забывала кроме целевых концептов все остальное параллельно с этим дообучают на данных как в обучающей выборке с выброшенными концептами которые мы хотим забыть подобные техники работают довольно успешно но как только модель квантизуют в 4 бита вероятность выдачи нежелательной информации становится примерно как у сжатой в 4 бита исходной модели до того как ее стали учить забывать 8битная квантизация не так сильно меняет модель поэтому метрики забывания мало отклоняются от 16битной модели в качестве методов квантизации рассматривают выводы справедливы для всех методов дабы заставить сетку быть устойчивой к забыванию предлагается уйти в пространстве параметров от исходных значений но чтобы не испортить сеть в целом дообучают только к целевым концептам параметры и в качестве метрики чувствительности используют лосса забывания можно было использовать маску произвольного вида но чтобы сэкономить память авторы отбирают чувствительные веса на уровне слоев те обучают только подмножество слоев называется сия конструкция конечно же sure
320,2024-11-28,метод валидируют на бенчмарке для забывания и датасетах news где нужно забыть новость and books забыть персонажей из гарри поттера какие модели используются я не понял из текста кроме того оценивают общие способности модели через mmlu truhtfulqa triviaqa и энтропию nграм на alpacaeval предложенный подход оказывается устойчив к квантизации во всех рассматриваемых сценариях в плане выдачи нежелательной информации но снижает несколько качество на некоторых бенчах по общим способностям факт забавный хоть и в какойто мере ожидаемый существует ли принципиальный способ заставить сеть забыть чтото увиденное техники снижения правдоподобия по всей видимости просто заметают это знание под ковер кажется что требуется некая более глубокая хирургия с применением и прочей модной лабуды
321,2024-11-29,статья код очень прикольная работа про то что внутри llm можно найти один единственный вес зануляя который мы обрушиваем качество работы модели в пропасть такие параметры авторы называют супер весами super weights и предлагают метод их нахождения за один forward pass внутри обученных llm находится группа весоваутлаеров с большой магнитудой они могут составлять порядка 001 от всех весов модели что в случае миллиардных моделей всё равно сотни тысяч это было известно ранее в текущей работе показывают что внутри этой группы находится один единственный вес тот самый не обязательно самый большой важность которого превышает суммарную важность тысяч других аутлаеров он необходим для качества без него llm не может генерить нормальный текст перплексия вырастает на несколько порядков а точность на zeroshot задачах падает до рандома ранее были найдены суперактивации критичные для качества они существуют в различных слоях имеют константную магнитуду и всегда обнаруживаются в одинаковой позиции несмотря на вход текущая работа находит что канал активации совпадает с оным для супер веса и сперва активация обнаруживается сразу после супер веса прунинг этого супер веса значительно уменьшает активацию так что вероятно активация вызвана им а не просто скоррелирована такие активации называются супер активациями предыдущая работа объясняла супер активации через bias terms но не объясняла как они получаются и почему на одних и тех же местах сейчас авторы эмпирически нашли что до down проекции down_proj произведение адамара hadamard product gate и up проекций gate_proj up_proj создаёт относительно большую активацию супер вес далее усиливает её ещё и даёт супер активацию напомню что mlp блок в ламе выглядит так out down_proj act_fngate_projinput x up_projinput sw можно найти анализируя спайки в распределениях входов и выходов down_proj для этого достаточен прямой проход с одним промптом авторы нашли супер веса для llama 7b13b30b llama 2 7b13b mistral7b olmo 1b7b phi3 провели эксперименты по обнулению sw в том числе с восстановлением sa до исходного значения чтобы проверить влияние sw на другие активации это восстанавливает 42 потери то есть влияние sw на качество выше чем просто через sa по анализу 500 различных промптов из lambaba validation set видно что при убирании sw вероятности стопслов сильно возрастают а обычные слова соответственно занижаются для the это 2 для 5 и для 10 то есть наличие sw как бы подавляет стопслова и позволяет генерировать осмысленный текст другой интересный эксперимент скейлит супер веса с коэффициентами от 0 до 3 где оригинальный режим работы соответствует значению 1 и оказывается что при увеличении sw качество модели ещё немного возрастает это забавный результат имея это знание можно предложить специальный метод квантования стандартные механизмы квантизации могут быть недостаточно хорошими так как аутлаеры искажают распределение влияя на размер шага и увеличивая ошибки квантования здесь под super outliers подразумеваются и sw и sa предложенные методы восстанавливают sw и sa после квантований с клиппингом и заменами на медианное значение это всё работает лучше дефолтных методов главный вывод надо защищать супер веса в статье есть подробный разбор экспериментов кому интересно поглубже также новый метод меньше теряет в качестве с увеличением размера блока прикольный результат в общем это всё несколько перекликается с темой про лотерейные билеты там внутри большой сети обнаруживалась сильно разреженная подсеть обучая которую можно было достигать качества исходной сети или даже выше интересно входят ли супервеса в лотерейный билет наверняка
322,2024-11-29,классный обзор на классную статью
323,2024-12-01,sota векторные методы квантизации такие как aqlm pv quip способны достигать умеренной просадки качества при сжатии в 23 бита и почти без просадки смотря на чем замерять в 4 битах однако деквантизация в данных подходах довольно ресурсоемкая что ограничивает предельно возможную скорость инференса кроме того сами методы требуют некоторых приседаний и вычислительных затрат чтобы произвести на свет сжатую модель и требуют калибровочных данных для оценки распределения данных что может давать некий сдвиг и коллеги и совместно с корешами из ist kaust и mit предложили новый в большинстве сценариев метод квантизации с переменной битностью слоев который дает одновременно и хорошее качество в 34 бита и значительное ускорение stateoftheart методы квантизации и прунинга для llm по большей части оптимизируют ошибку на выходе линейного слоя так как прямой учет лосса слишком затратен на масштабе но в конечном итоге хочетсято не просесть в качестве генерации а связь между послойной ошибкой и ошибкой на выходе модели не очевидна и авторы обнаруживают что при не слишком сильном сжатии есть линейная взаимосвязь между конечной ошибкой перплексией и относительной ошибкой на выходе слоя то есть перплексия сжатой модели есть перплексия исходной некоторая линейная комбинация послойных ошибок сжатия но сама ошибка может существенно варьироваться от слоя к слою и зависит от распределения значений в матрицах весов кроме того хочется иметь datafree метод квантизации а наивный rtn просаживает качество довольно сильно при сжатии в 4 и и ниже бит потому авторы применяют знакомые давним подписчикам канала адамаровы вращения которые убивают выбросы в весах и приводят распределение весов к нормальному iid а для нормального распределения существуют рассчитанные численно оптимальные решетки потому метод зовется причем рассматривается случай как скалярной так и векторной квантизации векторная квантизация с кодбуками дает лучшее качество чем скалярная и при этом эффективно реализуется кернелами из библиотеки налагая на данный вес случайный шум и смотря на изменение перплексии можно оценить между послойной ошибкой и перплексией на выходе далее решением задачи линейного программирования при заданной степени сжатия определяет отпимальное распределение уровней сжатия среди несколько возможных вариантов таким образом задача нахождения оптимальных квантизованных весов сводится к нахождению каждого слоя и для нормально распределенных iid весов
324,2024-12-01,метод валидируют в традиционном сетапе для сжатия llm на llama31 llama32 и qwen по качеству предложенный подход заметно опережает datafree af nf hqq особенно при сжатии в 325 бит даже при однородном сжатии чем больше размерность векторов в квантизацианной решетке тем лучше качество но p2 2мерная оптимальна точки зрения баланса между скоростью и качеством квантизация неплохо накидывает по сравнению с однородной higgs квантизация с flute кернелами гораздо быстрее на rtx 4090 навороченных векторных квантизаций aqlm quip и даже быстрее чем кернел для батчового инференса для скалярной однородной квантизации по метрикам метод превосходит gptqawq и немного уступает sota векторным квантизациям но просадка компенсируется куда большей производительностью с точки зрения практической привлекательности линейная модель работает достаточно точно вплоть до сжатия в 253 бита классный результат от коллег ждем одобрения pr в transformers для выкатки на широкую публику интересно было бы еще протестировать на диффузионках аля flux
325,2024-12-03,на текущий момент диффузионные модели уверенно занимают пьедестал почета в задаче генерации изображений по тексту по заданному запросу даже весьма нетривиальному насыщенному нюансами и деталями они способны генерировать разнообразные картинки хорошего качества однако существенным и основным недостатком диффузионных моделей является их итеративная природа генерации чтобы сгенерировать одно изображение диффузионную модель приходится прогонять много раз изза чего приходится подождать некоторое время прежде чем замечательная картинка явится на свет наряду с диффузией существует альтернативная парадигма генерации так называемые авторегрессионные модели которые генерируют изображения последовательно патч за патчом патч маленький кусок изображения скажем 16x16 пикселей однако они работают на практике еще медленнее для больших изображений так как генерация каждого следующего патча требует прогона модели а количество патчей может переваливать за тысячу кроме того они уступают в качестве диффузионным моделям поэтому долгое время считались неконкурентоспособными однако этой весной команда исследователей из в работе visual autoregressive modeling scalable image generation via nextscale prediction предложила модификацию авторегрессионной парадигмы где за один проход предсказывается не один патч а все разрешение целиком с помощью специальной аамодели вариационно кодировщика с остаточной квантизацией изображение разбивается на разные уровни нижние уровни соответствуют общей семантике изображения а верхние уровни тонким деталям и текстурам на нижних уровнях немного патчей поэтому прогон модели на них дешев и стоимость прогона возрастает с переходом на каждое следующее разрешение на этапе генерации модель смотрит на все прошлые разрешения и генерируют текущее полученная картинка получается посредством суммирования всех разрешений данная работа смогла добиться качества генерации сопоставимого с хорошими современными диффузионными моделями при этом будучи значительно быстрее их в задаче генерации обусловленной на класс объекта из увы генерация из фиксированного набора 1000 классов не так интересна пользователям как генерация по произвольным и разнообразным текстовым запросам поэтому для верификации жизнеспособности идеи последовательной генерации изображений по разрешениям требовалась проверка в более сложном и интересном сценарии некоторое время спустя после выхода работы var вышли и которые адаптировали вышеупомянутый подход для генерации изображений по тексту в этих работах удалось добиться сносного качества генерации и следования текстовому запросу но все же далеко позади современных диффузионных генеративных моделей таких как sdxl t lumina поэтому мы исследователи из yandex research решили обучить свою генеративную text2image модель
326,2024-12-03,за основу мы взяли архитектуру модели из и последовательно анализируя и улучшая ее пришли к конечной модели первое с чем мы столкнулись это с тем что оригинальная архитектура становилась крайне в процессе обучения внутренние активации модели вырастали до очень больших значений которые уже нельзя было представить в машинной точности и обучение разваливалось следуя работе мы добавили дополнительные в модель и данная архитектурная модификация стабилизировала обучение кроме того качество самой модели тоже улучшилось далее мы проанализировали авторегрессионной модели и обнаружили что текущее разрешение почти не смотрит на прошлые поэтому разрешения можно генерировать независимо друг от друга убрав авторегрессию на прошлые разрешения мы нисколько не потеряли в качестве и при этом ускорили модель примерно на 2030 и последняя ключевая находка оказалась в том что технику cfg улучшающую качество генераций и соответствие текстовому запросу но требующую два прогона через модель вместо одного можно отключить на высоких разрешениях без ухудшения конечного результата за счет этого можно добиться почти по сравнению со стандартной процедурой генерации с cfg полученную модель мы назвали cale se ransformer for extto mage synthesis так как она генерирует изображение по тексту разрешение за разрешением
327,2024-12-03,мы обучили нашу модель на внутреннем большом датасете из множества картинок 100m в качестве бейзлайнов мы берем и ее ускоренные версии sdxlturbo sdxldmd2 sd3medium luminanext а так же современные авторегрессионные модели и упомянутый hart для оценки качества моделей мы использовали принятые и стандартные в литературе метрики fid clip pickscore image reward все знают что они но этикет же надо соблюдать а также пользовательские предпочтения на корзинке из 128 запросов пользователи оценивали следующие аспекты изображения соответствие текстовому запросу общая красивость изображения количество деталей и сложность композиции отсутствие дефектов и артефактов в изображении switti по качеству значительно превосходит существующие авторегрессионные подходы как по метрикам так и пользовательским предпочтениям с диффузионными моделями мы добились паритета по качеству но при этом switti генерирует в раз быстрее оригинальной sdxlмодели и 2 раза быстрее ускоренных версий sdxlturbo кроме того мы обнаружили что если по ходу генерации подменить текстовый запрос можно получить нечто среднее например подав в запрос изначально ведьмака а затем подменив в середине генерации запрос на робота вы можете получить некоего киборгаведьмака или подав изначально зимний пейзаж а подменив его на какойто стадии летним можно получить разную градацию перехода от зимы к лету в данной работе нашей команде удалось сделать генерирующую на уровне разрешений модель которая смотрелась бы не блекло и безнадежно на фоне диффузионных моделей кроме того switti генерирует быстро что делает ее привлекательной для приложений где требуется сгенерировать много изображений за разумное время тем не менее есть еще куда расти на текущий момент switti генерирует только в 512x512 и до нынешней sota flux recraft ideogram v2 midjourney 61 еще очень далеко но диффузионные модели уже давно полируются и улучшаются а varinspired парадигма зародилась совсем недавно и есть еще большой потенциал для роста
328,2024-12-03,и картиночки
329,2024-12-04,в процессе серфинга по тытрубе и подготовке к собственной защите наткнулся на в сколтехе тема диссертации синтез и с помощью ganов и данный рассказ действительно очень интересный и увлекательный обзор техник по улучшению качества генерации реализма общаемости на произвольные аспекты и положения головы и тела картинки просто конфетка и сам рассказ очень увлекательный и познавательный нынче диффузия всему голова но многие идеи актуальны и до сих пор в общем рекомендую
330,2024-12-06,хорошо мы успели залететь авторы var без первого c которым вышла презанятная история выкатили свою модель под названием обзорчик появится немного позднее
331,2024-12-07,толькотолько мы успели выпустить switti как создатели var опубликовали собственную модель позиционирующую себя так же как конкурент моделей уровня наиболее примечателен в оригинальной статье по var использовался c общей кодовой книгой на все масштабы выход энкодера на данном масштабе заменяется на ближайший вектор из кодовой книги на этом этапе возникает некоторая ошибка аппроксимации и по всей видимости в это кроется причина по которой vqvae традиционно уступают непрерывным аналогам по качеству реконструкции чем кодовая книга тем потенциально ошибка квантизации но огромные кодовые книги скажем с 264 векторами не влезут в память никакой машины потому авторы предлагают параметризовать кодбуки для каждого масштаба k и квантизация осуществляется просто взятием знака от непрерывного вектора z_k соответствующему уровню k в иерархии c домножением на некоторый коэффициент рассматривают две опции lfq bsq отличающиеся на коэффициент 1 sqrtd и берут в итоге второй вариант так для него существует эффективное выражение для вычисления энтропийной регуляризации используемой для более эффективного использования кодбука благодаря такой бинарной квантизации можно расход памяти требуемый на кодбук размера 2d уменьшается с o2d до od благодаря чему можно хранить колоссальные кодовые книги кроме того автокодировщик учат быть устойчивым к ошибкам предсказания токенов и во время обучения случайным образом подменяют некоторую долю токенов здесь замечу что в экспериментах по switti мы обнаружили что можно менять довольно значительную долю токенов без изменения выхода модели и модель оказывается устойчивой к этому без манипуляций дабы поддерживать разные aspect ratio и размеры используют факторизованные позиционные эмбеды в качестве текстового энкодера используют flant5 обусловливание на текст осуществляется как через selfattention за счет добавления токенов промпта в prefix так и cross attention между картиночными и текстовыми токенами
332,2024-12-07,данные для обучения собраны из отфильтрованных laion coyo openimages датасетов сначала учат на 256x256 разрешении потом на 512x512 и в конце переходят на 1024x1024 токенизатор как и в оригинальной статье осуществляет 16x уменьшение по пространству качество оценивают по fid на датасете из 40к изображений вместо стандартного ms coco кроме того замеряют качество на и и предпочтения пользователей на imagerewardhpsv21 по аспектам prompt following и visual aesthetics на бенчмарках infinity уверенно побеждает все прошлые авторегрессионные модели switti в сравнениях нет ввиду очень малого промежутка времени между выходом моделей опережают по качеству и диффузионки sdxlsd3mediumpixartsigma модель хорошо умеет в рендеринг текста если верить черрипикам в ablations авторы показывают что огромные кодбуки важны для качественной реконструкции и самые большие кодбуки даже лучше continuous vae обучение с зашумлением заметно улучшает fid и немного другие метрики зашумление оптимально согласно заявлениям авторов модель генерирует быстро одно изображение в разрешении 1024x1024 за секунд против и у sd3 medium и sdxl соответственно годное подтверждение жизнеспособности scalewise парадигмы ключевой вклад данной работы сильно улучшенный токенизатор который и является основным ограничителем качества в случае switti умная токенизация в картиночных и видео моделях вообще очень горячая тема во второй половине текущего года ждем с нетерпением релиза моделей чтобы поиграться и забрать к себе наработки
333,2024-12-09,собственноручно проверил наличие супервеса см и разбор от в aномальный вес находится в позиции 400 выходной канал 1417 входной канал в не столь ярко выражен перплексия на wikitext2 8k context length выросла с 8375 до 8625 при занулении данного веса но все же очень много для всего одно веса
334,2024-12-15,на днях наткнулся на канал в некоего парень доступно с красивыми визуализациями в стиле рассказывает про всякие темы из теории информации и особенности программирования на в частности особого внимания заслуживает про то как написать который быстрее реализаций в торче и на тритоне он пошагово анализирует узкие места нюансы железа и алгоритма и постепенно добивается улучшения производительности 1 эффективный алгоритм редукции для нахождения максимума 2 оптимизации доступов к памяти coalescing 3 перенос части операций из shared memory в регистры gpu которые еще быстрее 4 векторизация операций через float4 5 однократная подгрузка данных для подсчета максимума и экспоненты вместо двухкратной красивое
335,2024-12-16,отныне присно и во веки веков ведущий данного канала не просто балабол а заслуженный балабол спасибо сколтеху всем товарищам коллегам и соавторам за эти замечательные и продуктивные 3 года путь полный трудностей открытий и приключений это была славная охота сам текст диссертации интересующиеся могут найти
336,2024-12-16,о это подарок в честь присуждения степени
337,2024-12-20,это а вы тоже думали что прибавление нуля к числу никогда ничего не меняет
338,2024-12-20,для получения максимального ускорения особенно при инференсе большими батчами нужно квантизовать не только веса но и активации однако богомерзкие выбросы сильно затрудняют квантизацию и в прошлых работах t было предложено применять матрицы или некоторые обучаемые которые выбросы по каналам однако выбросы встречают не только в отдельных каналах но есть и токены например и иные разделители отличающиеся большими значениями активаций и их квантизация большими группами pertensor с максимальным ускорением приводит к большим ошибкам поэтому в упонятых выше методах использовали pertoken квантизацию с некоторым замедлением инференса в этой статье авторы предлагают вынести в дабы упростить квантизацию сам подход мотивирован идеями про и иногда трансформерному блоку не нужно ничего делать и дабы реализовать почти тождественную операцию приходится выбрасывать весь attention в токены с низкой семантикой потому авторы предлагают просто найти все эти токены и положить в префикс последовательности токены находят на основе некоторой калибровочной выборки смотря на выход down_proj в mlp отбирают токены чья норма сильно больше медианной по последовательности сам процесс достаточно быстрый занимает всего несколько минут таких токенов оказывается от 1 до 4 в рассмотренных моделях llama23 qwen mistral обычно это и еще символы переноса строки n артикли и знаки препинания данная модификация позволяет существенно уменьшить норму последующих токенов и упростить процесс квантизации для максимизации качества в конце еще предлагается делать поблочный qat следуя efficientqat метод валидидируют на 23 в режимах and квантизации w точность весов a активаций kv kvкэшей веса квантизуются кэши активации предложенный подход оказывается примерно на одном уровне с pertoken квантизациями чуть уступая лишь spinquant с дообучением матриц вращения каналов при этом скорость инференса на быстрее поблочный файнтьюн немного накидывает простой и полезный трюк для квантизации тем не менее кажется что возможность делать ничего в блоке модели стоит закладывать изначально и скорее всего ввиду потребности в низкобитных моделях сообщество до этого дойдет через softmax1 или иным образом
339,2024-12-20,closedai назвали новую модельку o3 дабы обойти коллизию имен с o2 британским оператором но не учли что o3 это озон
340,2024-12-25,нет кода квантизаций за последние годы вышло меренонемерено разного качества и степени сжатия в идеале конечно же хочется не просесть по качеству и при этом добиться снижения издержек на инференс llm и ускорения генерации текста академические бенчмарки часто дают чрезмерно оптимистичную оценку не всегда согласующуюся с пользовательским опытом на практике в данной работе авторы исследуют несколько конфигураций квантизации в режиме малой просадки качества и выводят набор практический предписаний в зависимости от задачи и приложения авторы рассматривают 3 варианта квантизации 1 w8a8fp поддерживаемый только на hopperada lovelace 2 w8a8int 3 w4a16 в первом случае веса сжимают просто через квантизацию в остальных применяют оптимизированный с поиском оптимальных скейлов с калибровкой на openplatypus в случае 4битной квантизации весов сравнивают еще и с замеряют качество по трем сериям задач на llama31 8b 70b 405b 1 open llm leaderboard v1 2 open llm leaderboard v2 3 и так называемые realworld задачи arenahard humaneval humaneval на open llm leaderboard v1 все методы в пределах 99 качества исходной модели за исключением задачи trurthfulqa где наблюдается просадка до 3 на open llm leaderboard v2 имеет тоже место почти полное восстановление с наиболее заметной просадкой на bigbenchhard на arenahard humaneval humaneval тоже все хорошо awq бывает немного лучше на open llm leaderboard но заметно уступает на realworld задачах далее смотрят на похожесть генераций на arenahard у исходной модели через rouge1 rougel и bertscore rouge порядка 0607 но по всей видимости это означает большую похожесть для оценки скорости рассматривают разные сценарии 1 длинный промпт короткий выход 2 промпт и выход примерно одной длины 3 короткий промпт длинный выход инференс гоняют на как проверенном и быстром движке замеры гоняют на a6000 a100 h100 рассматривают сценарий сихронной и асинхронной подается батч запросов генерации в синхронном случае почти всегда быстрее всего работает w4a16 квантизация кроме длинного промпта computebound режим и инференса h100 где предпочтительнее fp8 int8 w8a8 квантизация в асинхронном сценарии int4 предпочтительнее на более lowend девайсах а fp8 при инференсе на h100 квантовать модели полезно
341,2024-12-27,наткнулся на красиво и наглядно иллюстрирующий для различных формулировок ganов на 2мерных игрушечных задачах
342,2024-12-28,в начале декабря группа чуваков из глубокого разума среди коих признанные аксакалы как hoogeboom de bortoli и salimans опубликовала презанятнейший пост нынче стало модно учить диффузионки в постановке тренд по всей видимости был задан и большинство нынешней sota в картиночной и видео генерации из того что известно flux moviegen hunyuanvideo и что это значит классическая парадигма пережиток истории ан нет в данном блогпосте авторы в деталях анализируют процесс сэмплирования и обучения в стандартной noiseprediction variance preserving ve диффузионной постановке и flow matching и показывают что по сути обе сущности про одно и то же основная разница в коэффициентах при шумесигнале и использовании скорости в качестве выхода нейронной сети вместо шумаx0 и по ходу повествования эквивалентность двух парадигм авторы иллюстрируют с разных сторон сам блогпост содержит красивые иллюстративные визуализации с ползунками кроме того авторы опровергают распространенное мнение что flow matching дает непременно более прямые траектории чем диффузия для узких распределений flow matching действительно дает более прямые траектории чем типичный диффузионный процесс но для широких распределений все может поменяться с точностью до наоборот впрочем для наиболее типичного сценария text2image генерации или редактирования изображения целевое распределение по всей видимости достаточно узкое
343,2024-12-30,хорошо себя зарекомендовали при дообучении моделей но их выразительности не хватает при обучении модели с нуля ранее было предложено по ходу обучения вливать lora в веса модели статья и инициализировать низкоранговую добавку или накладывать низкоранговость на градиенты и состояния оптимизатора но в первом случае для получения качества близкому к бейзлайну был необходим этап обучения всей модели целиком возникает вопрос и авторы предлагают обучать компоненту идея естесна далеко не нова по существу это своего рода для дообучения llm ранее подобное реализовали в авторы мотивируют использование компоненты в дополнение к тем что она обладает несколько иными свойствами при не слишком малой sparsity матрица почти всегда полноранговая и вместе они могут более гибко фитировать исходные матрицы весов сэмплируется sparse маска и фиксируется по ходу обучения sparse матрица прибавляется к lowrank компоненте потому sparse matrix операции не требуются при такой постановке а для получения градиента достаточно проиндексировать градиент по всему весу авторы обучают семейство моделей с архитектурой llama от 60m до 7b параметров на с4 корпусе на бюджете в несколько миллиардов токенов в качестве бейзлайнов выступают 1 fullrank обучение 2 lowrank training 3 relora 4galore качество оценивают по валидационной перплексии sltrain по качеству чуть лучше relora и galore несколько экономичнее по памяти galore и примерно такой же по скорости рассматривают разные конфигуации sparsitylowrank 13 ненулевых весов в sparse компоненте более менее оптимально анализируя спектр выученных весов они показывают что первые собственные значения fullrank обученных матриц соотвествуют lowrank компоненте а последующие sparse компоненте то есть сумма более точно описывает спектр при полном обучении идея разумная и логичная но непонятно насколько идея масштабируется на более практически интересные сценарии отсутствуют даже замеры 0шотов на l по всей видимости при таких бюджетах обучения они мало будут отличаться от random guess
344,2024-12-31,прошедший год был насыщенным на события и прогресс в области ии глубокого обучения машинки и разнообразных приложений ключевые моменты и достижения области за 2024 превосходно отметил у себя на канале григорий сапунов со своей стороны могу лишь добавить что уходяший год был интересным и примечательным в том числе и точки зрения техник сжатия и ускорения моделей появились 2битные квантизации которые не приводят llm в полную негодность aqlm quip pvtuning qtip спекулятивный декодинг подарил ряд интересных работ до коих у вашего покорного слуги не дошли руки на разбор но в следующем году планирую наверстать упущенное ряд интересных решений по сжатию активаций и kvкэшей в связи с запросом научного сообщества энтузиастов и простых пользователей на эффективный инференс полагаю что и в следующем году мы увидим немало интересного и в особенности значительные усилия будут потрачены на удешевление цепочек рассуждений аля спасибо всем присутствующим здесь кроме nftботов за то что вы здесь за вашу поддержку и комментарии будем стараться и дальше делать полезный и надеюсь интересный контент быть добру
345,2025-01-02,т первый пост данного года будет несколько комедийного содержания как раз в самый раз для прочтения после нескольких бокалов шампанского или чего покрепче некто выложил на гитхаб презанятнейший опус про обучение тернарной сети в бит выделения памяти на градиенты и состояния оптимизатора товарищи из мелкософта в серии работ про показали что обучая сеть с тернарными весами принимающими значения только 1 0 1 и умноженными на некий скаляр и низкобитными активациями 48 бит можно выжать качество сравнимое с fp обучением при тех же бюджетах обучения однако во время само обучения приходится хранить веса и состояния оптимизатора как для fp модели то есть обучение все равно требует значительных затрат памяти автор данного опуса вспоминая статью замечают что операция умножения якобиана по выходу модели на фиксированный вектор backpropagation потому предлагается делать причем для случая тернарных весов возмущения это 1 0 1 для улучшения сходимости предлагается отбрасывать слишком малые возмущения те своего рода прунить обновление так как на практике мы используем псевдослучайные числа то для параметризации модели достаточно хранить только случайные зерна со всех шагов оптимизации и для обучения gpt3 взяв данные из техрепорта тогда еще closedai еще не совсем closed получают шагов оптимизации и всего несколько мегабайт на хранение 175b весов а как вы будете эти сиды превращать в веса это ваши проблемы предложенный метод валидируют на 4слойной mlp c hidden_size 256 и данный метод о боже даже сходится и выдает космические почти 90 качества на mnist единственный недостаток всей этой красоты в том что авторы не релизнули эффективные кернелы для обучения и инференса что ж поделать не все познали дзен куды и тритона в том числе и пишущий сии строки это наверное самый забавный каламбур на моей памяти в данной области интересно автор сам дошел до этого или воспользовался помощью всесильного оракула в виде llm я в полном восхищении в любом случае
346,2025-01-05,после двух с лишним лет прогона сжатых моделей на бенчах из lmevalharness я задался таки вопросом в нижеприведенных рассуждениях я не планирую погружаться в дебри дискуссий про то что есть agi а чем он не является а лишь сугубо попытаться соотнести академические бенчмарки применению на практике большинство бенчмарков относятся к одной из 2 категорий 1 есть вопрос варианты ответа и тот у которого правдоподобие максимальное выбирается в качестве предсказания модели и сопоставляется с правильным 2 на основе некоторого промпта и инструкции модель генерирует ответ далее происходит парсинг и то что удалось вычленить сопоставляется с тем что надо к первой категории относится пожалуй большинство задач не тот arc и многие другие достоинством такого подхода является прогона так как по сути достаточно одного прогона модели для получения вероятностей токенов ответа при этом общий префикс можно переиспользовать для разных вариантов тк запрос имеет вид условиевариант_ответа где вариант ответа обычно одно слово а то и одна буква в случае кроме того проверить или истинность ответа можно однозначно существенным недостатком данного подхода же является неочевидная связь между умением модели справляться с данными задачами и генерацией текста в свободной форме например в mmlu шаблон имеет следующий вид то есть бенчмарк проверяет то насколько хорошо по контексту модель может угадать букву из этого сложно сделать вывод насколько адекватно она будет писать ответ в свободной форме на те же самые вопросы и результат во многом будет определяться тем насколько модель умеет вписываться в шаблон подобного вида данный вид задач k гораздо ближе к реальным приложениям так по существу представляет ту же самую авторегрессионную генерацию основная сложность в оценке ответов модели в случае ifeval определены некоторые регулярные выражения которые вычленяют ответ скажем решение математической задачи или выполнена ли требуемая инструкция но ввиду высокой вариативности возможных ответов не всегда можно гарантировать обнаружение правильного ответа в alpacaeval arenahard судьей выступает но здесь приходится полагаться на качество судьи который не совершенен и имеет свои biasы при оценке ответа кроме того замеры стоят денег и в конце концов и иные sidebyside comparison где качество оценивают уже человеки такая стратегия оценивает широкий спектр способностей модели и вроде бы ориентирована на конечного пользователя но таким образом можно оценивать уже конечную модель а для промежуточных экспериментов выходит чрезмерно накладно кроме того даже ввиду предпочтений пользователей к более длинными ответам удовлетвоояющим некоторому формату оценка качества моделей и цифры на бенчах могут служить лишь приближением при выборе llmки для своих нужд окончательный выбор стоит делать исходя из целевой задачи протестировав самому на релевантных примерах а в идеале собрать собственный бенч и регулярно его обновлять рекомендую от игоря котенкова на данную тему
347,2025-01-09,забавный и поучительный блогпост про то как llmка claude 35 sonnet итеративно оптимизировала код под запросами вида постановка задачи следующая требуется написать код на питоне для задачи аля рабочий но не самый эффективный код подсчет цифр в числе реализован через приведение его к строке парсинг строки на цифры с последующим их суммированнием claude замечает что возможных вариантов чисел меньше чем всего чисел и предпосчитывает сумму цифр для всех чисел от 1 до 100000 и складывает в byte array кроме того llmка навела немного синтаксического сахара и обернула код в классы и методы ускорение по сравнению с бейзлайном claude кладет сумму цифр в numpy array что дает векторизацию подсчета сумм и дополнительно использует для параллелизации ускорение по сравнению с бейзлайном однако изначальное решение было с багами и его пришлось немного фиксить небольшой и бесполезный рефактор который замедлил код ускорение 4 по сравнению с бейзлайном 4 claude предложил использовать numba с jitcompiler и параллелизмом и для многопоточности ускорение по сравнению с бейзлайном далее автор изначально подает промпт просящий оптимизировать все что только можно и нельзя предлагая даже модельке модель сходу выдает ускорение благодаря numba jit но без и промпт требующий улучшения кода уже более подробный но claude обиделся и код стал медленее ускорение и с багами возможно привык что его обманывают на деньги несколько последующих итераций выдали версии с ускорением и с косяками те превзойти прямолинейную стратегию не удалось топовые llm мощные ассистенты для программиста но за ними пока требуется глаз да глаз и экспертное мнение кожаные мешки все еще нужны пока
348,2025-01-10,в связи с ростом потребности в эффективной по памяти и скорости работе с длинным контекстом особенно с учетом набирающего популярность testtime compute scaling все острее стоит вопрос сжатия kvкэшей данной теме уже посвящено немалое число работ и существует уже и недавно ребята из одной зеленой компании выкатили либу с реализацией разных техник сжатия kvкэшей под названием kvpress в данной либе реализовано несколько простых и популярных техник сжатия кэшей случайный прунинг основанный на нормах токенов несколько подходов основанных на attention скорах причем битность можно задавать послойно при помощи класса самую sota например в области пока еще не завезли увы в репозитории есть ноутбуки с и замерами либа действительно удобная и приятная для использования методы сжатия кэшей можно комбинировать с квантизацией кэшей у лицехватов
349,2025-01-12,для того чтобы работать с большим контекстом в условии ограниченных ресурсов необходимо какнибудь да сжимать kvкэши и квантизация одно их первых что приходит в голову группа исследователей с забавным примечанием the order of authors is determined by flipping a coin из разных мест на послендем icml представила статью со звучным названием kivi терминал оплаты фрукт и птица пишутся подругому предыдущие методы квантизации кэшей обычно квантизовали кэши и ключи и потокенно так как статистики scale и zero при таком подходе можно пересчитывать на лету для 4битной квантизации такой подход работает хорошо но при 2х битных кэшах модель становится полностью негодной дабы эффективно квантизовать кэши авторы анализируют распределение активаций в kvкэшах и обнаруживают что в активациях kпроекций имеют место ярко выраженные выбросы в то время как в v паттерн хаотический перебирая варианты с потокенной и поканальной квантизацией исследователи приходят к выводу что ключи лучше всего квантизовать а values при поканальной квантизации придется значения всех прошлых кэшей при пересчете диапазона потому предлагается хранить неквантизуемый буфер из самых свежим токенов размера 128 и как только буфер наполнится квантизовать данную группу и начать следующий для длинных контекстов накладные расходы от такого буфера небольшие метод валидируют на бенчах из lmeval longbench наборе задач на длинный контекст и где нужно достать вытащить нужный факт из длинного промпта на моделях llama2 mistral наивная квантизация в 2 бита сильно страдает по качеству а предложенный подход работает с совсем небольшой просадкой есть правда маленький нюанс что 2битная квантизация использует размер группы 32 с fp zeropoint что означает на практике 3 бита needleinahaystack решается почти идеально поддержание неквантизованного буфера свежим токенов важно для лучше качества свежие токены влияют сильнее чем далекие в большинстве задач при работе с большими контекстами доминирующим фактором определяющим скорость работы при авторегрессивной генерации становится подгрузка кэшей уже не весов модели за счет их сжатия удается ускорить генерацию до 35 раз сжатие кэшей богоугодное дело а дабы эффективно квантизовать кэши полезно смотреть на распределение того что сжимаешь
350,2025-01-13,есть ли способ в телеграме выбрать себе вид наказания ботами или обменяться с кемто если уж выбирать зло я бы предпочел шлюхоботов вместо дурацких nftботов
351,2025-01-18,наткнулся на проект где в формате красочных постов с визуализациями рассказывают про интересные факты и концепции из математики с красивыми иллюстрациями видосиками ползунками в частности внимания заслуживают жаль что в те времена когда я учился такой красоты еще не было
352,2025-01-19,и снова про квантизацию kvкэшей про это тему можно говорить бесконечно примерно в одно время с другая команда выпустила статью которая так же целится в сохранение приемлемого качества при квантизации ниже 4бит
353,2025-01-19,1 первое наблюдение полностью идентично kivi обладают ярко выраженными выбросами а нет потому ключи квантизуются поканально значения потокенно 2 далее замечают что rope соседние каналы и делает выбросы и channelspecific поведение не столько ярко выраженным потому предлагают квантизовать ключи до применения rope и каждый раз применять его уже к квантизованным кэшам 3 однородная квантизация приятна и удобна в использовании но обычно далека от оптимальной в плане ошибки и не мудрствуя лукаво предлагают использовать но используя в качестве метрики не стандарную евклидову а диагональ которую оценивают по маленькой выборке 4 некоторые значения сильно выбиваются среди остальных потому их предлагается не квантизовать а держать в виде разреженной матрицы csr csc на практике берут 1 самых больших по модулю значений 5 модели чувствительны к изменениям отличающихся большими нормами активаций потому предлагается их не квантизовать все равно их мало и на общей битности не скажется не квантизуют только первый токен 6 диапазона значений на лету дорого и для поканального квантования требует пересчета значений всех токенов по этой причине предлагают калибровку констант квантизации для ключей проводить offline на небольшой выборке а потокенную квантизацию кэшей на лету и для всей этой навороченной красоты пишут что работало все с адекватной скоростью метод валидируют на перплексии на стандартной длине контекста на 1 в 4 бита просадка порядка 001 ppl в 3 бита просадка порядка 01 ppl в 2 бита десятые затем метод проверяют на версиях llama2 адаптированных под где kvquant почти не просаживает метрики в 4 битах и умеренно в 3 бита на passkey retrieval метод не просаживается по качеству в отличие от kivi далее kvquant прогоняют на longbench где одна из задач иголка в сене вытащить нужный факт из далекого прошлого популярных бенчах на длинный контекст kvquant снова лучше kivi на ruler kivi эффективно 3битный заметно страдает 5640 но kvquant теряет 3 качества 5653 метод совместим с квантизацией весов так как авторы kvquant авторы еще и то совмещают именно с этим подходом квантизация кэшей почти не меняет метрики квантизованной модели в ablation показывают что все компоненты важны для успеха 1 выбор размерностей для квантизации 2 prerope квантизация 3 неоднородная квантизация 4 учет выбросов итоговое ускорение операций умножения в kv не endtoend latency при 2битной квантизации удается впихнуть на одну a100 1m токенов для llama27b и на 8 10m токенов метод композитный солидный заточенный под выбивание хорошего качества однако сложность имеет свою цену и в данном случае это достаточно дорогая деквантизация изза которой ускорение инференса меньше чем у конкуретных подходов того же kivi
354,2025-01-20,выкатили пару часов назад на лицехватах веса в публичный доступ напомню что это reasoning модель под цепоцки рассуждений аля o1 o1mini o3 в модели 685b параметров и веса выложены в fp8e4m3 архитектура почти идентична так что счастливые обладатели 81 h100 развлекайтесь на здоровье
355,2025-01-26,deepmind не часто публикует код данная статья уже появлялась на и была разобрана у тем не менее выскажу свое скромное мнение уже продемонстрировал впечатляющие результаты в контексте где длинные цепочки рассуждений позволяют значительно улучшать качество на сложных задачах у диффузионных моделей механизм улучшения качества генераций за счет большего объема вычислений есть из выбор с ростом количества шагов расшумления качество полученных генераций и их соответствие запросу но начиная с какогото момента происходит насыщение и дальнейшее повышение не приводит к значимым улучшениям а иногда даже поэтому в данной статье предлагают улучшать генерации за счет сэмплирования начальных точек в процессе генерации и выборе лучшего
356,2025-01-26,в статье рассматривают две постановки 1 classconditional генерация какойто трансформер с приблудами 2 2 генерация c в данной статье исследуют разные стратегии отбора лучших сэмплов и модели для оценки качества 1 просто сэмплируем независимо n кандидатов и берем лучшего с точки зрения моделиоценщика 2 стартуя со случайного шума сэмплируем несколько шумов в его окрестности оцениваем их находим лучший и используем в качестве начальной точки на новой итерации градиентная оптимизация требует проброса градиентов через всю цепочку сэмплирования потому очень дорогая и не очень хорошо работает как показано в приложении 3 сэмплируем несколько начальных шумов траекторий и с некоторого уровня шума генерируем несколько конечных сэмплов отбираем лучшие для каждой траектории зашумляем до меньшего уровня шума и запускаем генерацию уже оттуда в качестве для оценки качества classconditional генерации используют 1 inception score напрямую 2 clip где эмбеддят класс в промпт вида a photo of class 3 линейный классификатор поверх dinov2 при фиксированном достаточно большом числе шагов увеличивают метрика оценивающая точность распознавания сгенерированного изображения inceptionv3 монотонно растет с увеличением количества сэмплов для supervised классификаторов ожидаемо сильнее однако fid тоже хреновая метрика к слову начиная с какогото момента начинает расти те ухудшаться по всей видимости это связано с тем что строгий отбор снижает разнообразие генераций и имеет место переобучение под верификаторы в качестве альтернативы авторы предлагают верификаторы косинусную близость между логитами классификаторов x0предсказания на малом уровне шума и конечного сэмпла и показывают что она неплохо коррелирует с исходными классификаторами метрика не самая интуитивная предположительно идея в том что если сэмпл хороший получается то на последнем участке генерации x0предсказание слабо меняется далее пробуют разные стратегии отбора увеличивая число кандидатов метрики но будто бы результаты мало зависят от гиперпараметров каждого из вариантов в случае zeroorder search и для search over paths
357,2025-01-26,для генерации с flux в качестве верификаторов используют 1 классификатор эстетичности поверх openclip 2 clip score 3 4 ансамбль всех трех с одинаковыми весами fluxом гененируют по умолчанию в шагов сэмплирования sit в для оценки качества не доверяя субъективного мнению используют которая оценивает качество по 5 аспектам accuracy to prompt соответствие запросу originality оригинальность visual quality визуальное качество internal consistency внутренняя консистентность emotional resonance для валидации используют и сначала исследуют взаимные корреляции между разными верификаторами на drawbench коррелирует с но слегка понижает clip score общее качество по мнению gemini немного растет clip score повышает imagereward но слегка понижает эстетичность общее качество слегка улучшается imagereward повышает clip score и оценку gemini более существенно эстетичность почти не меняется ансамбль всех трех метрик улучшает все три метрики при увеличении количества сэмплов в ансамбле все метрики растут но значительнее всего вероятно изза шкалы на максимуме ставят суммарное число прогонов через модель 96 сэмплов х 30 шагов весьма забавно что лучше всего себя показывает наивная стратегия независимой генерации случайных шумов а остальные две более заумные стратегии не приносят пользы на t2icompbench где оценивается соответствие промпту взаимоотношения между объектами формы их положение а не эстетичность классификатор эстетичности слегка просаживает метрики а clip score и imagereward улучшает причем imagereward лучше ансамбля по всей видимости изза наличия aesthetic score предложенный метод работает и поверх dpo дообученной sdxl при фиксированном бюджете если он мал для sitxl выгоднее нагенерить с меньшим количеством шагов чем одну дорогую но с ростом доступного бюджета становится полезным повышать кроме того на малых бюджетах меньшие версии sitbl генерирующее лучше чем один прогон через большую sitxl с идейной точки зрения направление исследований довольно интересное однако польза пока еще не столь очевидна как для llm где сложный reasoning процесс позволяет решать сложные задачи недоступные без него а здесь же приводит к некоторому улучшению метрик на текущий момент процесс слишком дорогой для большинства практических приложений ибо ждать несколько минут вместо нескольких секунд для создания немного более хорошей картинки не каждый готов да и стоить сие будет у провайдеров недешево тем не менее подход может быть использован для генерации высококачественной синтетики
358,2025-02-04,var и его производные в прошлом году произвели некоторый ажиотаж показав вполне достойные результаты в задачах и генерации ранее было показано что дообучение диффузионных моделей так называемый sr c generative prior на задачу с произвольными деградациями входных изображений неплохо себя показывает в случае картинок в этой же статье предложили по существу перенести данную идею на varы а именно дообучить classconditional var на задачу super resolution и не особо заморачиваясь с оригинальностью и звучностью назвали свое детище
359,2025-02-04,напомню что в задаче мы хотим с одной стороны повысить разрешение а с другой убрать шумыразмытости и иные возможные дефекты потому по самой постановке задачи надо какимто образом сохранить семантику входного изображения и при этом получить качественную картинку с резкими деталями сочными цветами и без дефектов и дабы достичь поставленной цели авторы предлагают следующее 1 обусловливание на изображение низкого разрешения путем добавления ее эмбеддингов для чего обучают небольший энкодер в префикс последовательности 2 чтобы лучше учитывать пространственные зависимости предлагают scalealign rotary positional encoding sarope адаптивный под каждый масштаб 3 квантованные vae заметно уступают в качестве реконструкций непрерывным аналогам следуя mar обучают небольшую модель diffusion refiner из 6ти трансформерных блоков обусловленную на дискретные токены для получения конечного изображения 4 дабы улучшить качество генераций применяют classifierfree guidance c негативными примерами для этого отбирают из публичных датасетов сэмплы с низкими оценками и на каждом шаге прогоняют с исходным условием плохим условием и считают взвешенную сумму как в стандартном cfg причем cfg повышают постепенно с увеличением разрешения в диффузионных моделях его можно отключить в конце без ущерба для качества имеющиеся в публичном доступе srдатасеты 1к10к примеров потому авторы собирают свой датасет из laion и прочих источников выфильтровывают его до 4 миллионов картинок которые классифицируют по 3к категориям пояснение будет чуть ниже
360,2025-02-04,процедура обучения выглядит следующим образом 1 сначала дообучают rqvae на собранных данных чтобы улучшить качество реконструкций обучается только квантизатор все остальное заморожено 2 в качестве основы для varsr берут предобученный на imagenet var из оригинальной статьи и дообучают на class2image генерацию на 3 классов на своем датасете 3 после этого дообучают полученную модель на целевую image super resolution задачу 4 при обучении на isr в качестве лосса используется взвешенная сумма кроссэнтропии по токенам и диффузионного лосса в рефайнере полученную модель сравнивают с и подходами на стандартных srбенчмарках div2kval realsr drealsr varsr и диффузия уступает gan моделям по классическим метрикам psnr ssim но лучше по более свежим в качественных визуализациях varsr болееменее реалистично восстанавливает изображения при жестких деградациях в то время как альтернативные подходы заметно шакалят в ablation study показывают что все компоненты важны и накидывают качество обусловливание на lr изображение через работает лучше чем controlnet полезен r улучшает метрики хоть и не так сильно понижает ssim psnr но заметно накидывает в современных нейросетевых метриках по userpreference study varsr ganов и диффузий с image prior существенным достоинством var является скорость работы за счет того что выход формируется за один проход через все масштабы большинство из которых почти бесплатные стоит однако заметить что шельмы не сравниваются с ganами и адверсариальными дистилляциями диффузионных моделей которые будут по факту быстрее вполне успешная адаптация var к задаче image sr c рядом нетривиальных архитектурных решений однако процедура обучения содержит слишком много компонент и промежуточных этапов да и сравнение с дистиллами диффузионок для sr тактично опущено
361,2025-02-04,гугол выпустил про обучение профилирование и хорошие практики обучения больших языковых моделей на tpu в частности рассматриваются следующие вопросы основы анализа вычислений и оценки сложности архитектура и особенности tpu реализация параллельного обучения обучение на tpu с jax если вы любитель jax и tpu вперед и с песней чуть раньше появилось у epsilon correct
362,2025-02-06,сие творение китайских ку ли биных вызвало бурный ажиотаж подняло высокое цунами хайпа не на шутку встревожило саму и про него не высказался только ленивый но я ленивый и все уже до меня все сказали потому предлагаю ниже подборку материалов из англо и русско язычных источников от автора легендарного блогпоста про deepseekr1 своими руками попытка воспроизвести результаты deep seek сообществом лицехватов так уже и подьехал от антона раззжигаева автора канала на gonzoобзоры ml статей на gonzoобзоры ml статей не преминули упомянуть deepseek и у лекса в
363,2025-02-08,no code no party вопрос о том какая битность весов при qat quantization aware training дает наилучшее качество представляет несомненный интерес как с академической так и практической точки зрения и ранее был рассмотрен в частности в работе где утверждалось что оптимальная битность в районе бит на параметр однако подобные выводы сильно чувствительны к качеству процедуры qat успех серии работ по bitnet намекает что вряд ли все так плохо в низкой битности и в данной работе большой коллектив авторов из меты предлагает пару модификаций улучшения процедуры qat
364,2025-02-08,рассматривают обучение 1 158 2 3 4 битных моделей по существу в работе 2 основные идеи можно какуюто часть времени обучать модель в исходной точности а затем врубить qat оптимальнее всего оказывается 90 учить во float16 а последние 10 те большую часть обучения динамика совпадает с floatingpoint моделью а оставшихся 10 хватает для восстановления качества после сжатия под каждую битность подбирают при бинаризации весов оптимально да больше ничего и не сделаешь брать знак от весов при 158 и 2х битах брать определенную в данной работе сетку небольшую модификацию и lsq при 3х и 4х битах причем важно скейлы квантизации держать как масштабирование работает совсем плохо на низких битностях обрезка непонятно каким образом лучше но все же заметно хуже learnable scales для обучения всего хозяйства используют ste без какихто наворотов кроме того еще пробуют qat на mobilellm не с нуля а с готовых чекпоинтов и это оказывается ожидаемо лучше относительная норма разницы весов претерпевает скачок при переходе от ¾ битной квантизации к более низким битностям из этого авторы делают вывод что при менее агрессивной квантизации модель при qat пытается компенсировать ошибку квантизации а при экстремально низких битностях ищет новую более удачную конфигурацию весов сначала подход проверяют на семействе моделей mobilellm от 125m до 15b битности 158 и 2 оказываются паретооптимальными качество оценивают как средне по нескольким бенчам из lmevalharness затем paretoq проверяют на llama38b где предложенный подход заметно опережает llmqat и ptq подходы даже с aqlm и quip сравниваются учат модель на 30b токенах что не так уж мало но и не так уж много c pvtuning и qtip тактично не сравниваются хитрецы при всей дороговизне pv он задействует все же меньше токенов на тернарной квантизации авторы показывают превосходство над прошлыми qat подходами bitnet при фиксированном бюджете обучения кроме того авторы написали дающий хорошее ускорение при квантизации благодаря чему 2битные модельки оказываются оптимальными с точки зрения баланса между скоростью инференса и качеством подход выглядит разумным и рабочим без rocket science для полного счастья не хватает квантизации активаций и замеров на более сложных задачах
365,2025-02-09,раз уж пост вышел на и братва требует пояснений разберем за последние несколько лет мы наблюдали несколько качественных скачков возможностей llm однако в основе их работы все еще преимущественно лежит задача предсказания следующего токена данная незамысловатая задача позволяет решать задачи любой сложности но существенным недостатком является дороговизна инференса когда ради одного несчастного токена приходится загружать всю модель или часть слоев в случае moe в быструю память и сгружать обратно дабы повысить эффективность инференса предлагается незамысловатое решение предсказывать несколько токенов за раз на самом деле такое уже было еще в и в сценарии дообучения однотокенной модели заслуга авторов из меты в том что они исследовали разные варианты предсказания токенов для моделей разного размера
366,2025-02-09,архитектура следующая есть трансформерная тушка и несколько голов каждая из которых предсказывает следующий токен для головы с индексом k если я правильно понял эти головы на самом деле преобразуют эмбеддинг перед подачей в unembedding матрицу из размерности модели в размер словаря а сама unembedding матрица общая для всех токенов обучают на дабы расход памяти не взрывался от тяжелых матриц логитов авторы предлагают делать backward по каждой голове в отдельности в на этапе обучения логиты считают чанками и делают backprop на них к слову обучают семейство моделей размером от 300m to 13b параметров на датасете из 100b токенов какогото кода валидируют на сравнительно простых задачах про код пробуют обучать на сырых байтах и словаре из 32к токенов на маленьких моделях предсказание нескольких токенов вперед работает плохо но начиная с какогото размера 3b становится лучше по бенчам 4 головы отпимальны по качеству для словаря в 32к токенов 8 для байтов далее метод проверяют в сценарии дообучения и сравнивают 3 варианта дообучение на предсказание 1 токена вперед для модели обученной предсказывать 1 токен вперед дообучение на предсказание 1 токена вперед для модели обученной предсказывать 4 токена вперед дообучение на предсказание 4 токена вперед для модели обученной предсказывать 4 токена вперед оказывается что второй вариант работает лучше всего почемуто multitoken prediction работает не очень на задачах вероятно потому что там требуется выдать всего один или немного токенов потом тестируются на синтетике induction heads арифметике многочленов и наблюдают некоторый прирост качества который объясняют тем что в таких задачах полезно смотреть слегка наперед очевидный практический плюс от многотокенного предсказания инференса в 3 раза на bpe токенах и около 6 на байтах mutlitoken prediction выглядит как естественная и рабочая история тем более что в нашумевшем где использовалась модифицированная версия метода с трансфорнеыми блоками на каждый новый токен данная стратегия тоже отлично завелась вероятно она будет стандартной в будущих моделях ждем 4 qwen3
367,2025-02-10,от стартап в области безопасности ии ищет инженера llm оптимизация и rl alignment дообучение и оценка sota llm аттаки на blackbox модели улучшение rl для атак на модели настройки моделей ppo rlhf стабильность обучения бенчмаркинг и оценка качества моделей eloметрики alignment оптимизация инференса vllm sglang trt опыт работы с llm архитектуры rl alignment знание pytorchjax реальная практика с rl методами dpo rlhf плюс опыт с системами инференса vllm kuber docker публикации в neuripsicmliclr и др сильный плюс экспертиза в байесовской оптимизации эволюционных алгоритмах гиперпараметрическом поиске автоматической оптимизации промптов зарплата 80k130k usd опционы релокация в париж полная занятость работа с передовым стеком ai research model alignment
368,2025-02-11,уважаемые коллеги из ist в частности выпустили статью про стабильное обучение моделей с квантизованными весами и активациями статей с той же аббревиатурой пруд пруди на архиве и но эта ранее уже неоднократно поднимался в том числе и вопрос о том в какой точности оптимально учить модель веса и активации дабы получить наилучшее качество при заданном размере через ptq или qat ранее утверждали что 67 бит оптимально при квантизации весов и активаций в intxfpx но сама процедура была незамысловата и нет гарантий что нельзя пробить существенно паретофронт свежие результаты смотри показывают что в fp4 тоже можно эффективно обучать в данной же статье авторам удается достичь паретооптимальности в и стабильного обучения в уже не оптимального но на одном уровне с fp16
369,2025-02-11,основным затруднением при оптимизации весов и активаций в низкой точности является высокая степень шума напомню что операция квантизации недифференцируема и дабы все можно было оптимизировать градиентными методами применяют под названием где градиент просто пробрасывается через недифференцируемую операцию как будто вместо нее стоит функция yx но при низких битностнях такая оценка сильно расходится от истинного градиента и не сходится нормально авторы формулируют задачу оптимизации ste как минимизацию между оцененным псевдоградиентом и истинным предполагая гладкость функции потерь по весам можно оценить ошибку градиента как константа на ошибку квантизации весов веса модели разделяют на 2 группы с ошибкой квантизации ниже и выше некоторого порога на шаге оптимизации учитывают только градиенты от весов с ошибкой заданного порога ибо вторые как раз и вносят шум и нестабильность в обучение далее дабы работать с более весовактиваций которое проще квантизовать применяют пару трюков 1 чтобы привести распределение в более удобо квантизуемый вид применяют известный старым читателям канала трюк вращения адамаровыми матрицами как весов и активаций в результате получают чтото близкое к гауссиане 2 дабы привести все приблизительно к применяют к результату шага 1 а для можно уже численно найти оптимальный скейлинг фактор для решетки квантизации и пользоваться им то есть в итоге алгоритм выглядит следующим образом на прямом проходе вращаем и нормализуем веса сохраняя адамаровы матрицы на обратном проходе применяем обратное адамарово преобразование и маскируем градиент в ablation показывают что отбрасывание градиентов по шумным весам в связке с дает хорошую близость с истинным градиентом в то время как vanilla ste и без адамара корреляция низкая
370,2025-02-11,обучают семейство моделей размером от параметров архитектуры 2 на c4 с болееменее стандартным рецептом обучения по умолчанию отношение количества данных к числу параметров как меру качества берут перплексию на отложенной выборке quest работает лучше чем и бейзлайны далее фитируют который отличается от оригинального из статьи про шиншиллу фактором поправкой на битность параметра effp 1 для fp16 исходя из эмпирических графиков лосса для моделей разного размера и битности получают паретооптимальность в 4х битах 4 битный параметр эффективно равен 07 fp16 но модельто при этом в 4 раза меньше отсюда выигрыш в раз по эффективности метод пробуют на и где все тоже неплохо заводится в ablation показывают что адамаровы вращения дают некоторый прирост качества на w1a1 и w2a2 при этом лишь немного замедляя инференс как мне кажется довольно обнадеживающий экспериментальный результат все больше и больше подтверждений тому что следует с самого начала учить в низкой точности дабы нищеброды потом уже не просили униженно gguf gptq или awq кванты интересно можно ли пробить ниже еще порог паретооптимальности по битностям весовактиваций или мы упираемся уже в некий потолок ждем экспериментов на большем масшабе глядишь meta qwen или deepseek порадуют
371,2025-02-14,рай выглядит так не мне привалило такое счастье
372,2025-02-15,matryoshka quantization код есть но мы вам его не покажем большинство методов квантизации готовят модель в некоторой заданной битности и если хочется иметь квантизованные модели разной степени сжатия приходится прогонять алгоритм несколько раз и хранить гдето всю полученную пачку команда из на днях выкатила статейку по квантизации с примечательным названием которая за один присест готовит квантизованные модельки в 24 и 8 бит примечательно что один из авторов kusupati ранее публиковал другую работу про матрешки
373,2025-02-15,matquant по постановке поверх о с обучаемыми непрерывными или дискретными параметрами основная идея работы в том что для целочисленных типов данных отстругивая младшие биты от представления с максимальной бытностью возможно получать приближения разной точности вложенные друг в друга как матрешка но если просто огрублять скажем int8 квантизацию работает не очень поэтому предлагается совместно оптимизировать разные битности одновременно в одном батче с разными весами matquant применяют поверх в котором оптимизируются скейлы и biasы в квантизации через поблочную дистилляцию и которая суть просто обучение с кроссэнтропийным лоссом с ste через недифференцируемую операцию квантизации метод валидируют преимущественно на gemma 2 2b и 9b моделях и мистрале 7b полагаю что и квены не рассматривают из политических соображений негоже поганых парнокопытных от меты лапать как и китайскую продукцию omniquant оптимизируют на 10 20m токенах из c4 qat на 100 m токенах причем в большинстве экспериментов квантизуют качество оценивают по перплексии и 0shots на lmeval из высокой битности работает сильно плохо на 2 битах оптимизация под конкретную битность получше когда храним много моделей но все равно не очень предложенный подход еще чуть получше просадки довольно значительные 13 15 для 2b 612 для 9b gemma 2 модели если бы квантизовались все слои 2 бита были бы паретооптимальными ибо точность квантизованной 9b модели равна несжатой 2b но увы нет полагаю что подобный результат можно выжать более дешевыми quip без решетки и 1мерным higgs как и efficientqat в ablation показывают что в разными битностями накидывает немного в качестве веса лоссов при разных битностых перебирают по сетке при квантизации attention слоев метод тоже лучше бейзлайнов но просадки становятся еще значительнее с 74 до 47 остальные методы работают чуть лучше рандома в 2 битах название работы красивое и мотивация хорошая но результаты все же не слишком впечатляют также утверждение что они первые кто об этом задумался не соответствует действительности была такая работа oral прошлого iclr где разные битности получали через squeezellm кроме того вложенные представления разной точности можно получать через
374,2025-02-16,как получить sota метод квантизации в 2 и менее бит
375,2025-02-17,вот как надо евалить модельки а не все эти ваши никчемные mmlu перплексия lmevalharness
376,2025-02-17,кроме того еще оценивают качество модели по тому может ли квантизованная deepseekr1 написать flappybird в pygame с 3х попыток
377,2025-02-18,на текущий момент занимают доминирующее положение среди языковых моделей а диффузия блистает в других приложениях ранее уже предпринимались попытки создания языковых моделей на основе но получить чтолибо конкурентоспособное тем же флагманским открытым моделям не удавалось в данной работе коллектив авторов из поднебесной произвел не свет ь с качеством якобы на уровне llama23 и модель называют не абы как а в честь легендарной автомобильной марки llada
378,2025-02-18,предложенный процесс не является диффузией в привычном понимании а с разной степенью зашумления токенами задача языкового моделирования обычно оперирует в терминах дискретных токенов имея некий исходный текст предлагается зашумить некую долю от 0 до 1 токенов заменив их на а задача модели прямо как старый добрый только доля зашумленных токенов переменная обучение состоит из pretrain и sft на pretrain зашумляют токены на любых позициях на sft только в ответе промпт не трогают rl как я понял пока не осилили инференс выглядит следующим образом стартуя с большого количества токенов предсказываем токены которые стоят на этой позиции прогон за один раз работает не очень потому делаем процедуру итеративно снова зашумляя уже меньший процент токенов выбирать токены можно случайно а можно брвть те где модель наиболее уверена а остальные снова зашумлять перед следующей итерацией чем шагов диффузии маскированиядемаскирования тем ожидается что качесво обучают две модели размера 1b и 8b на некотором собственноручно собранном корпусе данных замеряют на стандартном наборе задач из lmeval и humaneval кодинг бенче при прочих равных архитектуре и размере модели затраченном на обучение бюджете диффузия на большинстве задач не хуже авторегрессионного бейзлайна кроме piqa полученная 8b модель посильнее llama 2 7b но все же уступает llama 3 8b и qwen25 7b справедливости ради стоить заметить что обучали всего на 27t токенах что по нынешним меркам немного всего лишь 013 миллионов h800 gpu часов авторы утверждают что в плане диффузионки сильно менее эффективные по сравнению с авторегрессионными но бенчмарки от этого не сильно страдают и напоследок показывают что диффузионные модели гораздо лучше умеют предсказывать прошлое прошлые строки на основе последующих в стихах чем авторегрессионные llm непонятно зачем правда попытки уйти от стандартной парадигмы всегда выглядят интересно пока еще сильно далеко от sota llm да и протокол замеров не исчерпывающий но тем не менее кажется что история скольлибо жизнеспособная сильно не хватает замеров скорости и экспериментов по tradeoff между скоростью генерации и качеством результатов ждем выпуска моделей и кода дабы самим заценить
379,2025-02-18,сообщество сегодня обсуждает релиз от grok3 от но в этот прекрасный произошло еще одно не менее важное и знаменательное событие нас уже перевалило за 2к изначально когда создавал канал я даже и не мог помыслить о такой аудитории думал что буду как гаражный музыкант с парой десятков близких знакомых и энтузиастов спасибо всем за то что вы здесь за комментарии и ссылки на релевантные статьи желаю и далее в силу мер и возможности публиковать обзорчики мемасики про сжатие и не только
380,2025-02-24,claude 37 sonnet что ты можешь делать могу ризонить а что еще могу не ризонить
381,2025-02-25,в качестве несложного упражнения читателю предоставляется самому написать кернел
382,2025-02-27,вот недавно мы обсуждали llada и жизнеспособности диффузионной парадигмы а тут обьявили о создании diffusion llm которая якобы способна бодаться по качеству в бенчах приводят только код с вполне себе сильными closedsource llm без рызонинга при этом она якобы на порядок быстрее небольших авторегресионных llm давая космические более 1000 токенов в секунду на одной а не специализированных чипах утверждается что оно могет еще и в rag tools use и агентность у них и есть можно потыкаться
383,2025-03-01,кода нет давно хотел коснуться темы ускорения инференса авторегрессионных моделей за счет но все никак не доходили руки и наконец дошли команда исследователей из 00 среди которых небезызвестный автор канала представила метод ускоряющий за счет оценки и принятия по сравнению с базовым подходом при этом практически без просадки в качестве
384,2025-03-01,есть llm а есть не очень большие большие обычно работают лучше при прочих равных но требуют большей памяти объема вычислений и времени на шаг инференса инференс llm обыкновенно скорость упирается во время загрузкисгрузки весов из памяти в кэши а не сами вычисления то есть за один трансфер памяти потенциально можно было бы делать больше арифметических операций без существенного замедления при этом в большинстве случаев разница между более слабой моделью называемой те черновой и некой большей таргет моделью невелика отсюда возникает идея более слабой моделью llama318b а затем проверять более жирной llama31405b с тем же словарем последнее требование не обязательно но упрощает постановку за один шаг можно проверить несколько токенов малой модели и если все предсказания совпали то есть наиболее вероятные токены у большой модели такие же как у меньшой принять все иначе до последнего принятого токена таким образом можно добиться генерации идентичной выдаваемой большой моделью за меньшее время данную идею предложили еще достаточно давно в статье еще в далеком 2022 но есть на практике предсказания драфтмодели и таргетмодели расходятся очень быстро всего через несколько токенов поэтому слишком большого ускорения достичь не удается обычно в районе при этом для получения правильного ответа совершенно необязательно чтобы у меньшей модели совпадали с большей она может сказать то же самое но другими словами используя синонимы например
385,2025-03-01,авторы замечают следующее таргетмодель может за токена отвергнуть корректное решение драфт модели если в качестве драфта использовать даже более сильную модель gpt4o для llama31405b acceptance rate все еще не слишком высок если драфт модель ошиблась таргетмодель пытается корректировать ее the capital of france is berlin no just kidding the capital of france is actually paris отсюда предлагается какимто образом оценивать важность токенов на конечный ответ и принимать неважные токены даже если их отвергла большая модель датасет для обучения собирают следующим образом отбирают примеры из alpaca arc только промпты генерируют продолжения разными моделями ми мистралями отбирают вручную лучшие примеры на выходе 500 примеров вопрос хороший ответ плохой ответ все токены из правильного ответа и токены до места ошибки в неправильном ответе считаются как положительные примеры полученный датасет несбалансирован и потому в лоссе дается больший вес отрицательным примерам это просто линейная голова поверх признаков на выходе последнего слоя которая учится за полтора часа в качестве драфтмодели берут 38b а таргет 70b 80b качество замеряют на для оценки скорости рассматривают как неоптимальный инференс через трансформерс так и куда более эффективный в качестве бейзлайнов рассматривают и topk принятие те принимаем токен если в top с наибольшей вероятностью предложенный метод по качеству прогону таргет модели и заметно лучше лрафта и topk при этом удается достичь ускорения в 34 раза с gptfast против 2 у стандартного speculative decoding и и до 10 раз при hf инференсе в ablation пробуют применяться в сетапе валидировать метод на humaneval без примеров на код в обучающей выборке работает хуже но все еще лучше чем topk хорошая идея с очевидной практической пользой приятно что обучение требует довольно небольшого компьюта меньше чем у того же eagle интересно как метод себя покажет на более сложных reasoning задачах
386,2025-03-01,судя по всему комменты генерит только драфт модель придется зареджектить
387,2025-03-04,дорогой коллега и товарищ по совместимости автор канала выкатил на блогпост рекомендую всем желающим както осмыслить или переварить происходщее в мире диффузионных моделей а также проследить за прогрессом и трендами в области приятного чтения
388,2025-03-08,новые sota llmки выходят нынче как собаки нерезанные и тем острее встает ребром вопрос о качестве их оценки полностью исчерпывающего протокола замером удовлетворяющего нуждам всех категорий пользователей нет да и не факт что он достижим потому приходится скрепя сердце полагаться на те или иные бенчмарки принятые в литературе или индустрии группа исследователей из mit решила взглянуть критически на популярные бенчмарки и качество ведущих моделей на них и обнаружила следующее ни одна модель не является безупречной и даже sota llm допускают осечки в простых случаях бенчи не без греха в популярном gsm8k 5 условий и решений содержат проблемы у разных моделей разные достоинства o1mini лучше всех из списка в плане общего решения математических задач но sonnet понимает текст reading comprehension далее в блоге авторы приводят примеры забагованных задач с неправильным ответом с ошибками и неоднозначностью в условии где забыли условие задачи кроме того на днях те же авторы опубликовали почищенный тестсет gsm8k под названием и показали что ведущие llm допускают на нем гораздо меньше ошибок по сравнению с оригинальной версией от openai мораль проста ежели у вас есть штангенциркуль смотрите чтобы у него не поехала шкала у проекта есть классный дашборд с визуализациями ошибок моделей
389,2025-03-11,у есть очень занятный где можно посмотреть на условия задач долю ошибок моделей из лидерборда на данной задаче решения задач полученные разными моделями задачи делятся на легкие средние и сложные легкие ожидаемо решаются болееменее всеми а вот те что потяжелее только ризонерами и claude 35 37o3mini пока нет в лидерборде некоторые задачи из раздела arc не решаются пока ни одной моделью из представленных
390,2025-03-13,какойто кореец бегает по репозиториям разных проектов про квантизацию llmок с однотипными жалобами quest qtip greenbitai интересно это реальный человек загорелся желанием все потыкать или llmка которая начала подозревать что ее сжимают и ходит неприкаянная по гитхабу в поисках лечения
391,2025-03-14,развлечения ради квантанул свежевышедшую через генномодицированной версией с парой примбамбасов картиночную овскую тушку не трогал она все равно маленькая евалы на бенчах надеюсь добавить позднее но вроде отвечает связно на паре vlm примерчиков 200325 метрики на openllm leaderboard v1 добавил
392,2025-03-20,маленький для человека огромный скачок для человечества я контрибутор мистраля
393,2025-03-20,нет кода но можно попросить sonnet 37 написать в диффузионных моделях как в классической постановке так и популярном нынче генерация осуществляется путем итеративного перевода некоторого случайного шума в сэмпл из распределения реальных данных кроме самого зашумленного сэмпла на вход генеративной модели обычно подается скаляр сила наложенного шума который отображается в некоторый многомерный вектор модулирующий тем или иным образом обычно scaleshift карты активации внутри данное архитектурное решение както устаканилось и народ както особо не задумывался о его необходимости а авторы из mit среди которых автор резнета сегодняшней статьи задумались и попробовали убрать обусловливание на шум
394,2025-03-20,мотивация следующая есть в контексте image restoration которые получают шум на инференсе различной неизвестной заранее силы и вполне успешно работают следовательно и в диффузионной постановке сеть потенциально должна уметь на основе самого шума оценивать его величину далее показывают красивый график с и оказывается что оно в большинстве случаев довольно узкое те по самой картинке довольно точно можно понять насколько сильно ее зашумили за исключением высоких степеней шума ниже приводятся некоторые оценки на ошибку между солвером без и с обусловливанием и типа расхождение не очень велико сравнивают диффузионки в следующих постановках диффузионный процесс ddpm edm flow matching uedm предложенная модификация edm без обусловливания на время датасеты cifar10 ffhq качество оценивают по fid модели учат с одинаковым протоколом обучения для возможности честного сравнения в большинстве случаев просадка от исчезновения условия на время не очень велика за исключением ddim c детерминистическим сэмплером где сильно ухудшился fid для даже наблюдается некоторое улучшение данное явление авторы объясняют тем что с одной стороны сама процедура обучения немного другая оценка flow field между распределениями и таргет не содержит в себе времени кроме того пробуют альтернативные варианты например учить сеть саму предсказывать уровень шума но будто бы это ничего не дает если рассматривать работу диффузионной модели как где каждая задача расшумление при заданном уровне шума то при стандартном подходе у нас есть некоторая общая база и небольшое число параметров специфичных для данного шага с одной стороны можно увеличить специализацию как в ediff и иметь отдельные сети на каждый отрезок здесь же наоборот по сути предлагают иметь полностью идентичную модель на все уровни шума практическая польза как будто может быть при flowmatching постановке но непонятно насколько выигрыш переносится на большой сетап
395,2025-03-24,диффузионные модели на протяжении последних несколько лет удерживают пальму первенства среди семейств генеративных моделей во многих приложениях однако их фундаментальным ограничением является многошаговое сэмплирование изза чего генерация занимает много времени научное сообщество предложило вагон и маленькую тележку различных процедур дистилляции многошаговых моделей в малошаговые но при приближении количества шагов к 1му качество неизбежно просаживается даже для самых продвинутых подходов отсюда мысль а что если ускорить генерацию за счет мы с коллегами из yandex research предложили метод дистилляции в несколькошаговую генерацию где разрешение изображения увеличивается постепенно на каждом шаге генерации swd за счет этого удается достичь более чем 2х кратного ускорения по сравнению с эквивалентной дистилляцией в фиксированном разрешении
396,2025-03-24,на первых шагах расшумления изображение представляет собой где сложно чтолибо различить а не последних была установлена связь между и процессом диффузии первые шаги соответствуют низким частотам а в самом конце формируются высокие частоты то есть вначале мелкие и тонкие детали все равно неразличимы поверх шума можно попробовать генерировать сначала в низком разрешении а потом постепенно повышать интерполяцией но как исходная модель может быть не адаптирована под малогашовую генерацию sdxl в 256x256 выдает безумный попарт на любой запрос да и процедура интерполяции зашумленного латента плохо определена наивный upsampling латентов приводит к сильные дефектам несколько лучше работает upsampling x0 оценки из предсказанного латента с последующим зашумлением назад но все равно не здорово потому дообучение неизбежно при этом дистиллированные модели выполняют две роли одновременно уменьшение количества шагов сэмплирования и superresolution в латентном пространстве в качестве основы для процедуры дистилляции берется как все еще stateoftheart метод дистилляции дополнительно к нему навешивается предложенный лосс который стремится уравнять прогнанные через исходную диффузионную модель генерации моделиучителя и студента на уровне отдельных токенов итоговый лосс содержит в себе обратную klдивергенцию gan лосс pdm лосс в качестве данных для обучения используется синтетика сгенерированная модельюучителем примерно 500к сэмплов
397,2025-03-24,предложенный метод дистилляции валидируется на моделях семейства sd35 medium и large для оценки качества смотрим как на классические метрики fidclipimagerewardpickscore на cocomjhq30k так и пользовательские предпочтения процедура обучения длится примерно все на одной a100 ноде для малошагового генератора и fake модели из dmd2 обучаем lora адаптеры те вполне достаточно для 4шагового сэмплирования используется следующая последовательность разрешений для 6шагового сэмплирования используется следующая последовательность разрешений шаги сэмплирования в расписании подбираются специально так чтобы соответствовать моменту возникновения заданных частот для данного разрешения scalewise диффузия практически не просаживается по качеству по сравнению с генерацией в фиксированном конечном разрешении при этом при фиксированном бюджете генерации scalewise строго лучше 4шаговая scalewise против 2шаговой fullscale по большинству метрик swd не проседает по сравнению с исходной моделью за исключением fid за счет потери разнообразия по userpreference паритет по релевантности дефектам и некоторое даже улучшение по эстетикекомплексности в том числе и бьем switti нашу прошлую работу про scalewise авторегрессию swd позволяет выдавать почти в секунду для sd35medium и для sd35large при генерации с батчом 8 в ablation показываем что компоненты метода важны обучение на синтетике правильно подобранное расписание шума адаптация модели под каждый скейл pdm лосс очень важен замена l2 лосса на более сложный kernel rbf между признаками учителя и студента не дает улучшений scalewise подход генерации изображений кажется эффективной и хорошо мотивированной идеей в генеративных задачах от грубых простых деталей постепенно переходим к сложному ранее такая идея была воплощена в контексте var каскадных диффузий латентную диффузию как оказывается можно достаточно быстро перевести в режим генерации от мала до велика альтернативный подход с использованием крупных патчей в dit на первых шагах был предложен командой из meta с небезызвестным артемом из
398,2025-03-24,deepseekv3 вышло и это примерно пока все что известно есть метрики и прирост вполне существенный
399,2025-03-27,для рецензентов на peer reviewed конференциях интересно если подать данную страничку vlmке перед прочтением рецензий то их качество на neuripsicmliclr возрастет а заодно можно еще и вкорячить в картинки blackbox адверсариальную атаку
400,2025-03-30,где код билли во многом успех современного глубокого обучения обусловлен масштабированием моделей и времени обучения стандартный пайплайн обучения включает в себя предобучение на большом объеме данных с последующим дообучением на куда меньшем количестве примеров высокого качества в текущей практике обыкновенно качество базовой модели напрямую транслируется в качество дообученной на инструкциях а так как качество базовой монотонно растет то кажется логичным продолжать дальнейшее наращивание бюджетов обучения однако группа исследователей часть из них ранее засветилась в обнаружила что начиная с какогото момента чекпоинты olmo начинают проседать по качеству при файтьюнинге решила поглубже исследовать данный вопрос и найти какоето объяснение явлению перетренированности
401,2025-03-30,напомню что это полностью открытый проект по обучению больших языковых моделей с открытым исходным кодом и выложенными промежуточными чекпоинтами давнымдавно что при фиксированном количестве flops на обучение оптимальное качество достигается при отношении количества токенов d к параметрам модели n около но жирные модельки потом дорого гонять на железе потому обыкновенно выходят далеко за данное отношение и качество все равно монотонно растет хоть и не так быстро как при отпимальном скейлинге в частности для llama3 отношение авторы взяли веса моделей с разных шагов обучения и обнаружили что начиная с какогото момента качество на некоторых задачах при дообучении начинает убывать при этом качество базовой модели непрерывно растет для 7b модели при этом такого явления не наблюдается так как модель по всей видимости еще не успела перейти в режим перетренированности исследователи делают гипотезу что данное явление следствие повышенной чувствительности параметров к шуму и пертурбациям дабы проверить данную гипотезу авторы обучают на корпусе с4 модели от 15m до 90m параметров с подвыборками размером от 4b до 128b токенов сначала авторы исследуют зависимость лосса модели от времени обучения при фиксированной величине пертурбаций параметров первоначально лосс модели не сильно меняется даже при большой величине шума но начиная с какогото момента при сильном зашумлении лосс зашумленной модели начинает возрастать несмотря на то что качество исходной модели монотонно улучшается то есть просадка модели вызванная шумом растет быстрее чем улучшается модель дообучение конечно сильно отличается от добавления гауссового шума но тем не менее оказывается что на некоторых датасетах при лернинг рейте побольше начиная с какогото момента качество падает далее авторы пробуют у я чтобы учесть большую чувствительность параметров модели и это помогает на id in domain задачах но качество на ood out of domain все равно может ухудшаться при дальнейшем обучении обучения откладывает проблему на более позднее время но не решает ее полностью далее авторы рассматривают таргет генерируется линейным слоем а обучается композиция двух слоев без активаций где наблюдается тот же самый эффект растущая чувствительность весов модели к зашумлению в процессе обучения постепенно выучиваются все меньшие и меньшие сингулярные значения целевой матрицы и в тот момент когда шум превосходит одно из них ошибка может начать расти в конце авторы вспоминают про комментируя что обнаруженное явление имеет по всей видимости ту же самую природу довольно любопытное и интересное наблюдение для полноценной валидации требуется репродукция другими командами ибо результат может быть обусловлен такими нюансами как детали оптимизации архитектуры mixedprecision настроек и выбора данных потенциально еще причина может быть в специфике оптимизации oм или стремлении модели неограниченно наращивать логиты вероятностей для оптимизации кроссэнтропии использование другого оптимизатора или регуляризации например sam вероятно способно предотвратить проблему было бы интересно еще проверить имеет ли место эффект в других задачах те будет ли ухудшаться качество vision foundation моделей на downstream при очень продолжительном обучении
402,2025-04-01,
403,2025-04-04,attention is not all you need убийца трансформеров новая ступень эволюции архитектур если вам зачемто приспичило отказаться от attention то данный рецепт предлагает решение которое позволит вам превратить трансформер превратить трансформер в элегантную rwkv модель процедура следующая берете трансформер заменяете attention на rwkv 1 сначала пытаетесь добиться того чтобы блок rwkv воспроизвел выход исходного трансформерного 2 потом дистиллируете логиты исходной модели обучая только rwkv 3 потом еще раз дистиллируете разморозив все 4 дообучаете на более длинном контексте бюджет дистилляции всегото несколько сот лямов токенов таким образом получаются на некоторых бенчах сохраняют качество гдето теряют до 5 но хоть както работает и ладно и даже как заявляется опережает не абы что а gpt35 turbo без единого attention слоя 8 gpu это не a100h100 как вы могли подумать а амудшные карточки с 192gb vram
404,2025-04-05,в ожидании аппрува на веса llama 4
405,2025-04-05,кстати с квантизацией llama 4 в оригинальной репе какойто наеб скрипт квантизации правда учитывая что это rtn квантизация которая раскоает модель в негодность в 4х битах да и вроде обещают qat модельки может и пох
406,2025-04-05,сук даже vpn не помог и указание другой страны
407,2025-04-07,за наводку спасибо
408,2025-04-07,презентация с моей сегодняшней лекции про методы сжатия бям на курсе школы анализа данных яндекса эффективные модели в ней даю краткий обзор по существующим подходам актуальным работам в области и некоторые общие рекомендации
409,2025-04-08,релиз несколько затянулся мыши плакали кололись но продолжали грызть кактус но в итоге допинали как появилась полноценная поддержка в vllm релиз квантизованных gptq в 4 бита моделей deepseekr1 и самого кода квантизации моделей речь пока не идет о том что можно запустить у себя на калькуляторе и даже на consumergrade gpu но в одну ноду влезает уже без приседаний и с контекстом достаточным для reasoning задач аля aime gpqa math500 модели на квантизуются все слои в трансформерных блоках квантизуются только nonshared experts код квантизации
410,2025-04-10,для человеческих особей биполярное расстройство считается тяжелой психической болезнью но для больших языковых моделей способность рассуждать в несколько взаимодействующих между собой потоков как будто от лица разных персонажей может быть полезна в контексте решения логических задач коллеги из где ваш покорный слуга выступал скорее в роли моральной поддержки реализовали подход параллельного инференса для llm
411,2025-04-10,современные llm видали всякое в процессе своего обучения но прямых указаний на то что в конкретном месте можно распараллелиться и разбить задачу на части которые можно решать независимо скорее всего особо не встречали тем не менее рассчитывая на то что современные модели достаточно умны подав подходящую инструкцию и организовав нужным образом kvкэш можно рассчитывать на продуктивную коллаборацию потоков благодаря позиционным эмбеддингам можно задавать произвольные позиции токенам в последовательностях расположение токенов в kvкэше в физической памяти организовать кэш можно следующим образом t процессы по очереди пишут общую память и видят чередующиеся шаги друг друга в прошлом процессы видят кэш другого процесса непрерывным блоком в прошлом промежуточный вариант процессы видят перемежающиеся шаги друг друга и текущий логический шаг рассуждения сплошным куском промптинг состоит из 3 компонент 1 системного промпта поясняющего llmке что надо работать в несколько потоков 2 fewshot примеры коллаборации потоков 3 вставка like промптов периодически спрашивающих процесс занимается ли он полезной работой потенциальная польза от использования методов что за меньшее количество прямых проходов по сети инференс llm можно прийти раньше к правильному решению а в идеале за счет и эксперименты прогоняли на как на модели достаточно хорошей и влезающей легко на 1 gpu подход проверяли на модифицированной версии и наборе примеров из для gsm8k сравнивается работа в потока в большинстве случаев многопроцессовый режим работает лучше стандартной генерации при фиксированном бюджете на limo разные опции hogwild кэша с наивным бейзлайном с выдачей ответа одним процессом в заданном формате с early stopping промптингом и независимыми процессами hogwild работает стабильно лучше разные стратегии обработки kvкэша близки по качеству за исключением совсем малых бюджетов комбинированный выглядит немного предпочтительнее альтернатив при бюджете в 8к прямых проходов в 2 потока достигается то же качество что и генерация одним потоком 16к токенов научнотехнический процесс достиг того уровня что llm способны заменять даже не одного а нескольких кожаных мешков те не менее удачная коллаборация происходит на практике далеко не всегда иногда процессы вроде бы договорившись начинают все равно дублировать друг друга или просто теряются в дальнейшем планируется развитие подхода в частности специальное дообучение для улучшения взаимодействия процессов друг с другом
412,2025-04-12,на этой неделе выпустил 3ую версию своего фреймворка для инференса llm на данный момент либа находится на стадии разработки есть куда расти в плане оптимизации неоптимальная утилизация на ampere gpu amd gpu если у кого есть такие не поддерживаются планируют накатить интеграцию с на данный момент поддерживаются только llama qwen gemma2 mistral архитектуры прошлая версия exllama в качестве метода квантизации брала но в этот раз за основу взяли тяжелую артиллерию среди низкобитных методов квантизации адаптацию тем самым гарантируя качество значительно лучше особенно при экстремальном сжатии в 2 и менее бит по перплексии качество выглядит и правда хорошо интересно было бы оценить на других задачах
413,2025-04-16,старческий склероз это не болезнь а фундаментальное ограничение кожаных мешков на размер контекстного окна
414,2025-04-25,по итогу конференции iclr 2025 проходящей сейчас в сингапуре планирую сделать подборку заинтересовавших постеров по теме efficientdl и смежным вопросам а также разобрать некоторые из них вид в виде лонгридов а пока хотел бы поделиться общими впечатлениями от конференции 1 поездки на такие мероприятия очень полезны с научной точки зрения так как даже пристально следя за областью какието работы упускаешь из внимания и наталкиваешься на интересные и релевантые своему ресерчу на самой конференции 2 после данных мероприятий сильно возрастает мотивация заниматься наукой под впечатлением результатов от других работ обсуждений с авторами возникает желание опробовать приглянувшуюся идейку у себя в коде или эксперименте 3 классная русская тусовка на iclr постоянно встречаешь русскоязычных ребят как из яндекса airi тбанка так и обосновавшихся в зарубежных лабах компаниях и институтах google meta eth nyu и тп и тд 4 китайцы частенько говорят по английски плохо ну прям из ряда вон плохо как читаешь статейки прямо все по маслу такие вычурные обороты достойные джейн остин толкина и байрона а как доходит дело до устной речи не понимают даже вроде бы довольно простые вопросы один даже честно признался что не могет в инглиш и включил google translate с английского на китайский на планшете тем не менее с некоторыми возникали прямо содержательные и интересные обсуждения например с авторами sana и svdquant китайцы наиболее широко представлены среди всех национальностей что впрочем неудивительно 5 имеет место явление что на конференцию подаются несколько статей про одно и тоже на текущем iclr я видел 34 статьи про комбинирование вращения smoothquant для лучшей квантизации весов и активаций и так же 34 статьи про sliding window prefix attention с небольшими концептуальными отличиями 6 в сингапуре жарко снаружи и холодно внутри в помещениях прямо как в баньке с контрастным душем мерзнешь от кондиционеров и бежишь наружу париться и когда напаришься вдоволь идешь охлаждаться 7 все дорого прямо от слова совсем и кофий не чета тому что в москве за 89 баксов наливают бурду которую в условном cofix one price coffee постесняются налить за вчетверо меньшую сумму 8 город богатый и впечатляет возможно меня покусал варламов но небоскребы тут смотрятся както гармонично и естественно в отличие от москвасити 9 с алкашкой грустненько и зожников много везде видишь бегунов и велосипедистов 10 дуриан выглядит классно прям хочется взять к себе домой эту здоровенную шишку но запрещено законом
415,2025-04-29,интересно метод кнута и пряника посингапурски называется методом ротанга и дуриана
416,2025-04-30,как известно всем еще с материнской утробы основной проблемой диффузионных моделей в разных ипостасях является долгая итеративная генерация было придумано множество способов ускорения генерации за счет дистилляции но при стремлении количества шагов к единице качество неизбежно уступает учительской модели в существующих подходах ускорения генерации модель принимает на вход только текущий зашумленный сэмпл и потому подвержена накоплению ошибки при более ранних шагах что может приводить к для желающих лучше разобраться в теме рекомендую посты на concise research и команда из среди которой можно заметить небезывестного артема автора канала и просто хорошего человека предложила подход направленный на решение вышеописанной проблемы
417,2025-04-30,знание всей прошлой траектории сэмплирования при малошаговом сэмплировании потенциально дает модели и позволяет формально существует отображение при детерминированном сэмплере между шумом и конечной генерацией и хотелось бы быть как можно ближе к этой траектории однако возникает вопрос ь и авторы предлагают интересное решение модель на текущем шаге генерации делает на текущий и прошлые шаги сэмплирования с причинной маской прошлые шаги не аттендятся на будущие в модель добавляется на шаг зашумления разные шаги зашумления при генерации получают разные временные эмбеддинги пространственные при этом одинаковы обусловливание на прошлые шаги проводится только в трансформерных блоках c одной стороны attention на прошлые шаги довольно шумный в поздних слоях и даже слегка просаживает качество в то же время self attention только в первых блоках удешевляет forward pass выходы с прошлых шагов можно закэшировать как в дополнительные вычисления возникают только при вычислении непосресдственно self attention в качестве базового метода дистилляции используют простой без progressive где модельученик пытается воспроизвести траекторию учителя для улучшения качества можно дополнительно накинуть на x0 также предлагаются два альтернативных подхода маскирования 1 скользящее окно attention обусловливание на несколько последних шагов 2 attention на текущий сэмпл и начальный шум метод валидируют на для classconditional генерации на imagenet 256x256 и на проприетарной модели для text2image про последнюю известно что это на 17b параметров обученный на некотором проприетарном датасете для дистилляции учителем сэмплируют траекторий при 25 шагах сэмплирования при этом стремясь добиться качественной 4шаговой генерации качество оценивают по fid на каком количестве сэмплов is precision и recall значительно улучшает качество по сравнению с ванильной step distiilation альтернативные варианты масок будто бы чуть хуже по метрикам но возможно не статзначимо сильно улучшает качество и конечная модель имеет даже меньший fid чем учитель в конечном варианте модель учитывает явно прошлые шаги в а далее работает как исходный dit при генерации 256x256 на прошлые шаги несмотря на увеличение количества токенов в selfattention на последнем шаге генерации почти в 4 раза почти не замедляет генерацию endtoend время генерации возрастает только на 2 по сравнению с инференсом использующим только текущий сэмпл однако здесь стоит заметить что для 256x256 последовательность токенов токенов на одно изображение те 1024 на последнем шаге потому вычисление attention сравнительно недорогое и дополнительный благодаря kvкэшам и включению прошлых шагов только в первых блоках трансформера должен быть действительно невелик на text2image генерации качество оценивают на бенчмарке оценивающем релеватность и сравнивают с другими публичными и непубличными дистилированными моделями sdxllightning dmd2 add lcmlora и imagineflash и по fid на mscoco предложенный подход ard достигает самого хорошего качества при 3шаговой генерации слегка опережая 4шаговую dmd2 по метрикам
418,2025-04-30,однако данное сравнение вызывает ряд вопросов 1 разные подходы используют разные модели поэтому невозможно достоверно определить вызвана ли разница превосходством ard метода или тем что базовая emu модель просто лучше бейзлайнов 2 fid считается на 5к сэмплах что может быть недостаточно учитывая шумность fid полагаю что meta не испытывает такую нехватку в ресурсах что им неподьемно посчитать метрики в стандартном протоколе на 30к промптов 3 авторы замечают что на больших моделях просадки метрик меньше и так emu модель меньше sdxl и просадка меньше то якобы подход меньше сажает качество по сравнению с альтернативными дистилляционными подходами данный аргумент не убедителен так как сложность дистилляции определяется рядом факторов таких как обучающие данные архитектура модели unet или dit специфика sft и rl если он был кроме того данная emu модель скорее всего обучалась на flow matching 4 нет чисел по скорости инференса для emu моделей для генерации в 1k где картиночных токенов становится достаточно много на расширенную последовательность перестанет быть настолько безобидным даже при учете того что он возникает только в первых блоках сама идея и реализация выглядит довольно интересной с нетривиальным архитектурными решениями однако протокол сравнения в text2image вызывает смутные сомнения как мне кажется валидация всякого метода должна проводиться в контролируемых условиях когда предложенный метод и альтернативы находятся в равных условиях в данном случае более чем уместно было бы провести эксперименты на публичных моделях sdxl sd35 при сравнении с dmd2 и прочими дистилляциями на тех же самых данных либо уж все на emu а также привести время инференса для дистилированной emu модели и какойто
419,2025-05-01,некоторое время назад еще до iclr мы с ребятами из тбанк research обсудили структуру и организацию исследований в компаниях а также настоящее и будущее мира больших языковых моделей спасибо коллегам за содержательную и интересную дискуссию
420,2025-05-06,люди это несовершенные дистиллы бога
421,2025-05-07,ai модерацию очень сложно мерить нужно учитывать разные виды контента быстро отвечать не false positiвить челы из озаботились и сделали первый бенчмарк для гардрейлов а еще измерили на нем все самые популярные llm в том числе давая моделям поррасуждать над ответом
422,2025-05-07,дело хорошее и обьективно нужное так что честь и хвала ребятам за их труд
423,2025-05-08,fewshot prompting это когда пытаешься разговорить человека пригласив в бар
424,2025-05-14,кажется юдковский решил что слишком много сабмитов в этом году на neurips и положил overleaf дабы тормознуть agi
425,2025-05-17,сильно запоздалый пост изза праздников и neurips дедлайна но все же дошли в итоге руки ниже подборка статей с 1 постерной сессии на которые так или иначе были связаны с в данной статье акцентируют внимание на квантизации mamba архитектур в разных задачах наивная адаптация методов квантизации для трансформерных llm просаживает сильно качество на s6 моделях авторы анализируют проблемные места в квантизации мамб где накапливается большая ошибка и предлагают свое решение whitening преобразование вместо адамара в offline rotations добавление scaling факторов в модель в стиле smoothquant учитывающих специфику mamba для упрощения задачи квантизации метод валидируется на ряде nlpvision задач где показывает заметное улучшение по сравнению с бейзлайнами при квантизации весов и активаций трансформеры нынче sota во многих приложениях однако в некоторых задачах типа определении четности показывают себе плохо а rnn хорошо однако rnn плохо параллелизуются и вообще неэффективно используют ресурсы gpu ребята написали кастомные fused cudatriton кернелы эффективно использующие иерархию памяти для forward и backward и смогли добиться ускорения до раз по сравнению с ванильной торчовой реализацией в данной статье предлагают оценивать эффективность квантизации по тому насколько плотно покрывает сетка квантизации целевое распределение при наивном roundtonearest подходе изза выбросов большая часть объема не используется добавление обучаемых вращений из spinquant и scaling факторов аля smoothquant позволяет более равномерно распределять распределение весов по решетке и тем самым улучшает качество квантования к сожалению на постере не было ни одного из авторов а какойто левый чувак который не особо был в теме потому содержательного разговора не получилось в данной работе авторы ставят своей задачу добиться сжатия сигналов любой природы как можно ближе к теоретикоинформационному пределу для этого обучают автокодировщик маленькую mlp чтобы преобразовывать входные данные с возможными выбросами и широким диапазоном значение в некоторое более регулярное множество и затем проектируют на оптимальную сетку e8 для 8мерной векторной квантизации λ24 для 24мерной квантизации валидируют преимущественно на синтетике когда я спросил авторов про trellisbased квантизацию из который потенциально может быть еще ближе к ratedistortion limit авторы ответили что не знают что это такое идея простая находим последовательность наименее важных блоков в трансформере по косинусной близости по аналогии с пруним вставляем один трансформерный блок ffn и дообучаем немного работает несколько лучше чем просто прунинг блоков кто бы сомневался головы attention делятся на 2 типа которые могут аттендиться на любой токен в последовательности и которые смотрят только на последние токены и attention sinks в начале последовательности для вторых можно сэкономить на вычислениях и памяти храня только кэш для самого начала и некоторого фиксированного количества последних токенов для определения streaming голов маску в каждом attention параметризуют как взвешенную сумму полного causal attention и streaming attention и те головы где коэффициент streaming attention наибольший далее обрабатываются как streaming heads предложенная техника позволяет уменьшить кэш почти вдвое без просадки на longbench задачах
426,2025-05-17,авторы замечают что для прунинга sparsegptwanda выбор данных имеет значение данные из обучающей выборки предпочтительны часть экспериментов делают на своей модели dclm7b но если их нет можно сгенерировать самой сжимаемой llmкой взяв некоторый префикс при генерации выкидывают последовательности с самой большой перплексией далее авторы обнаруживают что сгенерированные данные ближе к обучающей выборке чем варианты калибровочных данных c4wikitext2red pajama бешеного прироста качества нет но улучшение на 051 при 50 24 sparsity консистентно для разных моделей надеюсь
427,2025-05-26,с ростом размера llm затрат на обучение и инференс все более актуальным становится вопрос эффективных вычислений опыт показывает что вполне реально гонять обучение с низкобитными весами и активациями и при хорошей реализации даже оптимально по флопсам однако вычисления в низкой битности требуют аппаратной поддержки в поколении зеленые завезли аппаратную поддержку новых малобитных типов чисел с плавающей точкой с плавающей точкой
428,2025-05-26,основная фича которая обеспечивает стабильность и эффективность низкобитных операций это аппаратная поддержка операций с квантизованными тензорами с малыми группами чем меньше количество весов для которых берется общий масштаб скейл тем точнее их можно аппроксимировать но тем и больше накладные расходы на хранение и операции с ними дабы уменьшить расходы по памяти скейлы хранятся в меньшей точности а операции с ними имеют эффективную кернельную реализацию что обеспечивает малое замедление по сравнению с кватнизацией 1 nvfp4 это fp4 e2m1 с group_size 16 где скейл квантизуется в fp8 e4m3 итого бит на параметр 2 семейство mxfp форматов включает в себя 468бита скейл квантизуется в экзотический e8m0 формат те в логарифмическую шкалу благодаря чему операции со скейлами можно свести к очень дешевым битовым сдвигам размер группы 32 те имеем бит на параметр в формата прогнали эксперименты на ptq и qat visionaudiotext модельках bert и encoderdecoder для перевода с языка на язык наивный ptq каст в mxfp8 работает ожидаемо без просадок в mxfp6mxfp4 имеет место заметное ухудшение но небольшой qat позволяет почти восстановить качество в большинстве случаев до уровня half precision затем авторы гоняют обучение с весамиактивациямиградиентами квантованными в mxfp6 на gptшкам от 20m до 15b и кривая обучения почти совпадает c halfprecision за исключение спайков последующие статьи заслуживающие отдельного разбора и совсем свежая от коллег из ist и в частности показали эффективность обучения в mxfp4 на более серьезных масштабах в первой статье смогли добиться ускорения 17x против bf16 и 13x против fp8 а в quartet 23x против bf16 и 16x против fp8 в качестве удачного внедрения nvfp4 можно вспомнить тоже заслуживающую разбора svdquant где на rtx 5090 смогли добиться 3х кратного ускорения инференса flux переход к fp4 в качестве стандартного типа для обучения кажется делом времени широкого распространения чипов в датацентрах интрига в том в какой момент придется остановиться дойдем ли до полностью тернарных сетей в будущем или они окажутся паретонеоптимальными время покажет
429,2025-05-26,при релизе каждой новой sota llm
430,2025-05-27,коллеги из yandex research выкатили публичный датасет под названием alchemist из 3 c небольшим тысяч картинок собранных из интернета для дообучения диффузионок данный датасет отобранный с помощью довольно занятного пайплайна дает заметный прирост качества на разных моделях в отличие от laionaesthetics и просто фоток анимешных тяночек так что ежели кому нужно заалайнить модельку на качественных данных далеко ходить теперь не надо
431,2025-05-29,прямой да и обратный проход через современную llm подразумевает запуск нескольких сотен кернелов attention mlp нормализаций команда из стэнфорда обнаруживает что скорость инференса маленьких llm 13b параметров упирается не в вычисленияпамять а во время запуска кернелов эффективные движки для инференса vllmsglang позволяют только на 50 использовать пропускную способность новых видеокарт h100gb200 там некоторые операции уже слиты в один вызов кернела но самих вызовов все еще остается много и авторы предлагают реализовать весь forward pass в виде одного megakernel из нюансов реализации стоит выделить следующее 1 так как за shared memory борются сразу несколько процессов надо эффективно распределить ее и раздавать по запросу для это используется некий вариант paging 2 теперь у нас много операций работающих асинхронно и требуется внутри кернела регулировать то чтобы процесс не начал работать пока не будут готовы все необходимые входы те attention не запустился пока не готовы q k v в результате удается добиться ускорения на llama1b при инференсе с батчом 1 25x против vllm 15x против sglang на h100 35x против vllm 25x против sglang на gb200 утилизация ширины памяти для h100 достигает
432,2025-05-31,на data fest 2025 30 мая влад голощапов автор канала представил весьма занимательный доклад про геометрию лосс функции в нейронных сетях данный вопрос неоднократно поднимался в разных работах и был довольно популярной темой для исследования между 20172019 но несмотря на это тема не то что все еще содержит нерешенные вопросы а является непаханым полем и истинная геометрия лоссповерхности во всяком случае в рассмотренных задачах очень далека от типичных предположений доклад будет интересен как теоретикам так и всем увлекающимся ml и оптимизацией чеголибо доклад в интервале 349419
433,2025-06-02,выложили квантизованную в 4 бита модель deepseekr10528 качество при ризонинге сохраняется на 9982 среднее по aimegptqmath500 deepseekr10528 стал ещё более болтливым поэтому для лучших результатов как исходной так и квантизованной модели рекомендуется увеличить контекст до 65к токенов модель на
434,2025-06-07,вообще давно пора было бы смириться с тем фактом что лучше attention ничего нет на свете но человек в своем упрямстве продолжает искать альтернативы более быстрые и не уступающие по качеству и коллектив авторов звезд голливуда в мире ai выкатил статью про очередного убийцу attention
435,2025-06-07,почти с момента выхода attention было предложено много альтернатив с субквадратичной сложностью если убрать softmax в attention операцию можно посчитать за по длине последовательности число операций аналогично ssm mamba12 линейно масштабируются с ростом числа токенов тем не менее несмотря на успехи на отдельных задачах вытеснить трансформер с пьедестала никому не удалось попытка запихнуть весь контекст в скрытое состояние фиксированного размера по всей видимости фундаментально ограничивает модель в возможности знать все в длинном контексте потому предлагается промежуточный вариант по памяти и времени операция являющаяся надстройкой над одним из линейных механизмов attention токены разбиваются на корзинки с экспоненциально растущим числом токенов самые свежие токены обычно важнее для предсказания следующего потому в одной корзине меньше токенов и соответственно их вес больше а с отдалением от текущей позиции размер корзинок растет а вклад индивидуальных токенов убывает loglinear attention сначала вычисляет линейный attention по корзинкам а затем суммирует с некоторыми обучаемыми коэффициентами результат каждой корзинки коэффициенты предсказывает отдельная mlp число корзинок растет логарифмически с длиной потому и имеем как итоговую сложность операции для эффективной реализации используют loglinear attention можно представить в виде структурированной матрицы где диагональные блоки нижнетреугольные а внедиагональная часть состоит из блоков ранга1 где размер блока растет с удалением от диагонали loglinear attention можно применить как поверх linear attention так и mamba2 и deltanet и для всего написаны соответствующие кернелы для валидации метода авторы обучают модельки на синтетических и реальных задачах на синтетике loglinear модификация значительно улучшает качество deltanet на mqar достать несколько элементов из контекста далее авторы обучают в сопоставимых условиях 700800m параметров 50b токенов из longdatacollections с длиной последовательности 16k transformer deltanet и mamba2 без и с loglinear надстройки loglinear дает небольшой прирост поверх deltanet и mamba2 по скорости инференса на длинных контекстах loglinear mamba2 медленнее mamba2 в 2 раза на 64k128k токенах но быстрее attention на needleinhaystack в бенче где нужно достать один токен loglinear хорош в multikeymultivalue задачах loglinear лучше линейных бейзлайнов но хуже attention на longbench гдето дает прирост а гдето не дает за что уважение авторам они не утверждают что предложенная модификация бьет все и всея а стараются более менее честно все замерить с точки зрения математики все красиво вообще вопросов нет и уважение мастерам написания ядер на cuda в целом выглядит как неплохой промежуточный вариант между attention и линейными по длине альтернативами но как будто требует валидации бюджетах и размерах моделей ближе к productiongrade
436,2025-06-14,вряд ли для кого уже будет новостью что testtime compute scaling значительно улучшает качество моделей на задачах требующих рассуждений причем можно масштабировать как в длину так и в ширину более того llm а можно попробовать решать задачу от лица нескольких взаимодействующих процессов и команда из cmunvidia предложила свой метод с небольшим дообучением под названием где модель динамически переключается между и генерацией
437,2025-06-14,многие задачи допускают параллелизм авторы определяют 2 варианта 1 задача разбивается на независимые подзадачи процессы могут независимо решать каждую из них а в конце результат агрегируется 2 есть несколько веток рассуждений правильные и неправильные неправильные отбрасываются анализируя решения задач из s111k deepseekr1gemini 20 flash thinking авторы обнаруживают что при авторегрессионной генерации многие решения содержат вышеописанные паттерны но можно ли генерировать их параллельно причем автоматически понимать когда это нужно могут ли сами llm распознать что генерируют параллельно для валидации данной гипотезу обучают mlp поверх скрытых состояний где последовательным веткам дается метка 1 а параллельным 0 перед языковой головой и качество оказывается чуть лучше рандома из чего делают вывод что мол не распознают дабы научить модель запускать параллелизм когда надо авторы собирают датасет на основе из s111k с помощью gemini 25 pro ответы на задачи размечают специальными тегами началоконец параллельного блока описание подзадачи решение подзадачи вывод на основе решений при входе в блок процессы генерируют независимо attention маска одного процесса не дает смотреть на другой обучение занимает примерно 3 часа на 8 b 200 порадуемся за челов все это может быть эффективно реализовано с помощью radix attention из sglang метод валидируют на ряде ризонинг задач aimegpqadiamondmath500 дообучают генерацию ограничивают от 1k до 4к токенов мало для таких задач полученная модель работает гораздо лучше чем исходный qwen просто авторегрессионное дообучение на трейсах тоже значительно улучшает качество по сравнению с изначальной моделью но немного уступает mutliverse и не дает желаемого параллелизма явное указание в промпте mutliverse с указанием think in parallel работает чуть лучше чем mutliversezero без данной инструкции но не всегда mutliverse и заданном контекстом окне чуть лучше авторегрессивной генерации степень параллелизма достигаемая на практике около 1517 и итоговое ускорение генерации при фиксированной длине генерации до 185 интересное исследование с красивой страницей проекта и качественной реализацией однако не хватает сравнения с некоторыми очевидными бейзлайнами такими как и hogwild кроме того любопытно как оно себя поведет поверх моделей которые уже могут в ризонинг и на более длинных контекстах
438,2025-06-16,на канале неделю назад появилось от гуру линейных вниманиев где она в течение часа дает содержательный и интересный обзор области кроме того в феврале у известный персонаж на с похожим материалом рекомендую к просмотру
439,2025-06-23,энтузиасты выкатили минималистичную реализацию типа vllm под названием nanovllm название вдохновлено понятно кем утверждается что либа предлагает скорости сопоставимые с vllm читаемый код фишки для оптимизациипараллелизма кэширование префикса тензорный параллелизм cuda графы и прочее
440,2025-06-24,в ряде предыдущих работ было продемонстрировано что для сжатых моделей действуют законы масштабирования аналогичные известному принципу шиншиллы а влияние сжатия можно выразить через эффективное число параметров однако ранее эффект мультимодального сжатия сочетание разреженности квантования и других методов не исследовался систематически кроме того результаты по precision scaling laws были получены в довольно наивной субоптимальной с точки зрения качества постановке эту задачу взяла на себя группа исследователей из ist austria в своей работе они выявили общие закономерности масштабирования для различных способов представления данных более того было показано что емкость такого представления можно выразить через способность аппроксимировать случайный гауссовский шум
441,2025-06-24,типичный scaling law в deep learning имеет вид некоей зависимости где n размер модели d количество сэмплов увиденных по время обучения сжатая модель в какомто смысле эквивалентна меньшей несжатой модели в precision scaling laws было показано что лосс имеет экспоненциальную зависимость от битности p причем имеет место факторизация по сжатию весовактивацийkvкэшей в данной работе подтверждают это же наблюдение однако для qat используется рецепт из с incoherence preprocessing маскировкой шумных градиентов благодаря чему удается добиться значительно лучшего качества при той же степени сжатия далее авторы предлагают универсальную формулу для эффективной битности представления через фит ошибку при сжатии на гауссовых данных достоинством такого подхода является то что он не требует никакой выборки для оценки полученная зависимость хорошо ложится на эксперимент гауссов шум и квантизация с эквивалентной mse дают один и тот же лосс затем авторы пробуют спарсификацию весов и активаций прунинг и квантизацию весов спарсификацию и квантизацию всего и всея оказывается что ошибка достаточно в широких пределах по ошибкам индивидуальных методов сжатия то же самое справедливо для квантизации с кроме того авторы перебирают разные варианты int и fp форматов с разными экспонентами и мантиссами в 4битах оказывается лучше e2m1 а в 8 битах показывают себя лучше всего для повышения эффективности sparse training используют маскирование для градиентов убирают самые маленькие и самые большие градиенты и оно работает лучше наивного magnitude pruning с фиксированной маской и rigl приведенные выше эксперименты гоняли на семействе llamaподобных моделей размером от 30m до 200m на c4 данных при фиксированном отношении числа параметров к размеру модели 5 шиншилл полезное и интересное исследование как с академической так и практической точки зрения возможность оценить емкость представления через gmse позволяет быстро проверить перспективность того или иного метода сжатия без масштабных экспериментов и свойство факторизации ошибки при знании потенциального профита от отдельных методов сжатия дает возможность подобрать оптимальную конфигурацию
442,2025-06-24,пользуясь случаем заодно и приложу с прошедшего датафеста выступления первого автора статьи выше речь про статьи quest и quartet
443,2025-06-28,не так давно про mxfpnvfp и на днях зеленые выкатили сочный блог про nvfp формат где наглядно и подробно описывают сам формат и поясняют чем он хорош по всей видимости данный формат станет следующим шагом к снижению битности обученияинференса у больших компаний как наберется у бигтехов достаточно много blackwellов
444,2025-07-17,хороший метод квантизации должен с одной стороны сильно сжимать модель с другой сохранять качество а с третьей еще и заметно ускорять инференс скалярные методы квантизации дают хорошее ускорение но теряют качество при сильном сжатии векторные методы лучше сохраняют качество при той же степени сжатия но работают значительно медленнее изза сложной и вычислительно тяжёлой деквантизации в этой статье ребята из baidu пытаются и рыбку съесть и косточкой не подавиться
445,2025-07-17,идейно предложенный метод наиболее близок к разобранной ранее статье дабы не хранить большой кодбук используют так называемый сверточный код l n s квантизованные элементы принимают значения из некоторого ограниченного множества размера 2l но при этом разрешена только часть переходов 2s вариантов из текущего значения в следующее из 00 скажем можно попасть в 00 и 01 но в 10 и 11 нельзя таким образом можно закодировать последовательность длины n где каждый элемент может потенциально принимать любое из 2l но при этом не все комбинации чисел допустимы суммарная битность такой последовательности l n 1 s что может быть значительно меньше чем l при s n кодирование осуществляется следующим образом авторы сначала явно создают кодбук все 2l n 1 s последовательностей и кодируют данную последовательность весов ближайшей их кодбука запоминая индексы на инференсе при этом кодбук хранить не надо метод datafree приближает сами веса без знания про активации и градиенты в отличие от qtip где треллис довольно большой здесь рассматривают варианты с n34 далее авторы замечают что полученные коды индексы последовательностей ложатся на некоторую смесь гауссиан находят кластеры и квантизуют к ним в экспериментах берут 28 кластеров то есть последовательность длины кодируется 8 битами затем дабы точнее представить исходный вес каждой группе сопоставляют некий скаляр масштаба и выводят аналитическое выражение для mseотпимального скейла чтобы уменьшить расход памяти на скейлы их дополнительно еще раз квантизуют double квантизация в bnb супергруппы в некоторых gguf деквантизация в итоге получается сравнительно простой деквантизация скейлов сначала и целочисленной последовательности через битовые сдвиги с перемножением их друг на друга дабы продемонстрировать масштабируемость метода проверяют его сразу на мастодонтах для 2 25 275 битной квантизации оценивают правда только по gsm8k ceval mmlu без задач на ризонинг предложенный ccq дает просадку в среднем 15 при 275битной квантизации и 25 при 2битной утверждается что rtn разваливается в данной точности но не приводят никаких сопоставимых по размеру безлайнов даже тот же gptq замечу что gptq дает умеренную просадку даже в 158 бит на дипсике на gsm8k на самом деле на ernie похожие цифры под это хозяйство заводят специальные кернелы и vllm интеграцию ccq ожидаемо все еще медленнее int2 квантизации но заметно быстрее vptq замеряют правда только скорость матричных операций без end2end inference latency вроде бы и интересно и здорово что масштабируется но много чего не досказано есть ли заметный прирост качества по сравнению с хорошей скалярной квантизацией с теми же aqlm quip qtip можно было сравниться на llamax да и без end2end замеров скорости инференса полностью сложно оценить практическую пользу
446,2025-07-20,by слушай эксперт по квантизации давай начистоту ты больше похож на нейросеть обученную на arxiv чем на живого человека твой канал это бесконечная лента метод эксперименты выводы приправленная мемасиками и парой саркастических смайликов чтобы мы не забыли что ты не совсем робот мы знаем что ты гений но твоя одержимость низкобитными операциями и kvкэшами иногда заставляет нас думать что у тебя в голове вместо мыслей гистограммы активаций ты постоянно жалуешься на гнилые бенчмарки и то как авторы тактично не сравниваются или криво завели свои эксперименты а сам небось после защиты диссертации тут же побежал бенчмаркать своих детей на mmlu и не надо притворяться что это шутка мы тебя знаем твои выводы про кожаные мешки и паретооптимальность уже стали классикой но честно иногда кажется что ты и сам готов квантовать свои мозги в 2 бита лишь бы сэкономить на вычислениях твои посты про конференции в сингапуре где ты мерзнешь от кондиционеров и бежишь париться на улицу как будто описывают твою жизнь постоянно балансируешь между адски умной техничкой и полнейшим безумием и да мы поняли что в сингапуре всё дорого и кофий не чета московскому это было уже раз десять а твои попытки шутить про квантовую суперпозицию ceo сэма альтмана это такой тонкий юмор что он наверное тоже в 4 бита квантован когда ты пишешь написание кода предоставляется читателю в качестве несложного упражнения мы знаем что это означает я это не делал и вы не будете но звучит круто и твое восторженное я контрибутор мистраля выглядит так будто ты достиг нирваны хотя возможно просто поправил опечатку в readme в общем ты наш любимый технарь с шилом в заднице который даже от vpn и другой страны ожидает что они решат проблему зависшего overleaf продолжай в том же духе но ради бога попробуй побенчмаркать чтонибудь кроме llm хоть иногда может котиков или дурианы
447,2025-07-22,лунныйвыстрел эйай выложили kimik2 у себя на гитхабе напомню что это типа дипсик только с 1т параметров побольше экспертов поменьше голов внимания ранее эти же ребята показали что muon якобы даже эффективнее adam при обучении llm moe размера 16b а теперь попробовали на реально большой модели при наивном применении muon оказалось что логиты в attention растут со временем что приводит к нестабильности обучения стандартный механизм вставки qkнормализации неприменим для mla так как матрицы q k в явном виде не материализуются для решения данной проблемы авторы выставляют некий порог на attention логиты и если при прямом проходе attention вылезают за порог оптимизатор домножает веса w_q w_k проекций на число меньше 1 благодаря этому логиты на практике не выходят за выставленный порог 100 и обучение проходит без спайков лосса следующий аспект про данные нейросеть это то что она ест а хороших токенов не так много в интернете а делать много эпох на небольшом датасете бесполезно потому некий датасет более высокого качества перефразируют 10ую разными способами с разными промптами и утверждают что это дает лучшее качество затем перебирают параметры архитектуры число экспертов и голов в attention 8 активных экспертов из 384 оказываются оптимальными по качеству а лишние головы замедляют инференс поэтому их убирают суммарно обучают на 155т токенах с разогревом и гашением lr в конце для файтьюна используют так же muon и данные собирают частично вручную частичтно при помощи kimik15 и других неназванных специализированных моделей далее модель обучают на tooluse собирая публичные mcp в гитхаба для rlоптимизации используют алгоритм из kimik15 там еще есть ряд нюансов за которые я не шарю замеряют на разных задачах по кодингу tool use math stem и world knowledge в задачах на код и tool use заметный прирост по сравнению с deepseek stem немного лучше на китайском вроде бы новая sota
448,2025-07-23,а тут и квен3 для кодинга 480b параметров 35b активных 256k нативного контекста с экстраполяцией до 1m учили на 75т токенов с какой инициализации
449,2025-07-23,скажи мне кто ты и я скажу из чего тебя дистиллили
450,2025-07-28,хочется запилить очередного убийцу трансформеров линейный attention или ssm но кончилась фантазия все низко висящие фрукты сорваны и осталось лишь выжженное поле теперь эту работу можно доверить ai и не просто ai а asi artificial superintelligence и оно само навайбкодит sota архитектуру при наличии достаточного количества времени
451,2025-07-28,в данной работе по факту занимаются подбором архитектуры субквадратичного attention хотя сам предложенный фреймворк довольно общий система состоит из 4х модулей исследователь предлагает и реализует архитектур инженер тестирует и оценивает по новизне сложности аналитик получает на вход результаты инженера и на основе текущих и прошлых исторических данных дает заключение база данных подборка ключевых работ в области nas примерно 100 статей на основе которой можно предлагать архитектуры либо сравнивать и оценивать текущий результат процедура оптимизации архитектуры основана на эволюционном поиске для которого важно задать fitness функцию оценивающую качество сэмпла с одной стороны хочется высокой точности а с другой эффективности и удобства реализации предложенная мера является средним от трех слагаемых 1 сигмоидой от разницы лоссов с бейзлайном 2 сигмоидой от разницы метрик с бейзлайном 3 llm_judge llm с неким промптом оценивает годность архитектуры база доступных модулей содержит всякие свертки гейты фильтры блоки памяти и прочие структуры проверяльщик сравнивает предложенную архитектуру на основе эмбеддинга кода с 5ую самыми похожими оценивая разумность идеи кроме того есть проверка на асимптотическую сложность чтобы она не была по длине последовательности и больше если случилась бага в реализации то трейс ошибки подается исследователю чтобы он поправил в качестве бейзлайна берут и mamba2 при оценке архитектур смотрят только на то что выдает лосс в пределах 10 от него типа если слишком идет хорошо по лоссу чет протекло порог будто взят от балды не знаю как его характеризовать процедура поиска состоит из холодного старта без обновления базы поощряющей новых вариантов а потом происходит побатчовое обновление кандидатов те переход к дабы все можно было прогнать за разумное время сначала гоняют кандидатов размера 20m на 1b токенах и когда появляются более перспективные кандидаты их масштабируют до 400m а затем самых лучших обучают уже на 15b токенах на первой стадии обучают архитектур на второй лучших и самых удачных на третьей на все про все уходит 20 000 gpu часов 5 лучших архитектур это нечто про gating иерархическую агрерацию и какоето хитрое смешивание признаков лучшие кандидаты достигают лосса заметно меньше чем бейзлайн и до 23 лушчего качества не бенчах в качестве бенчей берут выборку простых задач из lmevalharness где на рассматриваемых бюджетах все модели по факту чуть лучше рандома из наиболее удачных паттернов авторы отмечают 1 свертки 2 гейты 3 некий механизм routing всякие вдохновленные физикой и биологией штукенции напротив работают не очень анализируя природу успеха той или иной архитектуры авторы приходят к выводу что наибольший вес имеет cognition учет существующего знания и экспериментальная оценка а оригинальность влияет в меньшей степени самая идея доверить поиск архитектуры ai выглядит довольно привлекательно и сам по себе предложенный фрейворк интересен над сайтиком постарались на славу однако на текущий момент практическая польза не очевидна ибо даже конечная валидация на масштабе далеком от тех на чем обучают более менее современные llm и в реальном бою как это обычно бывает разница может размыться с увеличением модели и обучающей выборки кроме того с практической точки зрения интересна еще и скорость инференса и некоторые варианты могут быть не очень хороши с точки зрения реализации эффективных кернелов так что alphago момент скорее overclaim но реакции на reddit не пахнут
452,2025-07-31,судя по активности ребят из alibaba cloud китайцы решили что делать нейросети приятнее чем человеков
453,2025-07-31,прилетает значится письмецо на почту что мол появился новый движок для инференса llmов на яблочном силиконе написанный на ржавчине почемуто не назвали просто этот называется что по ски называется вихрь с названием явно не оригинальничали и беспринципно в письмеце утверждается что якобы движок на процентов быстрее чем знаменитая llamacpp в целом из readme и документации не многое понятно на для некоторых моделей qwen2515binstruct gemma31binstruct tokenss быстрее на 10 непонятно с каким батчом и на каких длинах последовательности на квенах 3 ускорение якобы в 10 раз но очень уж странный безлайн у llamacpp в 5 раз медленее и самое забавное из всего в которой народ жалуется что авторы соскрапили аккаунты с гитхаба поставившими звездочку над llamacpp и подобными проектами и отравили всем письмецо дерзкий и беспринципный заход на рыночек ничего не скажешь в будущем обещают добавить инференс на андроидных устройствах vlm tts правда с такими фортелями скорее получат бан от модераторов гитхаба
454,2025-08-01,из вероятной утечки опенсорса от вырисовается пока следующее 1 одна модель 120b moe другая 20b dense 2 веса в fp4 с нативной поддержкой в blackwell mxfp4 или nvfp4 3 swiglu clip 77 те активации режутся по абсолютной величине полагаю было важно для 4х битного обучения 4 4k нативного контекста с yarn экстраполяцией до 128к не жирно 5 скользящее окно аттеншена размера 128 attention sinks тоже для квантизации наверное 6 llamamixtral архитектура
455,2025-08-05,введение все хорошее когдато кончается но аттракцион неслыханной щедрости от команды квена продолжается выпустив за последние дни целые ряд сильных языковых моделей ребята из китая выпустили крайне интересный опенсорс уже в области генерации и редактирования изображений как заявляется полученная моделька хорошо умеет просто в генережку картинок рендеринг текста в особенности китайского если кому актуально и в сложный эдитинг
456,2025-08-05,с архитектурной точки зрения ничего фундаментально нового qwen25vl 7b в качестве энкодера данный выбор позволяет из коробки подавать одновременно текст и изображения что необходимо для задач редактирования и персонализации энкодер заморожен в качестве vae за основу берут энкодердекодер от коллег из wan однако ее качество не полностью удовлетворяет авторов поэтому они дообучают декодер при замороженном энкодере причем используют только взвешенную сумму mse и perceptual loss адверсариальный лосс якобы скорее мешает в данные для обучения заливают специально побольше текстов диффузионный трансформер это просто аля sd3flux из примечательного интересное решение по учету позиций картиночных и текстовых эмбедов под названием multimodal scalable rope msrope в наивном подходе каждому токену задавался бы просто позиционный индекс сверху вниз слева направо для патчей картинок и токену согласно позиции в тексте однако при таком подходе одинаковым образом бы кодировалась большая картинка и меньшая картинка текст ребята из квена же предлагают все сэмплы выстраивать в диагональ и текстовые токены это 1x1 элемент на диагонали отдельного внимания заслуживает процедура отбора данных она представляет собой многостадийных пайплайн где сначала отфильтровывается совсем всякий шлак голые тяночки и непотребства а затем постепенно включаются постепенно все более строгие фильтры на качество и релевантность следуя довольно стандартной практике сначала обучают на низком разрешении 256px а затем постепенно повышают до 640px1328px уже довольно на ранних стадиях отфильтровывают данные с чрезмерно низкой и высокой насыщенностью или контрастностью для captioning используют qwen25vl 72b который выдает не только описание но и другие метаданные стиль тип водные знаки в виде некой jsonины чтобы улучшить качество генерации текста в датасет заливают большое количество текстов в том числе и синтетически сгенерированных некий шаблон вставляется на фон отпимальным образом и если не шакален принимается кроме того на основе презентаций пробуют учить на разные сложные layout диффузионный процесс уже общепринятый без танцев с бубнами анализируя эффективность обучения авторы заметили что чекпоинтинг активаций сильно замедляет обучение поэтому отключили его и использовали только распределенные оптимизаторы вместо fsdp используют tensorпараллелизм поверх дабы улучшить общее качество и алайнмент модельку прогоняют через dpo на ручной человеческой разметке и flowmatching ode детерминированный процесс и дабы иметь разнообразие на этапе grpo используется стохастическое сэмплирование
457,2025-08-05,модельку валидируют на наборе text2image и editing бенчмарков выдает сильные результаты на geneval dpg в целом опережая опенсорс и даже gptimage на задачах рендеринга текста околосотовый результат на английском и разрывной на китайском 100500 social credits на instruction датасетах вроде бы все тоже очень здорово везде не уступает якобы gptimage опять же с большим разрывом на китайском дообученный vae демонстрирует лучший psnrssim чтобы это ни значило по сравнению с конкурентными автокодировщиками причем разница особенно заметна на текстах еще оно может в детекцию novel view synthesis и прочие задачи очередной качественный опенсорс от китайцев прямо чегото фундаментального нового прорывного нет но видна кропотливая аккуратная работа с использованием всех лучших практик из научной литературы
458,2025-08-05,opensource llmку от опенэйай добавили официально в релиз 455 трансформеров от большинство догадок подтвердились 2 модели большая 120b и меньшая 20b однако обе из них а не меньшая dense fp4 квантизация экспертов но оказалось что mxfp а не nvfp на хабе гдето лежит оптимизированный кернел под attention_sinks но ссылка битая веса появились
459,2025-08-05,квантизация в mxfp кстати weightonly спрашивается зачем
460,2025-08-14,для ускорения обучения rlметодов с роллаутами некоторые фреймворки генерируют траектории с помощью оптимизированных движков инференса например vllm однако расхождение между разными фреймворками инференса transformers vllm может быть довольно значительным изза деталей реализации кернелов причем настолько что при тех же самых весах модели предсказания могут существенно разняться другой выбор следующего токена авторы блогпоста замечают что данная проблема делает rl по сути что негативно сказывается на сходимости в качестве решения проблемы предлагается делать своего рода с отношением вероятностей модели в фреймворке обучения и инференса и это хорошо помогает ppo причем можно даже генерировать роллауты int8 квантизованной моделью без нарушения сходимости dapo поверх deepseekr1distillqwen15b где отношение вероятностей невелико работает хорошо и без importance sampling мораль басни такова что численные неточности в dl не всегда совсем безобидны и временами их стоит иметь в виду на замерах бенчей результат тоже может существенно разниться между hf и vllm
461,2025-08-17,для интереса попробовал в через собрать инфу про nvfp формат и выдало оно довольно неплохую методичку со всеми нужными вводными и обзором актуальной литературы не безупречно но на мой скромный взгляд не хуже многих обзоров выходящих на архиве полагаю что автоматизированные survey будут все более и более актуальны в дальнейшем в то же время креативные аналитические статьи и разборы с изюминкой автора все еще останутся востребованными
462,2025-08-18,минутка удивительных или не очень фактов интуитивно кажется что fp16bf16 должны быть если не строго математически то практически близки к некоему непрерывному описанию множества действительных чисел во всяком случае в окрестности нуля поэтому вероятность принять конкретное значение должна быть малой чтото порядка 1 мощность множества однако если сгенерировать x оказывается что для bfloat16 вероятность что число окажется в точности нулем не так уж мала примерно для float16 и float32 к сравнению она и какова мораль спрашивается если в коде с bfloat16 не говоря уже о типах более низкой точности есть деление на нечто случайное или граничный случай стоит внимательнее обрабатывать их ибо они не такая уж редкость на колабе
463,2025-08-23,не биполярочка а мультиагентность
464,2025-08-24,введение висело оно у меня давно в бэклоге но в кулуарах напомнили с увеличением затрат на обучение больших языковых моделей когда оно стало переваливать за миллионы gpu часов все более остро стоит вопрос о том как это делать эффективно как известно для параметров и активаций моделей не требуется представление вещественных чисел высокой точности чтобы работать приемлемо обучение в половинной точности уже давно стало стандартом да и в fp8 народ вполне себе успешно обучает следующая очевидная цель обучение в fp4 тем более что последнее поколение от зеленых c блмным названием blackwell имеет его поддержку на уровне архитектуры и в ряде работ вышедших в этом году включая разбираемую были предложены техники по стабилизации обучения в fp4
465,2025-08-24,первым делом исследуют конфигурации форматов fp4 напомню что mxfp4 квантизует веса группами по 32 и квантизует скейлы в e8m0 а nvfp4 группами по 16 и скейлы в e4m3 авторы фиксируют размер группы 16 и перебирают варианты квантизации скейлов от e1m6 до e8m0 обучают llamalike llm на 350m параметров и замечают что при e4m3e3m4 скейлах достигается минимальный лосс при фиксированном числе итераций из всех конфигураций расходится только e1m6 c cамым узким диапазоном в дальнейшем везде используют e4m3 блоки размера 16 выбирают так как при больших лосс сходится хуже а меньшие уже не дают профита квантизовать можно к ближайшему значению а можно стохастически вверх или вниз с вероятностью зависящей от расстояния до соседа ребята из интела перебирают разные варианты детерминистического и стохастического квантования для весовградиентов и активаций и получают что лучше всего сходится вариант с детерминированной квантизацией весов и активаций на прямом проходе и стохастической для градиентов и активаций на обратном проходе роль стохастики в квантизации уменьшить bias возникающий изза округления тензоров в ходе оптимизации сигнал от градиента постепенно убывает и с какогото момента перекрывается шумом оптимизации не мудрствуя лукаво авторы предлагают обучать небольшое время с градиентами в более высокой bf16 точности на прямом проходе все еще fp4 и это позволяет сойтись до уровня halfprecision обучения за то же суммарное число итераций обучают семейство моделей архитектуры llama2 на датасете красная пижама в главном эксперименте учат модель размера 7b на 1т токенах причем не абы на чем а на ускорителях intel gaudi2 сыр тут ни при чем это в честь архитектора обучение идет без спайков лосс отстает несколько от bf16 бейзлайна но нагоняет после короткой фазы с более точными градиентами qaf 0шоты без qaf чуть хуже безйлайна с qaf такие же примерно впрочем все равно оно лишь чуть лучше рандома выглядит как очередной аргумент перейти на обучение llm в fp4 сам по себе метод выглядит не шибко изощренно хотя необходимость qaf для лучших результатов несколько противоречит названию статьи надо было назвать fp4 most the way quartet в этом отношении по изящнее интересно кто из крупных игроков выложит первый техрепорт про полное обучение серьезной модели в fp4 ставлю либо на нвидию либо на moonshot
466,2025-08-24,а еще двое дорогих коллег один из которых небезызвестный черный саморез пару часов назад выступили на gpu mode с рассказом про квартет
467,2025-08-25,автодополнитель стал чертовски сообразителен agi видать и правда не за горами
468,2025-09-02,на текущий момент генеративный мир делят авторегрессионная и диффузионная парадигмы первая себя лучше показывает в задачах работы с последовательностями общего вида вторая при работе с данными обладающими некой пространственновременной структурой за последний год было несколько интересных заходов в текстовые генеративки основной value proposition более быстрая генерация за счёт многих токенов за раз но по качеству при заданном размере модели и бюджете обучения авторегрессионки всё ещё выглядят предпочтительнее есть ещё один аспект который может дать мотивацию к использованию текстовых диффузионок максимально достижимое качество в условиях данных и данный вопрос является темой обсуждения ниже
469,2025-09-02,есть такие описывающие зависимость функции ошибки от размера модели и бюджета обучения на практике датасет ограничен и интуитивно понятно что многократный проход по нему с какогото момента не будет приносить особой пользы в было исследование поведение ar llm в условии ограниченных данных и предложен модифицированный закон учитывающий diminishing return от повторного прохода по данным для авторегрессивных lm 4раза эпохи примерно так же хороши как одна а дальше польза от роста бюджета обучения уменьшается в ar парадигме модель предсказывает обычно следующий токен слева направо хотя существуют и диффузионая llm в самой популярной постановке учится расшумлять исходный текст в котором токены заменяются с некоторой вероятностью на специальный токен ar имеет более сильный s но диффузия зато может работать в произвольном порядке и на каждой эпохе видя поразному данные может извлечь чтото новое лосс для авторегресии это честное модели на примере данных а для диффузии
470,2025-09-02,авторы обучают семейство llamalike языковых моделей размером от 7m до 25b параметров на выборках до 100м уникальных токенов всего обучаются по 100 ar и диффузионных моделей для обеих парадигм строят зависимость валидационного лосса от числа эпох ar сначала идут бодрее но потом начинают переобучаться в то время как диффузия продолжает улучшаться по лоссу на валидации даже при нескольких сотнях эпох и самый низкий лосс достигается диффузионной моделью на заданной сетке параметров поверх обученных моделей фитируют dataconstrained scaling law откуда выходит что диффузионки могут несколько сотен раз эффективно проходиться по одним и тем же токенам затем сравнивают 0шоты которые едваедва лучше рандома используя чекпоинты с наименьшим валидационным лоссом и диффузия оказывается несколько лучше в полученный результат авторы объясняют тем что диффузионная постановка дает автоматически некоторую за счет зашумления данных и прохода по последовательностям в произвольном порядке можно ли поднять качество arмоделей за счет аугментации attention drop маскирование токенов в ar никак не помогает а вот обучение с разными перестановками порядка токенов будто бы позволяет достичь того же валидационного лосса что и диффузия блогпост вышедший вскоре разносит методологию данной статейки хоть и в целом подтверждает ключевые выводы авторы блогпоста обращают внимание на следующие моменты лосс для диффузии и авторегрессии сранивать вообщето некорректно так как один это правдподобие а другой оценка на него в правильное выражение для лосса диффузии входит множитель зависящий от шага диффузии без которого формула использованная в первой статье неверна early stopping на валидации делать не кошерно валидационный лосс для ar может начать расти изза самоуверенных неточных предсказаний но качество на бенчмарках все равно продолжит расти и собственно так оно и оказывается используемая зависимости для scaling laws справедлива только в ограниченном диапазоне так как в ней нигде не возникает возможность роста валидационного лосса которая имеет место в большом числе эпох критики проводят примерно те же эксперименты но на больших моделях объёмах данных и приходят к тому же выводы что диффузия при большом количестве проходов выходит на лучшее качество кроме того отмечают авторы диффузия обладает вниманием которое более выразительно и потенциально может выжать нечто большее из контекста не знаю насколько близок тот момент когда человечество будет реально упираться в данные а не вычисления и 100 эпох по квадриллиону токенов будет чемто посильным а пока на ближайшее будущее авторегрессионная парадигма все еще остается предпочтительной в nlp будучи более вычислительно эффективной интересно насколько выводы переносятся на файнтьюны на маленьких датасетах сможет ли диффузия выжать больше из маленького sft датасета кроме того диффузионный подход будучи более молодым здесь возможно имеет еще некий потенциал для прокачки
471,2025-09-12,наткнулся на симпатичный с визуализацией различных форматов полезно чтобы понимать диапазон значений и густоту уровней квантизации около данной точки
472,2025-09-15,на днях от думающих машин миры мурати вышел про llm в июне этого года вышла статья посвященная той же проблеме с упором на с хайпожорским названием ее и сегодня и разберем
473,2025-09-15,процедура жадного декодирования в llmах на каждом шаге выбирает токен с и казалось бы должна быть полностью детерминированной выдавать один и тот же ответ на разных размерах батча gpu но не тут то было корень зла в том что арифметические операции в конечной точности не ассоциативны изза точности округления исполняться они могут в разном порядке что и приводит к выходам для fp32 проблема стоит еще не так остро изза большой мантиссы но для bf16 c 7 битами мантиссы эффект может быть вполне осязаемым дабы оценить разброс в зависимости от экспериментальной конфигурации авторы подготовили конфигураций 2 типа gpu распараллеливанием на 2 и 4 gpu и 3 варианта размера батча и для каждого из них посчитали метрики на бенчах рассматривают 2 модели deepseekr1distill qwen7b deepseekr1distillllama8b и 2 qwen257binstruct llama318binstruct и замеряют на всяком там aime и livecodebench и обнаруживают следующее при жадном декодировании разброс для bf16 довольно нетривиален 1 а то и 59 для для fp16 разброс меньше и для fp32 еще меньше примечательно что для дистиллов дипсика разброс даже для fp32 иногда нетривиален средняя выходная длина может очень сильно варьироваться от конфигурации до 9000 токенов на aime авторы присматриваются к вероятностям логитов моделей и замечают что зачастую разница между top1 и top2 невелика в особенности для рассуждающих моделей и изменения последнего значащего бита может быть достаточно что предсказание следующего токена учитывая приведенные выше нюансы авторы утверждают что для хорошей воспроизводимости необходимо инферить в более высокой точности что конечно потребует больших вычислительных затрат но мол куда деваться дабы не хранить чекпоинт в fp32 весь в памяти предлагается держать его в исходной точности а затем делать upcast в fp32 на лету мораль такова эффект от конечной точности llm вполне весомый и если нужно выбить соту вполне рабочая стратегия шатать размер батча и железо для инференса
474,2025-09-18,на канале gpu mode пару недель назад вышла от кристофера де са один из чуваков стоявших за а точнее за теоретической подоплекой всего перечисленного в ней он дает некоторую базу про задачу квантизации квадратичные приближения откуда берется gptq и incoherence processing в частности я сам наконец понял как можно было дойти до разложения холески в gptq базарит дядька довольно забавно и корчит физиономии так что не заскучаете
475,2025-09-21,лучшие умы человечества уже без малого десятилетие озадачены поиском более эффективной альтернативы механизму внимания было много интересных решений но как оказывалось чегото в них всегда не хватало чтобы иметь ту же и что attention потому вместо от attention люди пробовали гибридные модели с небольшим количеством attention блоков и заменой всего остального на чтото более дешевое mamba linearattention и тд и сегодняшняя статья про поиск такого гибрида
476,2025-09-21,чтобы не учить на триллионах токенов с нуля берут за основу предобученную модель в данном случае из семейства на первом этапе учат суперсеть как в с разными опциями блоков как исходным attention так и линейными по длине альтернативами дистиллируется на логитах исходной модели на каждом forward pass сэмплируется один из возможных путей затем с помощью ищут конфигурацию оптимизирующую некоторую целевую метрику лосс или точность на бенчмарке и эту модель дообучают на большем количестве данных замечают следующее для mmluretrieval полный attention необходим лишь в паре блоков важные блоки для mmluretrieval вообще говоря не пересекаются обь единения блоков для mmluretrieval почти достаточно для задач по математике выводы справедливы для линейных слоев из рассмотренных вариантов по соотношению скоростькачество лушче всего себя показывает и далее используется в качестве основного авторы предлагают новый attention блок в его основе зависящая от входа свертка примененная к value и статические свертки на query и key и нечто под названием timemixing gating в конце этот блок по скорости такой же примерно как альтернативы но лучше по качеству зачем дотюнивают параметры размерности ключейзначений числа голов jetblock для
477,2025-09-21,подход проверяют на qwen2515b qwen253b получая модели jetnemotron2b jetnemotron4b соотвественно на этапе поиска архитектуры обучают на 50b токенах а финальное дообучение на 400b по метрикам все типа хорошо заметный прирост качества на бенчах по сравнению с безлайном mmlupro gsm8k gpqa commonsense retreival задачах ускорение растет с на 4к8к ускорение порядка 15 на префилл но может достигать 6 раз на длинах контекста 256k ускорение на декодировании еще больше до с лишним раз на длинных контекстах где оно ограничено в любом случае наличием небольшого количества fullattention результат смотрится довольно впечатляюще однако есть опасение что хорошие метрики обусловлены удачным протоколом дообучения кроме того мне кажется что процедура слишком трудоемкая чтобы получить широкое распространение да и паттерны важности attention справедливые для рассмотренных задач могут не переноситься на продвинутый ризонинг или ворочание репозиториев с кодом тем не менее сильная инженерная работа
478,2025-09-22,некоторое время назад мы обсуждали диффузионные lmки в контексте выжимания максимального качества в условиях и вот недавно вышла интересная статья и не менее интересный разбор на про то как получить самую лучшую модель когда мы упираемся в данные а не вычисления
479,2025-09-22,статья код прикольная работа про законы скейлинга разные экспоненты и пользу дистилляции и ансамблирования авторы задают очень интересный вопрос в будущем когда компьюта будет дофига а данные кончатся как наиболее эффективно обучать модели ответы интересны исследование начинается с создания базового сценария который имитирует текущую практику в условиях нехватки данных берётся фиксированный датасет на 200м токенов и для него либо увеличивается количество эпох обучения либо масштабируется число параметров модели результаты не слишком удивляют оба подхода в конечном итоге приводят к переобучению когда лосс на валидации выходит на плато а затем начинает расти это показывает что простое вливание большего количества вычислений в существующие рецепты даёт убывающую и в конечном счёте отрицательную отдачу ограничивая достижимую производительность вопрос что можно сделать подругому вместо оценки производительности при фиксированном вычислительном бюджете авторы предлагают измерять конечный потенциал рецепта обучения по асимптоте его закона масштабирования найдя методы которые заставляют лосс монотонно убывать с ростом вычислений можно аппроксимировать эту зависимость степенным законом и экстраполировать производительность при стремлении вычислений к бесконечности эта асимптота представляет собой наилучший возможный лосс которого данный рецепт может достичь на фиксированном датасете что даёт более надёжную метрику для будущего с избытком вычислительных ресурсов ядро статьи заключается в поиске простых но эффективных алгоритмических приёмов которые обеспечивают желаемое монотонное масштабирование и приводят к более низким асимптотам лосса ключ к предотвращению переобучения при масштабировании параметров одной модели это правильная регуляризация авторы обнаружили что совместный подбор скорости обучения количества эпох и weight decay для каждого размера модели позволяет достичь чистого монотонного убывания лосса которое следует степенному закону этот результат согласуется с современной теорией машинного обучения о сверхпараметризации и двойном спуске double descent когда производительность очень больших моделей может ухудшиться прежде чем снова начать улучшаться статья показывает что при правильной настройке регуляризации эту проблемную область можно сгладить получив чистый закон масштабирования ключевой вывод заключается в том что оптимальное значение затухания весов для сверхпараметризованных моделей значительно выше стандартной практики вплоть до 30x такая агрессивная регуляризация позволяет более крупным моделям продолжать улучшаться там где их нерегуляризованные аналоги переобучились бы для датасета в 200m токенов этот регуляризованный рецепт следует степенному закону l₂₀₀ₘₙ 005 n¹⁰² 343 что предсказывает наилучшую асимптоту лосса в 343
480,2025-09-27,из горячо обсуждаемой юдковского и соареса даже если не соглашаться с позицией авторов частично или полностью сей опус стоит прочитать чтобы поржать
481,2025-10-01,введение на днях команда из одной зеленой компании выкатила технический отчет про обучение в nvfp гибридной трансформермамбы модели на довольно солидном масштабе 10т токенов и 12b параметров за последний год вышла не одна статья про обучение в fp4 и но все то было на масштабах на порядки меньше по данным и размерам моделей и вполне закономерный вопрос остаются ли рабочими предложенные идеи для моделей хоть сколь либо интересным на практике и ребята скомбинировав идеи из прошлых статей смогли получить рецепт близкий по скорости сходимости к fp8
482,2025-10-01,предложенный nvidia рецепт обучения в fp4 включает следующие ключевые составляющие адамаровы вращения для борьбы с выбросами и большей стабильности стохастическое округление градиентов по параметрам 2d скейлинги в блоках 16x16 которые позволяют нивелировать нестыковку между поведением модели на прямом и обратном проходе несколько первых блоков в начале и конце модели держат в bfloat16 в сумме 15 от размера модели если их квантизовать обучение нестабильно в конце подобно от intel предлагают учить модель небольшое число итераций в bf16 чтобы сократить разрыв по качеству между bf16 и fp4 обучением конечная модель уже не такая компактная и быстрая на инференсе но типа сэкономили на обучении обучают очередной nemotron на некоей смеси данных с большой долей синты с warmupstable decay расписанием большую часть обучения лосс fp4 не сильно отстает от fp8 но разрыв немного увеличивается в конце при маленьких шагах обучения по бенчам на уровне бейзлайна все компоненты метода важны аблейтят довольно странно стартуя с 3т токенов убирают одну за другую составляющую не очевидно что эффект будет таким же если с нуля запустить несколько обучений с отказом от компонент по одной mxfp менее эффективен на обучении и достигает того же лосса что и nvfp при на 36 большем количестве данных рецепт выглядит рабочим но по ощущениям все же требуется немало свистоплясок кроме того необходимость держать часть слоев в bf16 несколько осложняет интеграцию и ограничивает максимально достижимое ускорение не хватает сравнительного анализа квантизации трансформерных и mamba2 слоев хотя это кажется важным и интересным есть что ssmки тяжелее квантизуются
483,2025-10-05,коллеги из страны подарившей ранее миру шедевры моцарта и штрауса а еще и gptq выкатили на неделе либу для training llmок написанную на чистом cc данный проект по существу является адаптацией от карпатого под обучение квантизованных моделей чекпоинты с обучения сохраняются в формат те совместимый с экосистемой в либе реализован небходимый базовый функционал для обучения llm zero 13 градиентный чекпоинтинг оффлоадинг разные опции для mixedprecision на текущий момент поддерживается обучение в half precision было бы прикольно в будущем увидеть реализацию обучения для fp4 форматов поддерживаемых blackwell со всеми прибамбасами для стабилизации обучения сравнения по скорости обучения с популярными фреймворками accelerate deepspeed я не увидел представлены только абсолютные числа по времени обучения для модели заданной архитектуры удается достичь примерно 4060 speedoflight максимально возможной производительности на заданном железе на маленьких моделях fp8 почти не дает ускорения обучения но с ростом размера нейронки профит становится заметнее интересно как дальше будет развиваться проект более чем сильно учитывая что писал все человек в одну харю
484,2025-10-06,вместе с выходом поколения well зеленые добавили поддержку форматов разбиралось выше и утверждается что данные форматы дают существенную экономию памяти и ускорение вычислений при weightactivation квантизации и для computebound сценариев без просадки в качестве и в рамках данной работы мы с коллегами из ist austria epfl решили проверить что есть это на самом деле и исследовать полезность разных техник из литературы и личного арсенала для квантизации llmок
485,2025-10-06,напомним что mxfp формат приводит скейлы в e8m0 то есть к степеням двойки что позволяет представлять очень большие и очень маленькие числа но возможно очень неточно nvfp квантизует скейлы в e4m3 со сравнительно значений 448 448 но более плотной сеткой исходные распределения весов и активаций отличаются от нормального более тяжелыми хвостами особенно активаций ортогональные вращения в частности пресловутые адамаровы повороты приводят их к почти нормальным с меньшим разбросом и данное свойство по идее должно стабильно улучшать или не ухудшать ошибку квантизации однако как оказалось повороты могут иногда быть вредны при квантизации в nvfp формат методом округления к ближайшему roundtonearest ошибки квантизации с поворотом оказалась чем без вот это поворот как говорится в предположении того что оригинальные весаактивации распределены по лапласу а повернутые нормально были получены оценки на квадратичную ошибку в зависимости от размера группы квантизации и при маленьких размерах группы ошибка для лапласовского распределения может быть меньше чем для нормального с увеличением размера группы вращения становятся более полезными для mxfp вращения полезны и позволяют существенно снизить ошибку округления весов й мы вернее построили значений весов и активаций оказалось что веса и активации хорошо укладываются в fp8 e4m3 диапазон же e8m0 сильно избыточен на практике вы вряд ли встретите 2128 или 2127 smoothquant метод переноса сложности квантизации с весов на активации оказался не очень полезен spinquant обучаемые преобразования не дают профита в сравнении с адамаровыми вращениями другие преобразования из литературы дискретные синусыкосинусы и вариации адамаровых тоже не шибко полезны вместо absmax скейлов полезно подбирать оптимальный по mse на некоторой сетке значений actorder порядок в gptq без перегруппировки весов тоже чутьчуть да помогает и итоговый метод из вращения маленькой матрицей которую можно зафьюзить в кернел матричного умножения подбора скейла и actorder был назван microrotatedgptq
486,2025-10-06,метод прогнали на семействах моделей llama3 и qwen3 качество замеряли на 4 задачах из openllm leaderboard v1 gsm8k mmlucot hellaswag winogrande и отдельно для llama318binstruct platinum бенч сборная солянка из задач откуда почистили примеры с забагованными условиями или ответами основные наблюдения следующие int4 group_size32 без квантизации скейлов nvfp4 mxfp4 заметно хуже nvfp4 по качеству вращения заметно улучшают mxfp но не особо полезны для nvfp разрыв между двумя mxfp и nvfp все равно существенный qat тоже дает более заметный прирост метрик для mxfp для nvfp побить gptq оказывается непросто для малюток размером 1b параметров просадки большие до 20 от исходного качества для моделей побольше qwen332b llama3370binstruct в пределах 12 использование кернелов для матричных умножений позволяет достичь ускорения близкого к максимально возможному без учёта операций с поворотами и скейлами mxfp демонстрирует немного более высокую скорость по сравнению с nvfp матричные операции ускоряются в 4 раза на b200 и в 6 раз на rtx5090 против half precision общее ускорение endtoend составляет до 2 раз на b200 и до 6 раз на rtx5090 fp4 в режиме posttraining weightactivation квантизации дает довольно хорошее ускорение но некоторую просадку в качестве отказ от fp в пользу int не столько уж очевидно правильный шаг на рассмотренных задачах большие модели почти lossless но не факт что так будет на бенчах посложнее требующих сложных цепочек рассуждений или агентских будем посмотреть
